{
    "queries": {
        "09af3cb7-62fb-4f7d-92a1-2afcd5fd24a4": "Here is a question based on the provided article:",
        "39e45d81-3ca3-4405-b199-5493d7dd4809": "What are the key differences between the authors' approach to forecasting and the approaches presented in references [1], [4], [5], and [8], particularly concerning the type of forecast, variables, and factors considered?",
        "8e86acec-befb-46c8-8c5a-b1afbf27385e": "Here is a question based on the provided context:",
        "389b4347-6f1d-49b7-a8e0-650af9aa11f7": "The document describes a \"sum constraint based modelling technique\" to overcome two challenges. What are these two challenges, and how does the fundamental assumption about the sum of sales over a week for a given category contribute to addressing them?",
        "c88fafe6-e9a9-410c-be1f-98311b68636a": "Explain the purpose of the \"back-padding\" process in Stage 1 and how it relates to the input features and output label.",
        "3dbdc20d-1b84-495d-abbc-d6b775ec8272": "In Stage 3, the objective function `l3` for training XGBoost3 is defined. Explain the purpose of each of the two main terms in the `l3` equation (7), and how `YpredRatioi` (8) from Stage 1 contributes to the second term.",
        "21b8d086-cb8a-4ea2-bf10-60088515edfa": "Describe the purpose of each of the three stages in the algorithm's flow chart, highlighting how information from previous stages is utilized and what the objective of each stage is, particularly focusing on the roles of XGBoost1, XGBoost2, and XGBoost3.",
        "6f09f150-2d5a-4a99-9891-0f58e6973479": "What is the purpose of Stage 3 in the described framework, and how does it adjust device-level predictions?",
        "77a09d1e-ac49-4ac4-9cd1-a53c5851111f": "According to the document, what specific events are identified in Figure 1 that lead to spikes in sales, and how does the newer model demonstrate its ability to capture these events?",
        "241e152b-7e49-4a8c-97d3-5c3fe3ff4da1": "Here is a question based on the provided context:",
        "ad1f31a5-9e06-4aa8-941e-7035f657ea56": "Based on the provided references, what are some of the diverse applications of machine learning and deep learning techniques in forecasting, and which authors explored these applications in 2019?",
        "effbc2cf-0203-4890-af6f-349e5c776563": "**Who are the authors of the work \"Application of Predictive Analytics to Sales Forecasting in Fashion Business\" and what is their affiliation?**",
        "fac1a2bf-fd74-465b-8e01-41ea12cc05c1": "Based on the provided text, describe the key characteristics of the apparel supply chain and explain how these characteristics influence the challenges faced by fashion companies in a global and competitive environment.",
        "89b40ab9-77c9-4fab-92e3-b5de96cf4b79": "According to the text, what are three benefits of accurate sales forecasting in the apparel supply chain?",
        "582b415f-4333-4bf8-a53f-34a674dba3f7": "What are the two distinct horizons of sales forecasting that need to be distinguished in the fashion industry, and what is the purpose of each?",
        "2b607b06-2bbb-4745-9c40-3a34b3f79040": "According to the text, what are the two major categories of forecasting techniques, and which type has traditionally been used for fashion sales forecasting?",
        "d9024f08-70df-44b5-afd8-38a389903d72": "According to the text, what are the main limitations of statistical forecasting methods when applied to the fashion industry, and why are qualitative methods often used in conjunction with them?",
        "4c19eeb7-d081-4967-a79f-183d366899bc": "Describe the three-stage analytical approach in business intelligence, detailing what each stage aims to determine and the methods or characteristics associated with it.",
        "e1dd2ce4-727f-4631-a101-c73127ea07d4": "What are the key characteristics and applications of Fuzzy inference systems, Neural networks, Extreme learning machines, Decision trees, and Clustering as predictive modeling techniques, according to the text?",
        "292b0b00-e11c-4e56-9170-e40f5c46b1fc": "What are the advantages of using fuzzy logic and neural networks for fashion sales forecasting, and for what type of forecasts are they especially useful?",
        "c8bc2709-8e59-449d-aeae-149eae441ae3": "What are the drawbacks of the traditional sales forecasting method commonly used in fashion companies, as described in the text?",
        "5697015c-2091-45e6-89d1-0b646348d6ae": "What was the response rate for the interviews, and what percentage of participating companies offered clothes for women, men, and children?",
        "5c5b3c61-23e4-404d-84a7-878f21a00094": "According to Mayring's qualitative content analysis, what are the four main steps involved in analyzing the data, and briefly describe each step?",
        "ad962565-85c8-4da4-8aaa-1c1e02e6b9a1": "What two primary components form the basis of sales forecasting for 23 out of 24 interviewed companies in the fashion business?",
        "d83fe079-a94b-4880-adf1-bae29ed840ec": "Here is a question based on the provided text:",
        "502d27f8-8781-4e6a-9ac8-caa3b6293330": "What qualitative factors do companies consider when making adjustments to their sales forecasts, and how do buyers and sales personnel contribute to this process?",
        "ccdb0290-e5b6-4e51-b272-9da89e87f5cb": "What percentage of interviewed companies conduct sales forecasting three-quarters up to one year in advance, and what percentage do so half a year in advance?",
        "bc79daef-9504-4d79-acd4-0ba8eff965ec": "What are the key departments usually involved in the sales forecasting process, and what is the role of top management in this process?",
        "29808cde-dc50-4ae8-8f60-ee406c063730": "What percentage of interviewed companies refer to sales data of comparable articles when forecasting new items without historical sales data, and what other methods do they use?",
        "9f5c2f20-f22e-47a7-9c59-318f79ff78f4": "What are the primary obstacles to the widespread implementation of predictive analytics in the fashion industry, as identified by the interviewed companies?",
        "856f3a89-91d8-498f-a508-9344de5126b1": "According to the \"Data availability, variety and quality\" obstacle, what are some of the key challenges companies face when trying to implement predictive analytics, and what specific examples do participants provide regarding data collection and integration?",
        "cefe7491-0d9d-4f04-aed7-9addb1f23d89": "According to the provided text, what are the primary reasons cited by participants for the lack of acceptance of predictive analytics methods by decision-makers?",
        "244637a4-e942-4815-9d55-359c2e4b9369": "What are the main obstacles that fashion companies need to overcome before fully adopting predictive analytics for sales forecasting, according to the interviewed experts?",
        "c8ec4850-c9fb-40c6-b7f2-6a331600d81c": "According to the text, what are three key practical implications for fashion companies to improve their analytics culture and sales forecasting, and why are each of these important?",
        "4e34f0c1-6b56-4e06-b643-0519f4b72693": "According to the text, what are the three stages of the transformation process that fashion companies should follow, and what is the primary focus of each stage?",
        "58774d08-cc65-4aed-bf16-ea7948df38c0": "Which authors published works related to \"Predictive Analytics\" in 2014, and what were the titles of their publications?",
        "0493a894-3328-4cc8-880e-f7106f702e69": "According to the provided context, which authors reviewed applications of artificial intelligence in the apparel industry and in what year was their review published?",
        "cba020d6-fc6b-4711-aa76-b6ba59c2a7da": "According to the provided references, which authors have contributed to research on sales forecasting specifically within the clothing, apparel, or fashion industry, and what are some of the key methodologies or applications they discuss?",
        "5c368c9c-e745-49f5-9421-39606d1e3296": "According to Zhao, Xie, and Lau (2001), what are two approaches explored for improving supply chain performance?",
        "62f8edd2-115c-4618-8842-4784e71216d2": "According to the abstract, what were the key findings regarding the effectiveness of judgmental adjustments in improving forecasting accuracy in supply-chain companies, particularly concerning the size and direction of adjustments?",
        "d36a47f5-e6ef-4300-9e01-c05669765493": "According to the text, what are some of the data difficulties that make the forecasting task challenging, and how are these difficulties compounded in practice?",
        "3264614a-8ba8-44d9-8ff6-02bb5ee0bfd3": "According to the text, what are some of the reasons why forecasters might make unnecessary judgmental adjustments to statistical forecasts, even when these adjustments may reduce accuracy?",
        "a3572584-6c30-4412-91e6-60ed08aa289b": "According to the text, what are three reasons why smaller judgmental adjustments are likely to be less effective in improving forecast accuracy?",
        "8e1c3fa3-8e6b-464f-80f1-bd10e133d796": "According to Sanders and Ritzman (1992), what is the relationship between the volatility of a time series (as measured by the coefficient of variation) and the relative accuracy of judgmental and statistical forecasts?",
        "f10523b0-c0cd-47c4-a975-87f109944888": "According to the text, what are the two components into which bias can be decomposed, and what does each component represent?",
        "4b830c81-af12-4032-954a-f9e540156316": "What are some of the human cognitive limitations that can affect forecasters' ability to incorporate information from multiple sources, and how might these limitations manifest in their forecasting process?",
        "5bbd664c-f10c-4e85-acaf-ae37dc105017": "Based on the provided text, what criteria were used to exclude SKUs from the analysis, and what is the stated reason for excluding SKUs with a final forecast of zero?",
        "79b143c7-b937-463e-b7b0-cb163418aa5e": "According to the text, what is the chosen \"more intuitive measure\" for relative adjustments, and what are its components?",
        "cc93e5bb-7424-4d90-bcb0-a83decc3e9be": "Based on the provided context, here is a question for your quiz/examination:",
        "ff4d4a7b-4449-4192-a5a3-feefb076dc01": "According to the text, what statistical measure did the authors primarily focus on for interpreting forecast error, and why did they choose this over the trimmed MAPE?",
        "a92ed316-c0ee-4cef-a8d9-e88e64855daf": "What two factors are investigated to determine their effect on the accuracy of judgmental adjustments, and how is each factor measured in this study?",
        "ed73ab95-2822-4506-863f-f14b0ac4b797": "According to the text, what is the formula for FCIMP, and what does a positive or negative value of FCIMP indicate about the effect of judgmental adjustments on forecast accuracy?",
        "83841e33-52cc-4d05-8091-19154423577f": "According to the text, what is the primary difference in effectiveness between negative and positive judgmental adjustments made by forecasters, and what evidence is provided to support this distinction?",
        "44bb95bd-6a3a-45e2-94fd-fd9f5114a96e": "Based on Table 4, compare the mean percentage errors for \"Na\u00efve System forecast\" and \"Final forecast\" across different organizational groups (A\u2013C vs. D1 & D2) and directions of adjustment (No adjustment, Positive adjust, Negative adjust). What general trends or differences do you observe in how these forecast types perform for each group and adjustment direction?",
        "344eadcf-ad1a-41d8-9e9d-18f59ce1f2a8": "Based on Table 6, compare and contrast the regression coefficients (\u03b1 and \u03b2) and R\u00b2 values for positive and negative adjustments in organizational groups A-C versus D1&D2. What do these differences suggest about the nature and magnitude of bias in these groups?",
        "6b1fe7ec-fcbc-4430-9d6c-253fbcce2d87": "According to the text, what is one specific reason why managers at the retailer (D1 & D2) were making poor positive adjustments to forecasts, and what potential danger did this pose?",
        "2b1e3002-8e1d-424d-8c93-1c05b4845f11": "According to the article, what are the two plausible explanations for bias associated with positive adjustments in forecasts for companies A-C, and which explanation does the data primarily support?",
        "3793f4b4-04b8-4747-864e-ed5b09d89911": "According to the text, what are some of the specific inefficiencies observed in the forecasting practices of firms A-C and D, and how do these compare to the findings of Goodwin and Fildes (1999)?",
        "44997ad5-04d3-475c-a411-b6922dceaba6": "According to the \"Blattberg\u2013Hoch approach,\" what is the formula for the resulting forecast when managers have already seen and potentially overweighted the system forecasts before making their adjustments, and how does this method generally impact the adjustments?",
        "5fcceda7-7888-45b9-a43d-1d6f3570b655": "According to the text, what is a limitation of bootstrap rules in forecasting, particularly in cross-sectional forecasting problems, and when have they been shown to perform worse than raw judgments?",
        "020611fd-adaf-497c-b951-31532b92a71a": "According to the text, what is \"error bootstrapping\" and why is it considered more likely to improve forecast accuracy than conventional bootstrapping in the context of the companies studied?",
        "0dca39f3-1d86-49fe-88d1-c2ac0b30f28b": "The text mentions two models that were analyzed: the full model and the optimal adjustment model. Describe the key components or cues incorporated into each of these models.",
        "0c6c339b-6f1b-46ac-ae9d-a5e9ad92a7ba": "According to the text, what is the impact of preventing forecasters from making smaller adjustments when positive adjustments are made, and why?",
        "72e2f601-d940-44a2-aded-6d86d6e240ee": "According to the text, what is the impact of removing smaller negative adjustments on accuracy, and why does this occur?",
        "ddd539f2-3184-45ca-b43d-d5b794c819a3": "Based on the provided context, here is a question for an upcoming quiz/examination:",
        "a8ff838b-3a07-4979-80cd-84bbe50a94e6": "What methods did the researchers use to gather information about forecasting processes from the companies, and who were the different types of staff they met during these interactions?",
        "a1be058e-b352-4427-8eff-3950b429e1ce": "According to the article, what were some of the identified shortcomings of the forecasting support systems (FSS) in the companies studied, and how did these shortcomings potentially impact forecast accuracy and the ability of forecasters to learn from experience?",
        "787d7c77-8319-4df5-a7f3-e28ad27d7e94": "According to the text, what are the primary reasons identified by Moon et al. (2003) for the often flawed process of gaining market intelligence?",
        "c072666c-d8ef-4cc5-945d-b6562876b39e": "What are some of the limitations of the study mentioned in the article, particularly concerning the companies involved and the nature of the forecasts examined?",
        "4a3c126f-1d68-49fb-a9e5-c017c46d41cb": "What are two potential causes for the observed over-optimism in forecast adjustment that the authors suggest need further investigation?",
        "36888c0e-8a40-47e3-80b1-25df552cbb39": "According to the article, what factors might prevent the effective implementation of new forecasting system facilities, and what type of research is suggested to address these issues?",
        "e685e769-382d-4f25-93b0-7e700c252e49": "Based on the provided text, what are two key strategies companies could implement to substantially improve their forecasts, particularly concerning judgmental adjustments and market intelligence?",
        "989e63a9-126e-4a5c-a740-de15b9c5bbd7": "Based on Table A.1, for Organizational Group A-C with positive adjustments, what are the coefficients for \"System forecast\" and \"Adjustment\" in the M3 model, and what does the text imply about the forecasters' response to market intelligence in this scenario?",
        "6d09f340-4f06-4198-a3a3-22695802ab33": "What is the implication of positive versus negative information on forecaster adjustments and final forecast accuracy, and how does this compare to the retailer's situation?",
        "6d23580b-a335-4233-916f-6a2e61f2c4b8": "According to the provided text, which authors discussed the role of future focus in optimistic time predictions in their 2003 work, and in which journal was it published?",
        "3520e313-d7c1-4a64-8c34-6c1e056f4f0d": "According to the provided context, which authors explored the concept of \"illusion of control\" in relation to decision-making and forecasting?",
        "73dc2856-fc3e-4f24-8a3d-8da82f94df3b": "Based on the provided references, what are some of the key themes or areas of research that J. S. Lim and M. O'Connor have focused on in their work related to forecasting?",
        "915528af-c3f3-4532-bc2a-633fee1c9d12": "Based on the provided references, what are some of the key themes or areas of research that B. P. Mathews and A. Diamantopoulos have explored in their work on forecasting?",
        "84c87b3a-3f7e-4f46-82f0-38f38f6d876e": "Identify two individuals mentioned in the text who have served as Editor-in-Chief of the *International Journal of Forecasting* and briefly describe their other significant contributions to the field of forecasting.",
        "13742d7e-3669-4bdb-91ca-96a54c6a1cb6": "Who are the authors of \"La Gu\u00eda Completa sobre Machine Learning en la Previsi\u00f3n de la Demanda en Retail\" and what are their respective roles and qualifications?",
        "85dff050-73e5-40a6-a1fe-8737dd8db5f3": "According to the \"Contenido de la Gu\u00eda,\" what are some of the challenges in retail demand forecasting that machine learning can address, and what other factors, both internal and external, are mentioned as affecting sales?",
        "3f158e77-0f6e-48c7-b5c5-858bc6464da3": "\u00bfCu\u00e1les son algunos de los factores internos y externos que afectan la demanda en el sector minorista, haciendo que la generaci\u00f3n de pron\u00f3sticos precisos sea un desaf\u00edo para los planificadores humanos?",
        "0b3de7b7-7168-40f4-a011-7fa012fc2ba5": "Seg\u00fan el texto proporcionado, \u00bfcu\u00e1l es el t\u00edtulo del documento y a qu\u00e9 sector se dirige la pregunta principal?",
        "97f41b7b-1363-4442-9029-f50ce86d8275": "\u00bfCu\u00e1l es el principal beneficio de un sistema de aprendizaje autom\u00e1tico en el sector minorista, seg\u00fan el texto?",
        "9bc285be-8b5f-4321-9155-16e1cd8c47e2": "\u00bfQu\u00e9 factores actuales han permitido que los algoritmos de machine learning sean m\u00e1s efectivos que en el pasado, a pesar de no ser una tecnolog\u00eda nueva?",
        "73ac4dd2-8743-4d40-ad3c-47f0698204c4": "\u00bfQu\u00e9 tipo de retos aborda el Aprendizaje Autom\u00e1tico en el sector retail, seg\u00fan el texto?",
        "6930a6c2-8219-49cf-aac3-298191aff77d": "\u00bfCu\u00e1l es la principal ventaja del aprendizaje autom\u00e1tico sobre los modelos de series temporales para la previsi\u00f3n de la demanda, seg\u00fan el texto?",
        "12b202e5-fc75-4e39-a719-56acf60480b4": "\u00bfCu\u00e1les son las ventajas del Machine Learning en la previsi\u00f3n de la demanda del retail, seg\u00fan el texto?",
        "1e7248b3-45a6-42ea-89d3-5ecb04f9f2fe": "Seg\u00fan el texto, \u00bfqu\u00e9 factores adicionales a la elasticidad de precio deben considerar los retailers al ajustar los precios para reflejar el impacto completo de los cambios de precios?",
        "57a47a18-a394-41c0-9080-291ce2201581": "\u00bfQu\u00e9 es la canibalizaci\u00f3n de ventas y por qu\u00e9 es importante considerarla al hacer previsiones de demanda, especialmente para productos frescos, seg\u00fan el texto?",
        "913fbe80-93ee-4f49-b9fa-f733150aca51": "\u00bfPor qu\u00e9 es dif\u00edcil ajustar manualmente los pron\u00f3sticos para productos canibalizados en el sector minorista, y c\u00f3mo pueden ayudar los algoritmos de aprendizaje autom\u00e1tico en esta situaci\u00f3n?",
        "567d2b9e-79dc-4369-9c0e-b1a3d2e4d492": "Aqu\u00ed tienes una pregunta para el examen:",
        "b738d220-30c9-43e9-8812-f2f7f541b8c3": "Adem\u00e1s de los datos meteorol\u00f3gicos, \u00bfqu\u00e9 otras fuentes de datos externos se mencionan en el texto que pueden utilizarse para mejorar los pron\u00f3sticos de demanda en el retail, y c\u00f3mo RELEX ayud\u00f3 a WHSmith a mejorar sus pron\u00f3sticos utilizando datos externos?",
        "3d92a995-ce2b-4599-8fc8-29d2a27f88d7": "\u00bfC\u00f3mo puede el aprendizaje autom\u00e1tico ayudar a los minoristas a adaptarse a los nuevos niveles de demanda cuando los cambios en la exposici\u00f3n de un producto no se registran en los datos maestros?",
        "b61ac721-466f-4638-99be-34d261702082": "\u00bfCu\u00e1l es el tema principal que se aborda en el texto y c\u00f3mo se relaciona con la gesti\u00f3n de la demanda?",
        "2be77e43-c7fb-429a-91fe-226b9f4fa56e": "\u00bfCu\u00e1les son los desaf\u00edos espec\u00edficos que enfrentan los retailers al usar el aprendizaje autom\u00e1tico con productos de baja rotaci\u00f3n, y c\u00f3mo el sobreajuste de los algoritmos puede afectar la precisi\u00f3n de las predicciones de demanda en estos casos?",
        "deaf8337-7088-4461-884c-87e2e0b3b8b6": "According to the text, what are three important considerations for machine learning algorithms when dealing with low volumes and dispersed data at the product-store/channel level in retail?",
        "9a270e1f-9b73-4e17-92e6-2d27d7e27ff9": "Seg\u00fan el texto, \u00bfpor qu\u00e9 los planificadores siguen siendo necesarios para guiar el sistema de previsiones de demanda y reabastecimiento, incluso con la automatizaci\u00f3n?",
        "2306d947-508a-4a41-b550-6e1acfb888db": "Seg\u00fan el texto, \u00bfpor qu\u00e9 es importante que los planificadores entiendan c\u00f3mo se produce un pron\u00f3stico y puedan ajustar los c\u00e1lculos usando su experiencia, especialmente en el contexto de la inteligencia artificial?",
        "b809969d-bb22-4128-907f-0311f0dfd0d9": "\u00bfQu\u00e9 procesos principales de planificaci\u00f3n est\u00e1n aplicando los retailers m\u00e1s avanzados con inteligencia artificial para mejorar la rentabilidad y sostenibilidad?",
        "7a961aa2-ff2a-486d-8c58-d6e15a249d4a": "\u00bfCu\u00e1les son las tres \u00e1reas principales del comercio minorista que RELEX Solutions ayuda a optimizar, y qu\u00e9 tipo de soluci\u00f3n ofrece para lograrlo?",
        "2abefde1-7851-4c1c-a405-6511c38d98e9": "Here is a question based on the provided context:",
        "8ba502e8-fe77-4094-8aea-e617620066f7": "According to the text, what are some of the key differences in evaluation practices between traditional machine learning/deep learning tasks (like classification and regression) and forecasting tasks, and why is proper forecast evaluation particularly important in the current era?",
        "47ff6045-269e-454b-a569-26b6fb0a07a6": "According to the text, what are some common pitfalls and inadequate practices observed in the evaluation of forecasting models within AI/ML research, particularly concerning data partitioning, error measures, and benchmark comparisons?",
        "003c54b2-17ef-46bc-9e74-f249d9c5c52f": "According to the provided text, what are the two main setups for out-of-sample (OOS) evaluation in forecasting, and where can a visual representation of their difference be found?",
        "9ecc914a-f1ed-4b4e-899c-ba3a7d133109": "What are the key differences between a fixed origin setup and a rolling origin setup in time series forecasting, and what are the alternative names for the rolling origin setup?",
        "e8c252ab-3f9b-4b09-81eb-9e800dc6b4d6": "Explain the key differences between fixed origin and rolling origin setups for forecast evaluation, as illustrated in Fig. 2, and discuss how the testing data instances are handled in each setup.",
        "02d11b55-16de-4ad5-9d9d-d4a8f842ba9c": "According to the text, what is a \"structural break\" and how does it relate to \"sudden concept drift\" in an ML environment?",
        "d0c9a7f6-5870-4d55-a91d-a85e77179fa2": "According to the text, what is the \"na\u00efve forecast\" also known as, and how is it defined? In what specific scenario does it demonstrate competitive performance, and why is it considered the \"theoretically best forecast\" in that scenario?",
        "f6af7927-a13d-425a-9752-9711564b11f5": "What is the theoretical best forecasting method for a random walk simulated time series, according to Table 1 and the accompanying text?",
        "5aa1f8f1-45c2-4152-8b39-0e249eb6470a": "According to the text, what is a significant challenge in forecasting financial time series like exchange rates, and what benchmark model is often considered the most challenging for exchange rate forecasting?",
        "6e9bd473-3372-4724-a8c0-384ec2141236": "Based on the provided text, here is one question for your quiz/examination:",
        "a7f74465-b38f-44e8-9bc4-f608ad23e85e": "According to the text, what are two fundamental issues with forecasting exchange rates, and what specific economic hypothesis is violated by attempting to forecast them?",
        "08a2ecc4-d1be-4005-83c1-9d935910824a": "Here is a question based on the provided context:",
        "baa603f1-ec1d-4fe5-9617-9ca775462980": "According to the text, what are two common problems in the ML-based forecasting literature regarding dataset usage and evaluation measures, and provide an example for each problem from the text?",
        "1e7e1d9a-0b8b-4475-a6df-a5645252416d": "Explain why using forecast plots alone can be misleading when evaluating time series forecasting models, providing examples of how visual appeal can deceive evaluators, and discuss the importance of appropriate benchmarks and evaluation metrics.",
        "91fa9e99-ae12-414c-9a2f-b8930cec42fc": "According to Table 5, which forecasting model (Na\u00efve with Rolling Origin, Na\u00efve with Fixed Origin, or ETS) has the lowest RMSE, and what does this suggest about its performance in the scenario described?",
        "f3cd0e50-571e-4102-be24-be40edecc833": "According to the text, what is data leakage, and why is it considered a more challenging problem to avoid in forecasting compared to other machine learning tasks like classification or regression?",
        "f310eb66-fe6b-4e3a-857e-86987f9f17fb": "Explain how data leakage can occur in forecasting, providing specific examples from the text, and describe an experiment conducted to demonstrate its effect on a random walk time series.",
        "3420d3cd-7071-4d7b-8aa8-7a1e7ef9c3cb": "According to the text, what are three common scenarios or situations where data leakage dangers can occur in self-supervised forecasting tasks, and what is the recommended approach to avoid leakage in each of these scenarios?",
        "c281e395-25f7-447c-9a73-43b46fcc31fe": "Compare and contrast the \"fixed origin setup\" with \"rolling origin, time series cross-validation (tsCV), and prequential evaluation setups\" for forecast evaluation, discussing their advantages and disadvantages based on the provided text.",
        "3bae0033-b03e-4666-bc97-7b688a64d319": "Explain the key differences between the Expanding Window and Rolling Window setups for rolling origin evaluation, including when each approach is typically preferred and how they handle the training data over time.",
        "ae08e1f4-ef87-4f99-8c2f-28b54cab2582": "What are the main criticisms of using a randomized cross-validation scheme for time series data, and under what specific conditions can it still be effectively applied to pure AR models?",
        "6a7900a1-9f01-45e3-857a-0cf30f3dfe89": "Here is a question for your quiz/examination:",
        "00c28c7e-d867-4462-a04f-6baba010eb93": "According to the text, what is the most adequate cross-validation strategy for non-stationarities, and in what situations might even this strategy be misleading?",
        "5663b875-92fa-481b-b284-21cfb4ebaf7d": "Explain the difference between scale-dependent and scale-independent error measures in forecast evaluation, providing examples of each from the text and discussing why scale-independent measures are often preferred in business contexts.",
        "f32eed23-f8f3-4120-bac2-e9ae15320a1c": "Discuss the different approaches to scaling in forecast evaluation and explain why selecting an appropriate error measure is crucial. Additionally, elaborate on the controversy surrounding the evaluation of the same forecasts using multiple error measures, citing different perspectives presented in the text.",
        "dc5a85ed-bf83-417e-aa78-c6e83d3a28fe": "Explain the difference between the \"Percentage error (In-sample scaling)\" and the \"Percentage absolute error (In-sample scaling)\" as defined in the provided text, including their respective formulas and how they are named in the work of Petropoulos and Kourentzes (2015).",
        "d3e7cb6a-fccc-4d4f-9b6c-ff015d87458f": "Explain why percentage-based error measures may underestimate errors at peaks and overstate errors at troughs in series with seasonality, and suggest a method to overcome this issue.",
        "d7a7d8f7-8c02-4858-9873-1d3099d9a794": "Define the Geometric Root Mean Squared Error (GRMSE) and the Geometric Mean Absolute Error (GMAE) based on the provided table.",
        "faad5ecf-b598-4141-8f22-8b4409c4b144": "What is the definition of the Mean Arctangent Absolute Percentage Error (MAAPE) as provided in the table?",
        "e0b4aad3-4191-4726-9c32-681572efc50f": "According to the provided table, what is the definition of the Normalized Root Mean Squared Error (NRMSE) and how is the scale in its denominator computed?",
        "d5270374-616a-453a-a1ed-0120e35a7237": "Compare and contrast the definitions of Relative Root Mean Squared Error (RelRMSE) and Root Relative Squared Error (RSE), highlighting their key differences in how they incorporate benchmark methods or baseline errors.",
        "4aa509cc-c909-444a-981e-dcca52909ffc": "How is the Percentage Better (PB Score) calculated, and what does it measure according to Hyndman and Koehler (2006)?",
        "6dcd67f5-9de3-4f2a-99fb-ac55079e4e48": "Discuss the challenges of forecast evaluation for intermittent series and how these challenges can be addressed. Additionally, explain how forecast evaluation measures can be made robust against outliers, contrasting this with scenarios where capturing outliers is desired.",
        "ee5bde17-bad4-46a0-9fd8-044817953d6f": "Based on Table 9, if a time series exhibits stationarity, count data, seasonality, a linear trend, unit roots, heteroscedasticity, structural breaks, intermittence, and outliers, but *not* scale differences, which error measure is recommended for final forecast evaluation, and what scaling, forecast horizon, training region, and forecast origin are associated with it?",
        "ae3bdcdb-9f2e-462e-980f-78d1067e8c66": "Based on Table 9, what are the key characteristics of the \"Stationary\" data type, considering its properties related to seasonality, trend, unit roots, heteroscedasticity, structural breaks, intermittence, and outliers?",
        "75f8914c-336d-4a30-a3a2-9cb771882748": "What statistical tests are mentioned for comparing the mean performance of more than two methods, and what assumption do they share?",
        "331d7f81-65b0-4892-b801-5e79d0389291": "What is the purpose of a Critical Distance (CD) diagram, and what information does it convey regarding the comparison of different methods?",
        "6552a900-3f1a-4f75-84d9-42b047a2c9a6": "According to the text, what factors can influence the results of significance tests, particularly regarding the Critical Difference (CD) and the perceived significance of model differences?",
        "7bb975c4-f1fb-4a05-ab1a-45460352a973": "According to the provided text, what are some of the key best practice guidelines for forecast evaluation that the authors propose to support the ML community?",
        "09cd4d33-01cd-4241-94a5-62a522c97cd8": "According to Hewamalage et al., what is the proposed benefit of designing combination-based evaluation measures for forecasting, and what existing measure is cited as an example of this approach in model training?",
        "a0cfd0e7-b100-4d64-91e7-6497558f52a3": "According to the provided text, who authored \"Data preparation for machine learning: data cleaning, feature selection, and data transforms in Python\" and in what year was it published?",
        "3b26892e-79ab-4cf0-8a35-eb0ed136d4c2": "Which authors contributed to research on \"Learning in nonstationary environments\" and \"A survey on concept drift adaptation,\" and in what years were these works published?",
        "40007f7a-d098-43ea-b8a1-ef19bd6e745d": "Which authors investigated \"Tests of conditional predictive ability\" and in what year was their work published?",
        "eb29bd23-70bf-40b0-baaa-d24dde0daa84": "According to the provided text, which authors published \"Forecasting: principles and Practice, 2nd edn.\" in 2018, and what is the associated URL?",
        "fa1fc746-76da-4405-a724-00d0e6d00b42": "According to the provided context, which publication details the M5 accuracy competition, and what are its key findings and conclusions?",
        "0229ba2e-0659-40b1-bb67-47965951b3ac": "Which research groups or individuals have explored the application of transformers for time series forecasting, and what specific variations or improvements have they proposed (e.g., iterative multi-scale refining, exponential smoothing, decomposition with auto-correlation)?",
        "68bab876-0ea7-412f-b7a9-eb48bfc6eb80": "Which publication venue hosted the \"26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining\" and the \"28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,\" and what are the page ranges for the respective papers by the Association for Computing Machinery?",
        "4640118b-4da7-4db3-8507-50918d6da389": "Based on the provided text, what is the title of the paper authored by Zhou T, Zhu J, Wang X, Ma Z, Wen Q, Sun L, Jin R (2022c), and what is its arXiv identifier?",
        "15c7f33f-8dcd-4871-9912-ec7be0f5306f": "Based on the provided information, what is the title of Valery Manokhin's PhD thesis, and when was it published?",
        "ee0ce1c0-4e88-4dd7-9277-74bd1583db3e": "Who are the supervisors for Valery Manokhin's Ph.D. thesis and what is the title of the thesis?",
        "f475f776-941a-4358-8fed-8bd172e38472": "Based on the provided context, what is the significance of the number \"2\"?",
        "912713a7-9fa0-456c-86b1-7e6688c6d18f": "**Based on the \"Declaration of Authorship,\" who is declaring that the thesis and the work presented in it is entirely their own, and on what date was this declaration made?**",
        "f4f2c554-0ba8-40d5-8994-01b4a9ccac81": "According to Mark Twain in \"Following the Equator,\" what is his perspective on the profession of prophesy?",
        "ea52b092-0a69-45b4-8b42-3b6262ca72d0": "What are the main limitations of classical and modern calibration methods as described in the abstract?",
        "2c5d5a04-aa71-4ed3-8152-8dc3e44535a7": "What is the number provided in the context?",
        "27a4b239-6ad7-4fec-84cd-f16a37d1e55e": "Who are the individuals acknowledged for their support and guidance during the PhD research?",
        "ce95ec81-6472-44ca-a8f1-ed42b3c29077": "What is the numerical value presented in the provided context?",
        "6919b019-9392-49a6-be7a-53246cd2fe5d": "**Question:** What are the main topics covered in the \"Introduction\" section of the document, and what specific sub-topics are discussed under \"Machine learning\" and \"Probabilistic prediction\"?",
        "331c1ced-a458-468e-9950-4a89bcefc2d1": "What are the computational details of Inductive Venn-Abers Predictors (IVAP) and Cross Venn-Abers Predictors (CVAP)?",
        "565996d9-b8d1-467a-836e-0eb228100f5e": "What are the page numbers for the \"Waveform data set\" and \"Abalone data set\"?",
        "1b840020-3c32-4d46-9688-788ab8377543": "What are the key differences and similarities between the \"Validity of the LSPM in the online mode\" (Section 4.7) and \"Validity of the LSPM in the online mode\" (Section 4.7)?",
        "58dfdc75-896e-4b7d-9f73-a311c86aba25": "What is the numerical value presented in the provided context?",
        "087ae2c9-2b1c-4393-8ade-bf0e2c90f762": "What are the page numbers for the figures related to the Boston Housing data set in Chapter 6, and what specific aspects of performance or calibration do these figures illustrate?",
        "1ff263f3-b890-4f59-99af-38228a951978": "**According to the provided context, which two data sets are used in sections 6.10 and 6.11 for calibration curves?**",
        "05aec974-cb15-4f6d-979e-733ce9879a24": "What is the page number for the table showing the Brier loss for the `abalone` data set?",
        "dd449b69-5d43-4907-961d-c8967b8f964a": "What is the numerical value presented in the context?",
        "3358b0f5-13a8-4773-9da7-ab0dea7c8ec7": "What do the abbreviations CP, CPD, and CPS stand for, and how do they relate to each other based on their names?",
        "7703e75e-4ddc-4614-a44d-0c5d94c68d4b": "What is the number provided in the context?",
        "88e5477c-2ebf-4680-95c3-a669c15b82d8": "What are some examples of machine learning applications mentioned in the text?",
        "680b57d7-38cd-4687-aa7e-02656a36c441": "Differentiate between classification and regression problems within supervised machine learning, providing examples for each.",
        "50bd4267-9369-4210-a7cd-b1a284a277d0": "Here is one question based on the provided context:",
        "bb100f55-b3c5-4247-a8e6-1a7c6b4a8b1f": "According to Professor Emmanuel Candes, why is it extremely important to understand the uncertainty about predictions made by predictive models?",
        "d2757dc9-33d4-4956-9ab6-e3eaad86213f": "According to Gneiting and Katzfuss, what is the general objective of probabilistic forecasting, and how are \"sharpness\" and \"calibration\" defined in this context?",
        "ff895aac-dc32-47cb-8747-56a77c716a49": "Compare and contrast the Logarithmic Loss and Brier Loss functions. For what type of machine learning problem are they typically used, and what property do they share?",
        "18a82f69-788d-4d86-90cd-2e7b083f77d3": "Explain how a conformal predictor constructs a prediction region \u0393\u03b5, detailing the role of the significance level \u03f5 and conformity measure. Additionally, describe the historical origins of conformal prediction, tracing its ideas back to earlier concepts and fields of study.",
        "26cd582c-6ce9-45dd-8c88-d03b04555df5": "Here is a question based on the provided context:",
        "69889ce1-f92c-43ed-8bf6-1e32f249cf83": "What are the main advantages of Kernel Ridge Regression Prediction Machine (KRRPM) and the split and cross conformal predictive distributions (SCPS and CCPS) as discussed in the text?",
        "4e1fc80f-6cea-49ff-8517-b3b759433805": "What is \"Awesome Conformal Prediction\" and what is its significance in the context of the ICML2022 \"Workshop on Distribution-Free Uncertainty Quantification\"?",
        "de678e85-2d8e-478c-9b9e-8f26025dee89": "According to the text, what is the primary objective of a supervised machine learning classification task, and why is the generation of accurate, reliable, and robust class probabilities considered a desirable property of such an algorithm?",
        "1a86e287-1680-4afa-8559-f5ecda761f60": "According to the text, what is calibration in the context of machine learning models, and why is it becoming increasingly important?",
        "6f1249dd-1f36-49a5-bf96-f2868f0eb0e3": "According to the provided text, what are some of the identified issues with model calibration and confidence in both Bayesian methods and deep neural networks, and how have researchers attempted to address these issues?",
        "fccc2cff-dd45-459b-a908-054f3fd61d75": "According to the text, what were the key findings of the study by Perreira et al. regarding the performance of Platt's scaling, isotonic regression, Conformal Prediction, and Venn-Abers prediction in producing individual probabilistic predictions for clinical decisions?",
        "e87a0f8e-d6eb-4676-a16c-ee8bafdca9fc": "Explain the concept of a \"perfectly calibrated classifier\" as defined in the provided text and discuss why this is an intuitive goal for model output.",
        "120a717b-a62a-4034-bfa3-7681a0644e42": "According to Platt, what is the relationship between classification scores produced by the SVM and empirical probabilities, and how can this relationship be described mathematically?",
        "bac60919-79c0-495d-a509-8ac77033eee6": "How does Zhang's piecewise logistic regression method extend Platt's method, and what advantages does this extension offer, particularly in terms of classifying objects into different certainty levels?",
        "d6ed47e5-8005-42eb-8b21-29b50ee20f8d": "What is the major drawback of isotonic regression as a calibration method, and what assumption leads to this drawback?",
        "a10a10dc-0947-40bd-a97f-83e06fed8949": "According to the text, what are two specific assumptions made by Platt's scaler that Beta calibration addresses, and how can these assumptions lead to poor calibration results, particularly with certain classifier outputs?",
        "f99f5a0e-a5a4-45cc-ac7b-954877751915": "According to the text, what are the limitations of approaches like Platt's scaling and isotonic regression in terms of measuring their true calibration error, and how does the scaling-binning approach address some of these limitations?",
        "7760390b-4900-4e80-8083-0262d5b08c68": "What is the primary hypothesis behind Probability Calibration Trees (PCT) and how does it aim to improve upon global calibration methods like Platt's scaling and isotonic regression?",
        "9a264dd6-71ae-4e28-82c0-5d7247a066af": "Here is a question based on the provided context:",
        "07f7fd16-9b40-433c-9146-5e3611e94159": "According to Guo et al. [44], what factors contribute to the miscalibration of deep convolutional neural networks, and how does temperature scaling address this issue?",
        "1fe97267-3efe-417d-8abf-f137fcf0a189": "According to the provided text, how do overconfident predictions in deep neural networks relate to output distributions, and what techniques are mentioned to mitigate these overconfident predictions, leading to better generalization?",
        "4ad84564-c2ad-4eb9-baf8-e7eeb300ca6a": "What is label smoothing, and how does it address overconfidence in deep neural networks? Explain its mechanism in minimizing cross-entropy and discuss its observed benefits in terms of performance, generalization, and calibration, referencing the findings of Mueller et al. and its impact on NLP tasks.",
        "6ed5554e-89e4-467b-a873-e32aa62d6453": "According to the provided text, what is the primary purpose of the modulating factor (1 - pt)\u03b3 in Focal loss, and how does its effect change as the model's confidence (pi) increases?",
        "1e002307-8b70-46c5-ad4b-11377714890b": "According to Mukhotiet al. [94], what are three interesting findings related to the training of neural networks, and how does focal loss address one of these issues?",
        "f41dd11b-28e6-4e6b-86ec-16c92a2a6457": "What are Venn predictors, and what are their main characteristics and advantages as described in the text?",
        "6ac903e6-c806-42f2-9421-729ed1e5032c": "What are two limitations of isotonic regression for probabilistic calibration, as discussed in the provided text?",
        "9d76c587-0ea4-4237-a16f-9d7423833f4f": "What is the number mentioned in the provided context?",
        "3ed3bc47-8639-4a0c-9d79-4baed79cc7e3": "According to the text, what is the primary objective of an \"ordinary\" classification problem, and how does a probabilistic classifier differ in its output and utility?",
        "0116c9fc-fcbd-4574-9366-d9def0b35f12": "According to the text, what are some of the disadvantages of Platt's scaling, and for which type of classifier was it originally developed to address distortions?",
        "502cd5df-7e7f-4c56-8e1d-79c593fc388b": "Compare and contrast Platt's scaling and isotonic regression as methods for probability calibration, discussing their respective advantages, disadvantages, and specific scenarios where one might be preferred over the other based on the provided text.",
        "c88b8901-3d45-4b6c-8e59-8bed5b37c7dc": "**Compare and contrast IVAP and CVAP, highlighting their relationship to isotonic regression and Venn-Abers predictors, and discuss their advantages in terms of overfitting and calibration.**",
        "b4f6bf21-1431-42b5-b060-0d602e4c74e1": "Compare and contrast the \"one-versus-all\" and \"one-versus-one\" methods for multi-class classification, discussing their respective approaches to building binary classifiers and the implications for computational efficiency based on the time required to build a single classifier.",
        "035ea650-0385-4b1e-8df4-d835cb8d61f2": "Here is a question based on the provided context:",
        "24dff502-7c1d-4a20-9ad4-e7f9b9fa9507": "Explain the relationship between Venn predictors, Venn-Abers predictors, IVAP, and CVAP, highlighting their key characteristics and how they address the challenge of estimating true conditional probabilities.",
        "84a22574-578c-4a9e-b5e3-67da3b274a98": "**Explain the purpose of applying the function g(s(x)) in binary classification problems and describe the key property this function should possess to maintain the integrity of the underlying classifier's ranking. Additionally, identify the most widely used algorithm for learning this function and briefly describe its operational principle.**",
        "68e2492b-1254-416c-857a-fee714a701e5": "Here is a question based on the provided context:",
        "c6b78056-146c-470c-8fa0-e7beb84aed3c": "Explain how Inductive Venn-Abers predictors (IVAP) compute a multi-probability prediction (p0, p1) for a test object, detailing the role of isotonic regression and the two sets of dependent variables.",
        "fdecfc33-1ffc-4caa-b336-e590930896b1": "What are the underlying algorithms used in the experiments on multi-class data sets, and which of these algorithms are not available in Scikit-learn?",
        "8af71851-c45c-43c8-9287-44e37001819d": "The authors describe a specific data splitting strategy for their experiments. Explain this strategy, including the proportions of data allocated to the training, calibration, and test sets, and mention any specific tools or parameters used to ensure reproducibility.",
        "39aa8bb8-29dc-43b7-aab1-9dfb9f659c38": "For the `waverfont` dataset, which calibration method resulted in the lowest Log loss for Na\u00efve Bayes, and what was that value?",
        "666d5b86-3dd6-4671-b0c7-670e292ec7cb": "According to Table 3.3, which classifier achieved the lowest Log loss for the `satimage` data set when using the `CatBoost` calibration method?",
        "5b90d322-9971-4ebb-94c7-0de041b31cf3": "Here is a question based on the provided context:",
        "703ab6bf-e844-4029-a864-3a5ba3ad5a6d": "Compare the performance of \"logistic regression\" and \"Random Forest\" across both Log loss and Brier loss metrics for the \"vehicle\" data set, considering all calibration methods (sigmoid, isotonic, ivap, cvap). Which model generally performs better and under what calibration method?",
        "7c0e8c7c-1c76-4d70-b025-518e0f3167dc": "For the Balance scale data set, which calibration method (sigmoid, isotonic, ivap, or cvap) consistently yields the lowest Log loss across the majority of the machine learning models listed in Table 3.7?",
        "ef758cdb-e388-4bf9-96f4-d130a60c6857": "Based on the Brier loss, which classification models show calibration improvements when using CVAP?",
        "cb51aa4e-3e78-4d3a-a22d-dff1e2e7502f": "Based on the Log loss for the pasture dataset, which calibration approach delivers the best result for Random Forest, and which approach is best for neural network?",
        "68493b55-f2f3-42cf-a864-d8b2eddbfc9a": "Based on the Brier loss for the pasture dataset, which calibration method (sigmoid, isotonic, ivap, or cvap) resulted in the best performance improvements for the majority of the ten classifiers, and for which two classifiers did isotonic regression deliver the best calibration result?",
        "1304ab1f-8150-4723-8bf5-b786d9ed64e6": "Based on the provided text and Table 3.14, which calibration method (sigmoid, isotonic, ivap, or cvap) resulted in performance improvements for the most classifiers when evaluated using the Brier loss, and for which specific classifiers were these improvements observed?",
        "93872471-ecbb-4835-b001-ac694e430aa3": "What is the main contribution of the chapter regarding probabilistic classification, and how do the proposed multi-class predictors compare to classical calibration methods?",
        "fd7c196c-9486-47aa-a04a-e22ce440754d": "How does the approach to predictive distribution functions discussed in Chapter 4 differ from traditional statistical methods, particularly in terms of underlying assumptions and model specification?",
        "a6357e03-caa6-4e8b-9cef-e57868675480": "According to the text, what is the primary focus of Chapters 4-6 in the context of probabilistic prediction for machine learning regression tasks?",
        "6798f379-c248-45c2-8389-7e8f5c8a4802": "According to the text, what is the primary difference in the approach to constructing predictive distributions between the methods described in Lawless and Fredette [73] and Shen et al. [121] on one hand, and the methods developed in the current chapter using the conformal prediction framework on the other?",
        "fa57f66f-ad7c-46a2-a587-8f009a13b911": "**Question:** Define a Randomized Predictive System (RPS) based on Definition 4.4.1. What are the three requirements an RPS must satisfy, and briefly explain each one?",
        "2e0663c5-3bca-4e4e-b16f-3b6ad9592dde": "Here is a question based on the provided context:",
        "7118f60e-d6b6-475f-b658-600cfebb8484": "What is the purpose of Function A as described in the text, and how is it exemplified by the conformity measure in equation (4.6)?",
        "a6e48a2f-3c79-4af6-b049-3463cb5df342": "Here is a question based on the provided context:",
        "e6253e61-53e5-4605-bea6-022c136b3048": "Here is one question based on the provided context:",
        "01bd9e6f-d7d7-48c7-80ee-d56a37d1baa0": "According to the text, what two conditions does a conformal transducer defined using conformity measure (4.10) with $\\hat{y}_{n+1}$ defined by (4.11) satisfy, making it both a Randomized Predictive System (RPS) and a Conformal Predictive System (CPS)?",
        "4483f62c-42dc-4ca7-9888-9005ddda790a": "**Compare and contrast** the Ordinary Least Squares Prediction Machine (LSPM) and the Deleted LSPM, highlighting the key differences in their conformity measures and how the prediction for $y_{n+1}$ is computed in each.",
        "2aaed543-48bf-46de-81de-5534bcf6b359": "**Question:** What is the significance of the condition $\\bar{h}_{n+1} < 0.5$ for the ordinary Least Squares Prediction Machine (LSPM), and how does it relate to the concept of an \"influential point\"? Explain what happens if this constant is replaced by a larger value.",
        "16882a41-260d-436e-a1f8-2a625b168e43": "According to the text, what is an important advantage of using studentized LSPM, and what proposition supports its use as a predictive distribution?",
        "66984796-e15e-47a3-a0a9-2c2eeac14738": "Here is a question based on the provided context:",
        "242bd5b0-b934-4980-974c-dba06957d856": "Based on the provided context, here is one question for your quiz/examination:",
        "1b4924dc-f865-4be3-b4c5-461d7c8fe9bd": "Here is a question based on the provided context:",
        "cd322af3-2e3a-44c3-b5cf-ee60cd620041": "Here is a question based on the provided context:",
        "fb33e4af-e26f-4a98-a06b-329e65dc2b1f": "What are the key assumptions made about the data generation process and the characteristics of the input objects ($x_i$) and labels ($y_i$) for the efficiency results discussed in this section?",
        "0d9abf7a-7662-43c2-936e-055c9acd1265": "Compare and contrast the predictive distributions output by \"Proper Oracle I\" and \"Proper Oracle II,\" highlighting the specific information each oracle has access to and how that influences their respective outputs. Additionally, discuss the relationship between \"Proper Oracle I\" and its \"simplified version\" as well as \"Proper Oracle II\" and its \"simplified version,\" noting any practical implications mentioned in the text.",
        "e29af0fc-e7ad-4136-9ce4-a16325d84932": "**Compare and contrast the definitions of the random functions $G_n(t)$ in Theorem 2 and Theorem 3, highlighting the key differences in the predictive distributions being compared and the scaling factors involved.**",
        "80c44469-644c-42e9-97f3-ced484235160": "Based on the provided context, here is one question for your quiz/examination:",
        "5bc76e86-bbe3-4781-b3e9-a8791bf77101": "**Short Answer:** Explain the historical lineage of the Dempster-Hill procedure, tracing its ideas back to earlier statistical methods and identifying key figures involved in its development and naming.",
        "28a5069e-05ac-4242-8667-3c9b86a77909": "Here is a question based on the provided context:",
        "a3f1ce99-134e-4fe2-9539-642f7dbfaa43": "Here is a question for your quiz/examination:",
        "7b9f6ffc-b963-4713-90a0-f7dd53f11d47": "Here is a question based on the provided context:",
        "ca8ce895-ae57-4a75-b943-74bc8ecdb48c": "**Short Answer:** What is the main contribution of the chapter, as stated in section 4.10 Conclusion, and how does it differ from traditional statistical approaches to predictive distributions?",
        "3abf8cae-eec1-468b-acb4-4aeff0bb1889": "Here is a question based on the provided text:",
        "d23be4a3-301a-442d-af9d-dd9c934d9189": "**Question:** Based on Figure 4.6, describe the key differences between the left-hand and right-hand plots, specifically addressing what each plot represents and how the \"proper oracles\" are incorporated. Additionally, explain how the normalization process in the left-hand plot relates to the \"true distribution function\" mentioned in the text.",
        "4ab27502-9003-40d3-a0d8-097e6aecb119": "What is the number mentioned in the provided context?",
        "8cbc61d5-1d5c-4986-9d33-c33bafe386e6": "What is the primary focus of Chapter 5, and how does it build upon the concepts introduced in Chapter 4?",
        "aefb53ef-9f74-439e-a25f-d276ef199242": "The text mentions two key developments in adapting predictive distributions. Describe the \"first recent development\" and the \"second development,\" including the individuals and institutions associated with their origins.",
        "5332926e-bf10-47f0-b68d-dab4e3ba77d1": "What are the two main components of an observation $z_i$ in probabilistic regression, and what spaces do they belong to?",
        "492b1fef-44e7-467b-b865-a3dcdf9e3afd": "Here is a question for your quiz/examination:",
        "c2613af9-f827-4fae-8154-38baa90f50c9": "Compare and contrast the approaches of Fisher and Neyman to statistical inference, highlighting their common ground and their differences, particularly in relation to Bayesian methods and the concept of uncertainty.",
        "51cc1328-435d-409a-9755-a744b7fe22e6": "According to the text, what is the \"biggest blunder\" often attributed to Fisher, and how did Neyman's approach simplify the concept of validity in statistics?",
        "5aa5db40-281a-4500-8ee4-6371a0c458be": "**Compare and contrast** the \"deleted KRRPM\" and the KRRPM defined by conformity measure (5.4) and (5.5). Specifically, explain how the prediction for $y_{n+1}$ is computed in each case and what training data is used.",
        "d99858be-a9ce-4598-9da1-a751abff07ec": "Here is a question based on the provided context:",
        "104175db-9af5-404b-9a22-82c1b35c768a": "Here is a question based on the provided context:",
        "1c1b304a-10fd-4e7a-ae08-8105d5d6f640": "**Derivations and Explanations:**",
        "d9ea282c-fdbc-4da2-9251-7c85d10b22ec": "Based on equations (5.17) and (5.18), explain the relationship between `ad` and `\u00afhn+1`. What does this relationship imply about the value of `\u00afhn+1` given the range of `ad`?",
        "50724ae6-ecbf-400c-89e0-c9a66c66e5c8": "Here is a question based on the provided context:",
        "651eae6d-c6e8-424d-86d5-8ef779478f64": "**Analyze the limitations of the KRRPM:** The text states that the KRRPM makes a significant step forward compared to the LSPM but is still restricted. Discuss the nature of this restriction, specifically focusing on how the soft model (5.1) impacts the KRRPM, and how the ordinary KRRPM is used as a technical tool to understand this restriction. Furthermore, explain the implications of the Bayesian predictive distribution being Gaussian and its variance being at least $\\sigma^2$.",
        "1aa79707-5d33-443d-b54d-4210da0c22a0": "According to the text, what is the primary reason that the conformal predictive distributions output by the KRRPM are not universally consistent, and what is one proposed method to achieve universal consistency?",
        "a07132be-d91f-416f-962e-b5cab47ae244": "Here is a question based on the provided context:",
        "d128f7b2-b571-4196-a08e-72742958a936": "According to Figure 5.1, what is the impact of using a non-universal kernel (specifically, the linear kernel) on the conformal predictive distribution compared to using the true kernel or the Laplacian kernel, given a training sequence of length 1000?",
        "a7e28b82-3d94-47a1-a682-b2c06aa88720": "**Short Answer:**",
        "054e3d68-7c6c-433f-ad7c-6b2579ffca30": "What are the two types of kernels mentioned that are considered universal, and which type is explicitly stated as *not* universal, according to the provided text?",
        "351a5128-c1fe-4fd2-8982-3ecb46ed8412": "How is the continuous ranked probability score (CRPS) adapted for use with conformal predictive distributions in the provided text, and what specific modification is made to the Qn(y, \u03c4) function to enable its application?",
        "c4e33c2f-540b-46e6-9d64-b9758f0285a6": "According to Table 5.1, what are the parameters (degree and 'a' value) for the polynomial kernel that yielded the best cumulative loss, and what was that loss value?",
        "05075e9e-11e2-44b8-b0dd-9972d9742ffd": "According to the text, what advantages do the kernelized versions of CPDs offer, and which specific kernel is highlighted for its performance in generating accurate probabilistic predictions for test objects?",
        "9b4b5e86-2fee-4b91-a6eb-37ce4250143e": "What is the numerical value presented in the context?",
        "27a506e4-834e-4fda-b4a2-cc7789663fb7": "What are the two novel computationally efficient conformal predictive systems introduced in Chapter 6, and what are the main advantages and guarantees associated with each?",
        "af094357-9348-48ec-87aa-fea6ba70fc61": "Here is a question based on the provided text:",
        "772ade24-631f-4bf8-88e3-df35c83f1c41": "Here is a question based on the provided context:",
        "554124dd-202e-46c5-a5c9-18b7f83b7ff4": "**Explain the significance of an isotonic split conformity measure being \"balanced\" in the context of a split conformal transducer. How does this property relate to the set defined in equation (6.3), and what are the typical forms this set can take?**",
        "9f3136d1-ed9b-4ab6-8316-857b88717836": "Here is a question based on the provided context:",
        "1da11d71-2f42-4028-8aa6-08121ff0b952": "Here is a question based on the provided context:",
        "eb6c3d41-949b-483a-9a90-b9ab936b5005": "Here is a question based on the provided context:",
        "b32cfbac-131b-4de9-a070-a02d35d889ff": "Here is a question based on the provided context:",
        "6faf4e3d-6976-4239-8b48-e9d947522ecd": "Here's a question based on the provided text:",
        "016a877b-627b-4058-9201-3d6dea0f50c9": "According to the text, what is the lowest possible value the Continuous Ranked Probability Score (CRPS) can attain, and under what condition does it achieve this value?",
        "d9a5330c-ebdf-4359-bc2f-0eb7cf6d8ef4": "Based on Figure 6.1, describe what the split conformal predictive distribution (SCPD) represents for a random test object in the Boston Housing dataset. What do the blue solid line and red dashed line signify in this figure?",
        "f3a577bb-5319-42e9-9e9f-2e30a698f18d": "Here is a question based on the provided context:",
        "e498299c-6c63-4f74-a573-423a36a6b46f": "Explain the procedure used for experiments with SCPS and CCPS, detailing how datasets are prepared, how training and testing sequences are defined, and how the \"training sequence proper\" or \"K folds\" are determined for each method.",
        "ad19d31d-e22b-45b7-b08b-8e9d11f50b76": "Here is a question based on the provided context:",
        "6ec891a4-29a1-40b8-ac24-450b0a1ac989": "What dataset is Figure 6.3 an analogue of Figure 6.2 for, and what are the reasonable numbers of folds for CCPS in this dataset?",
        "f3d9f898-3a11-4fcb-8d19-fe99b4b850d1": "What is the significance of the \"m/n\" and \"K\" axes in Figure 6.4, and how do they relate to the performance metrics (CRPS (LS), CRPS (RF), CRPS (NN)) for the Yacht Hydrodynamics dataset?",
        "75221a42-eaa5-4756-8428-92f359602cfd": "What dataset, besides Wine Quality, is mentioned as being used for experiments, and what is the task associated with it?",
        "7081147b-d0cb-46c0-8884-d0bc5e58a16a": "Here is a question based on the provided context:",
        "baba3475-ffda-495c-8bc2-ba6e4986c0cd": "Based on Table 6.1, for which dataset and underlying algorithm does SCPS perform better than CCPS in terms of the median CRPS loss?",
        "c5c7ee58-dcee-4f91-a9b8-11b5edb30e10": "According to the text, what are the main advantages of Split Conformal Predictive Systems (SCPS) and Cross-Conformal Predictive Systems (CCPS), respectively, and under what conditions does the validity of CCPS hold?",
        "4855fdec-8d06-4e4e-8d98-66d353f940be": "**Analyze and Compare:** Based on Figure 6.8, compare the calibration curves for the Diabetes dataset when using SCPS (m/n=0.5) versus CCPS (K=5) for each of the following models: LS, RF, and NN. What general observations can you make about the differences in calibration performance between SCPS and CCPS across these models for this dataset?",
        "4270916d-1e8c-4b91-9e76-40cb5687caf9": "Based on Figure 6.9, what are the two different calibration methods being compared for the Yacht Hydrodynamics dataset, and for which machine learning models are these comparisons made?",
        "128fbe5f-01b0-4254-afd9-dd1291cd0141": "**Analyze Figure 6.10:** Describe the overall purpose of Figure 6.10 and identify the different types of calibration curves presented. What dataset are these curves associated with, and what specific parameters are being varied or compared in each subplot (e.g., m/n, K)?",
        "8e84aa52-0294-49b7-9ab5-adaf698b9d8c": "**Analyze the provided Figure 6.11.**",
        "517694a7-7e89-4b85-b004-127485725cf1": "What are some disadvantages of classical calibration methods like Platt's scaling and isotonic regression, and why is there a need for non-parametric machine learning methods in probabilistic prediction?",
        "aed5eb95-372c-4b33-9105-28a4c910c198": "According to the text, what are the advantages of using predictive distribution functions over traditional conformal prediction intervals, and how do the non-parametric predictive distribution functions introduced in Chapter 4 differ from the parametric approach in statistics?",
        "a76a5b69-fd27-4a40-86e0-82b3fccbaea4": "What are the two novel computationally efficient conformal predictive systems introduced in Chapter 6, and what is the main advantage of each?",
        "4b7adaca-24e7-404c-98e4-a5ca7cd4e162": "What is the number presented in the context information?",
        "1c1adbf7-0392-41e0-9fa0-4e6bb5067bf5": "Which authors contributed to \"Conformal Prediction for Reliable Machine Learning: Theory, Adaptations, and Applications\" and in what year was it published?",
        "b194f6a0-d2e4-4c97-a8b2-d4a0d7d8d2dc": "Compare and contrast the two entries for \"Efficiency of conformalized ridge regression\" by Evgeny Burnaev and Vladimir Vovk (entries [12] and [13]). What are the key differences in their bibliographic details, and what might account for these differences?",
        "34b25f09-e9e8-4add-8352-d598cb147508": "Which publication is a PhD thesis and from what university?",
        "cc6c3ed1-b22b-45c3-a2c0-707b94a3fc8e": "**Question:** What are the DOIs for the two works co-authored by Alexander Philip Dawid that are cited on pages 99 and 104?",
        "1d96afe3-45c2-45ab-ac40-e217af2676d5": "**Question:** Identify two distinct publications by Ronald A. Fisher from the provided bibliography, noting their titles, publication years, and the specific pages in the current document where they are cited.",
        "f59264be-5730-47ff-8c88-82ac0b8f954a": "Which publication by Tilmann Gneiting is cited on page 117 and what is its ISSN and DOI?",
        "56dae946-b5f1-45bd-89bc-750a86b38973": "Which publication is cited on pages 91 and 92, and what is its ISSN and DOI?",
        "50d325d7-8fe3-4155-97bf-336f90d430fa": "Which publication details the \"Gradient Theory of Optimal Flight Paths\" and when was it published?",
        "356b7962-430e-4a11-8830-49016cf53f6e": "Which publication by Alex Krizhevsky is a technical report from the University of Toronto, and in what year was it published?",
        "4a8daf28-e8ca-4b05-abb9-bf98cd298622": "Which publication details \"Trainable calibration measures for neural networks from kernel mean embeddings\" and who are its authors?",
        "96e6d5ef-ee1b-43ce-84fc-30d14f93cfcd": "Identify the authors and publication details for the paper \"Heteroscedastic gaussian process regression,\" including the conference, year, and page numbers.",
        "827d8276-a009-4898-82cd-19295792c765": "Which publication from the provided bibliography was cited on pages 129, 130, and 138, and what is its URL?",
        "b5899782-6159-4fe2-a3e5-1c26d8420a66": "Which publication, authored by Peter McCullagh et al., discusses \"Conditional prediction intervals for linear regression\" and was presented at the 2009 International Conference on Machine Learning and Applications?",
        "32e41432-f946-4c1c-9c4f-779fa6a829f8": "For the paper \"Obtaining Accurate Probabilities Using Classifier Calibration\" by Naeini, Cooper, and Hauskrecht, which organization published its proceedings, and on what pages is it cited?",
        "797d9325-084c-4756-8029-88cdd5586208": "Which publication by Harris Papadopoulos focuses on reliable probabilistic classification with neural networks and was published in Neurocomputing 107 in May 2013?",
        "71eb82d3-a74a-439d-bbd3-136981708290": "What is the title of the article published in *CoRR abs/1701.06548* by Gabriel Pereyra, George Tucker, Jan Chorowski, \u0141ukasz Kaiser, and Geoffrey E. Hinton, and on which pages is it cited?",
        "4a0e3624-e7a3-49aa-8424-4d8fce5cb8a0": "Which publication is cited on pages 71, 95, and 104 by both Tore Schweder and Nils Lid Hjort, and Jieli Shen, Regina Y. Liu, and Min-ge Xie?",
        "fd973568-354d-44b7-989e-88ca290198a4": "Which publication details \"Dropout: a simple way to prevent neural networks from overfitting\" and who are its authors?",
        "4a5290ed-7ddb-40e0-aee8-593a30a39b48": "Which publication by Vladimir Vovk, Alexander Gammerman, and Glenn Shafer is cited on the most pages, and what is its ISBN and DOI?",
        "f0c0be0d-4d42-4e21-b29a-652b60718df0": "What are the publication details (title, authors, journal/conference, year, and page numbers) for the work cited on pages 27, 77, and 133, and what is its DOI?",
        "ff2446fb-a265-47e0-a47e-15527becbd83": "What are the two different publications in which Vladimir Vovk, Jieli Shen, Valery Manokhin, and Min-ge Xie published their work titled \"Nonparametric predictive distributions based on conformal prediction,\" and what are the key differences in their publication details (e.g., type of publication, date, page numbers, and cited pages)?",
        "f33c56bb-fc90-4bff-a9af-957be8afda95": "Compare and contrast the works of Bianca Zadrozny and Charles Elkan in the context of obtaining and transforming probability estimates from classifiers, citing their respective publications and the conferences where they were presented.",
        "5a54ac82-bf21-4936-8c20-150ce10f1fa5": "According to Bibliography 171, what is the title of Venn's work, and what specific method is mentioned as being incorporated into it?",
        "cd0b6fea-f157-4278-893f-ed0852a28892": "The paper \"Prediction Intervals for Model Averaging\" by Qu, Wang, and Zhang introduces a framework for prediction intervals in model averaging. What are the two main types of data for which coverage guarantees are established, and what are the corresponding assumptions for each?",
        "16712718-2f3c-4df9-8b5f-301d5903fde9": "The text highlights a significant gap in the existing model averaging literature. What is this gap, and what two main issues does the paper aim to address in constructing valid prediction intervals for model averaging?",
        "384c7537-a9ff-407f-9510-e686b3888802": "The authors claim their work fills a gap in existing research. What specific gap do they identify regarding conformal methods and frequentist model averaging?",
        "56168cb3-3272-473d-9bd4-0641cae9dcfd": "The document discusses the application of proposed algorithms to both exchangeable and time-series data. Compare and contrast the conditions required for validity in these two settings, and describe how the finite-sample performance of the methods differed between the exchangeable and time-series cases, particularly regarding interval width for adaptive methods.",
        "f096fbb0-1dea-4fb1-b5eb-bd99cd09b80b": "According to the paper's structure, which section introduces the core problem of measuring prediction uncertainty under model averaging, and what is the general setting considered for this problem, including the notation for candidate models, sample size, and variables?",
        "73c3a9b5-96fb-4824-aa01-5187d5ca7d45": "Here is a question based on the provided context:",
        "9c23e86a-edbc-4256-b245-ac8f96188c0c": "Here is a question based on the provided context:",
        "ac81f651-0107-41a7-947f-562b724478c9": "Here is a question based on the provided context:",
        "489c0b95-c0f9-40df-8c88-bbd15c749b7b": "Here is a question based on the provided context:",
        "da5debf3-998e-402b-b507-e3ed853b03e3": "Here is a question for your quiz/examination:",
        "9454780a-bf7c-48a2-a79d-0935df3d8891": "Here is a question for your quiz/examination:",
        "ce64cbe2-fd72-4725-9471-3b98f598d0c5": "**Compare and contrast the assumptions required for coverage validity in the context of general model averaging versus time series model averaging. Discuss why the strict stationarity assumption for time series data cannot generally be weakened to weak stationarity, referencing the provided text.**",
        "3a4c94e9-450a-40d6-9297-a314315c034a": "**Explain the significance of Assumption 4 in the context of model averaging, detailing both parts of the assumption and providing examples of prior work that supports the conditions for parameter convergence.**",
        "60de96fa-d55d-4876-963f-5ef67d28f848": "Here is a question based on the provided context:",
        "9a5a3337-6cf1-476c-9238-e2ac80abab3e": "Here is a question based on the provided context:",
        "4d6ed988-8717-4767-a2ae-4b7c2d981df5": "**Compare and contrast** the method for obtaining critical values described in the first paragraph (Andrews, 2003) with the conformal inference approach used in Algorithm 1, highlighting the key differences in their procedures.",
        "17fd309d-5785-498c-b64c-aef1f36d66f8": "Here is a question based on the provided context:",
        "6fd927a3-c47e-49f5-a980-95fd7f571294": "Explain the purpose of Condition 2 in the context of applying Algorithm 1 to model averaging methods. What types of processes does it permit, and what does this imply for the applicability of the algorithm?",
        "ac0815ca-7901-464c-97e1-ac78c68ecf68": "Explain how model weights are defined using the Akaike Information Criterion (AIC) or the Bayesian Information Criterion (BIC) in the context of smoothed information criteria, and provide the formulas for AICm and BICm for linear regression models estimated by OLS.",
        "2db5a11a-d1f6-4ae6-943e-0e3eacd5ce4c": "Explain the convergence behavior of estimated model weights for both AIC and BIC when multiple models achieve the smallest Kullback-Leibler Information Criterion (KLIC). Discuss the rates of convergence (exponential or polynomial) and how they differ between the two criteria, particularly within the set of best-performing models.",
        "66034cf7-8d0f-4d16-8461-2f8dd02d4329": "**Explain the Mallows criterion for selecting weights in model averaging, including its key components and the theoretical basis for its optimality. Additionally, describe how the Mallows criterion approach differs computationally from other model averaging techniques discussed in the text.**",
        "2d4fac7b-1939-41e1-b6cf-561e5989f6a5": "Here is a question for your quiz/examination:",
        "b13a714b-40fe-4251-b1b0-6ead26b760a8": "Compare and contrast the models used for model-averaging residuals in the exchangeable observations case and the time-series case, highlighting the assumptions made about the error terms and the motivations behind each model's components.",
        "0830afc2-f44d-4d65-917c-f885b408e98e": "Here is a question based on the provided context:",
        "9443aa45-5e36-4edb-81cc-fadd25c54225": "**Compare and contrast** the conditions and assumptions for the exchangeable data case and the time series case as presented in Corollary 1, specifically highlighting the \"main strengthenings\" required for the time-series setting.",
        "dbb225ff-428e-47aa-af49-4ee5a9174db8": "**Algorithm 3, Step 1 states: \"Split the index set {1, . . . , n} into two subsets I1 and I2 of (approximately) equal size.\" What is the recommended method for splitting the data for exchangeable data, and how does this differ for time series data?**",
        "b425d421-4868-474e-a0d3-7dcff9100cf9": "Here is a question for your quiz/examination:",
        "69ae90c2-4383-4e00-bb31-9b963ec64b06": "Here is a question based on the provided context:",
        "3e46fb51-a04b-43db-a8c0-498aea5aa6b7": "Explain how Algorithms 3 and 4 can be applied to Bayesian Model Averaging (BMA) for constructing prediction intervals, detailing the steps involved from data splitting to the final calculation of the prediction interval. What is a key advantage of this approach over conventional Bayesian credible sets, as mentioned in the text?",
        "d46b94d5-a5a2-44c6-8caf-64e137eaa84e": "**Explain the process of applying bagging in a linear model setting, as described in the text, including how the final prediction is obtained and how conformity scores are calculated.**",
        "09c47b56-2765-43f6-ba6c-2331d8e66964": "Here is a question based on the provided context:",
        "b78ced1f-f7df-434f-8dd6-ec1064548ce2": "Here is a question for your quiz/examination:",
        "b5266ba5-a573-418c-b639-19d65b0ce501": "Here is a question based on the provided context:",
        "5c895d32-68a5-48f6-b261-ca95912d9fcb": "**Analyze Figure 3 and Figure 4.** For the \"S-AIC (full)\" method, describe its coverage relative to the target 90% and its interval length compared to \"Equal (full)\". Discuss any potential trade-offs observed between coverage and interval length for these two methods.",
        "2654fb1b-d5cc-4982-8e75-75059059d5e6": "Here is a question based on the provided context:",
        "4b59f879-5f7f-4ecb-94c1-720331f43121": "Here is a question based on the provided context:",
        "d840c7ab-660b-4a59-9162-ec5943edef79": "Based on Figure 7 and the accompanying text, what general conclusions can be drawn about the coverage performance of adaptive versus non-adaptive methods for time-series data with homogeneous variance when n=200, and what specific observation is made about regression-based approaches in this context?",
        "75d3232a-5336-49d8-8a05-0a965c6c74d0": "**Question:** Figure 8 presents \"Interval Length for Time-Series Data with Homogeneous Variance (n=200).\" Based on the visual information in Figure 8, what are the two main categories of methods being compared, and for each category, list at least three specific methods shown?",
        "1dee51bc-eb0b-44ea-9d9d-2e1d6c951056": "Compare the \"Reg (full)\" and \"Reg (split)\" methods for both 95% and 90% prediction intervals across all metrics (Coverage, Ave.Len, Sd.Len) for both Non-adaptive and Adaptive approaches. What are the key differences and potential implications of using a split training sample versus a full training sample for this method?",
        "3c4f56bd-747a-4232-a987-fb29ef77701a": "Describe the difference between \"adaptive\" and \"non-adaptive\" intervals, and explain how the \"Full\" and \"split\" methods relate to the training sample usage.",
        "b73f1d79-3bde-4189-ad38-cb5a08a550fe": "The study mentions using 63 candidate models. Explain how this number was derived based on the appraisal factors and what each candidate model represents.",
        "bd9b7fee-0305-48c3-8035-a4b5d7c4c752": "In the context of the cross-sectional application discussed, what are the key advantages of using adaptive conformal methods for prediction intervals, particularly concerning their length and variation, and how does this relate to property valuation?",
        "a83fbc58-c6ef-48b6-8de6-8af60074c828": "The study utilizes a rolling-window design. Explain how this design works, including the sample size used for estimation and the total number of predictions generated. Additionally, what specific statistics are reported for all methods, and how are these statistics aggregated?",
        "b8d4235b-acd1-4900-b7fe-1043d74fe3e6": "For the \"Reg (full)\" method, what are the average length and standard deviation of interval lengths for the 95% prediction interval under the non-adaptive approach, and how do these values compare to the 90% prediction interval under the adaptive approach?",
        "6fb7aabd-f9bc-4fc2-95e8-e27620c0537b": "According to the text, what is the difference between \"Full\" and \"split\" training samples, and how do \"Adaptive\" and \"non-adaptive\" intervals differ in their adjustment for data characteristics?",
        "5315a59e-a902-43a7-aac2-173e5753046c": "The provided text discusses the feasibility of valid interval prediction for model averaging even with misspecified models and dependent data. What are the two sets of assumptions under which coverage guarantees were established, and what types of data are they relevant for?",
        "a5e0d05b-c7bc-4658-87b8-9752d5964e69": "Which authors investigated the empirical performance of equity premium prediction in 2024, and in what journal was their work published?",
        "677939dd-3946-44bd-9810-dbcc73c34c34": "Which authors published works related to \"Asymptotic Theory for Econometricians\" and in what years were these editions released?",
        "3287c542-b2aa-48d4-86a5-38ed8f24782d": "Compare and contrast the contributions of Zhang et al. (2013) and Zhang et al. (2020) to the field of model averaging, specifically highlighting the differences in their approaches or the types of models they address.",
        "fd6d5637-cf52-4c73-9771-6c025d1f2e69": "According to the article, what are the three sub-cases for dividing product introductions, and for which of these cases would you \"simply copy relevant data from the older product to the new one\"?",
        "7e7a0833-03d2-4586-adb3-541e476c1399": "When forecasting the introduction of a new version of an existing product, what key effects, beyond historical sales data, should be considered, and what teams can provide insights for these considerations?",
        "7782995a-45c0-4e3f-8ff2-f8af3a697c3c": "According to the document, what are the three cases for forecasting demand for new products, and what is the recommended approach for each case?"
    },
    "corpus": {
        "583470eb-5d1f-40c5-9be4-61ea684c0553": "ARTICLE TEMPLATE\nAn XGBoost-Based Forecasting Framework for Product\nCannibalization\nGautham Bekal and Mohammad Bari\nT-Mobile, Two Newport, 3625 132nd Ave SE, Bellevue, Washington, 98006, USA\nARTICLE HISTORY\nCompiled November 25, 2021\nABSTRACT\nTwo major challenges in demand forecasting are product cannibalization and long\nterm forecasting. Product cannibalization is a phenomenon in which high demand of\nsome products leads to reduction in sales of other products. Long term forecasting\ninvolves forecasting the sales over longer time frame that is critical for strategic busi-\nness purposes. Also, conventional methods, for instance, recurrent neural networks\nmay be ine\ufb00ective where train data size is small as in the case in this study. This\nwork presents XGBoost-based three-stage framework that addresses product canni-\nbalization and associated long term error propagation problems. The performance\nof the proposed three-stage XGBoost-based framework is compared to and is found\nsuperior than that of regular XGBoost algorithm.\nKEYWORDS\nCustom Objective Function, XGBoost, Product Cannibalization, New Product\nIntroduction(NPI)\n1. Introduction\nDemand forecasting is an important use case in supply chain, inventory management,\nretail, etc. Predicting the sales of various products within the portfolio is crucial for\nmaking business decisions and other downstream tasks such as inventory optimization\nand target calculation, etc. One of the major challenges in making such forecasts\nis taking the e\ufb00ect of product cannibalization into account. Product cannibalization\noccurs when demand for a certain product within the portfolio increases that may be\ndue to launch of a new product. This consequently reduces the sales of older products.\nThis interaction between di\ufb00erent data samples leads to the fact that total demand\nof all products remains stable but with large variations in the demand of individual\nproducts within the portfolio.\nMachine learning allows us to model complex dynamics and capture large number of\ninput variables over traditional statistical models. Generally, machine learning models\ntry to optimize the cost function by using input features to the model and updating\nthe model parameters accordingly. However, in product cannibalization the demand of\na given product is being impacted by the demand of a di\ufb00erent product that is not a\npart of the input feature set. In this work, the proposed framework is to make accurate\nsales forecast of old products that are cannibalized due to launch of newer products.\nConsequently, o\ufb00 the shelf machine learning models like XGBoost or neural networks\narXiv:2111.12680v1  [cs.LG]  24 Nov 2021",
        "b5b6f4eb-de8d-4e80-9537-59495ba7ad88": "are not able to capture the interactions between di\ufb00erent data samples during the\ntraining.\nThe work presented in [5] focuses on the product demand forecast for the distri-\nbution team, and therefore only tackles one week ahead demand forecast. One of the\nimportant work [4] assumes short sale cycle for a given product and hence is useful in\nmaking short term forecasts. We tackle both short term and long term cannibalisation.\nAlso, we have focused on long term forecasting for 8 or more weeks. The work [5]\nuses multinomial model hence output is discrete, i.e. it is for categorical output. Our\nuse-case focuses on continuous variable since we are trying to forecast product sales\nthat is a real number.\nThe second major hurdle in the above use case is when making long term predictions\nunder limited training data. One of the standard techniques is to use Recursive Multi-\nstep Forecast [2] technique where prediction at a given time-step is used as an input\nfeature to make forecast for next time step. The problem with this approach is that it\ncauses error propagation when making long term forecast.\nOn the other hand, [8] provides with multiple approaches to modelling long term\nforecast. However, these are standard approaches that do not cater our use case com-\npletely. The reason being, the paper\u2019s standard approaches do not take cannibalization\ninto account and thus the performance is severely deteriorated.\nThe paper [1] deals with cannibalization due to promotional impact whereas in\nour use case we are concerned with cannibalization due to new product launches.\nIn both cases however the underlying assumption is on casual relationship between\ncannibalized and cannibalizing product.\nOur work improves the forecast accuracy of old products whose sales have been\na\ufb00ected by the addition of newer products to the portfolio. In this study any product\nafter 4 weeks of launch is assumed to be an old product. Thus, all products within 4\nweeks of launch period are NPI products. The machine learning algorithm we introduce\nin this work handles these limitations.\n2. Data Preparation\nThe dataset is a tabular data with n rows and d+1 columns. The rows are divided\nbetween train set, indexed 1 to m rows, and test set, indexed m + 1 to n rows. The\ninput matrix to the model is de\ufb01ned as X and is in d-dimensional space. The target\nvariable is Y, which is a continuous real value corresponding to number of units sold\neach week. We train and predict using our three-stage framework for each product\ncategory separately independent of one another. Thus for our experiment we have\n3 datasets corresponding to 3 separate product categories. Each product category\ncontains count as the number of products sold for a given speci\ufb01c week. Also, the\nnumber of products within a product category can vary week-over-week. The general\noverview of the dataset is as shown in Table 1.\nEach row corresponds to number of units the product sold in the corresponding\nweek as given by the column sale(t). Hence, sale(t) is the target variable Y. Every\nother column in above table corresponds to input feature X which impacts the sales\nof column sale(t). Our objective is to make accurate prediction of sale(t) for a given\nproduct on any given date. Since, we are implementing a forecasting model the sales\nof a product in a given week, i.e. sale(t), is a\ufb00ected by sales of the same product in\nprevious weeks. Hence, we have included sales of previous three weeks of the same\nproduct as a part of the input feature set X. For historical data when the sales is\n2",
        "968a6d8a-91f7-402f-88d1-ffedabea44a0": "Table 1. Experiment Results\nCategory Date Product Promotion Seasonality sale(t-3) sale(t-2) sale(t-1) sale(t)\nCategoryA Date1 A 1 Promo1 Season1 1000 1110 1150 1200\nCategoryA Date2 A 1 Promo1 No Seasonality 1110 1150 1200 1500\nCategoryA Date3 A 1 Promo2 No Seasonality 1150 1200 1500 1300\nCategoryA Date4 A 1 Promo4 Season2 1200 1500 1300 2000\nCategoryA Date5 A 1 No Promo Season3 1500 1300 2000 1650\nCategoryA Date1 A 2 Promo4 Season1 2000 4200 4000 5000\nCategoryA Date2 A 2 Promo1 No Seasonality 4200 4000 5000 3600\nCategoryA Date3 A 2 Promo6 No Seasonality 4000 5000 3600 3900\nCategoryA Date4 A 2 No Promo Season2 5000 3600 3900 4200\nCategoryA Date5 A 2 No Promo Season3 3600 3900 4200 5500\navailable, it is possible to \ufb01ll sale(t-1), sale(t-2) and sale(t-3). However, when making\nlong term predictions on future dates the previous weeks sales is not available to us.\nFor example, if we have historical sales available upto August 31, 2020 in which case\nwhen making a prediction for September 28, sales of September 07, September 14 and\nSeptember 21 is not available to us. Hence, sale(t-3), sale(t-2) and sale(t-1) will be\nun\ufb01lled in the input matrix X. To overcome the issue we will borrow an idea from\nnatural language prediction. While making prediction on future dates for testing, we\nwill use the prediction made on previous dates and append it to the columns in X. In\nthe above example, this would mean we would \ufb01rst make a prediction on September\n07, 14, and 21 and append these predictions to columns of sale(t-3), sale(t-2) and\nsale(t-1) respectively. This above concept of using previous week sale prediction as an\ninput to make a prediction on next week is referred to as back-padding. Later, we will\ndiscuss how this widely used concept of back-padding in time-series forecasting leads\nto the problem of error propagation for longer time horizon during prediction.\n3. Motivation\nThe standard XGBoost model for regression modelling uses SE loss function given by,\nl =\n\u2211m\ni=1(f(Xi) \u2212Yi)2\nm (1)\nwhere, m is the number of training samples.X is an m x d input vector to the XGBoost\nY is the target variable which takes on continuous real value and has dimension m x\n1\nThe XGBoost model is trained on X to approximate the target variable Y by a\nfunction f. The usual assumption here is that the behaviour of Y can be completely\nmodelled using X. However, in product cannibalization we observe Yi also depends on\nthe values of Yj where, i \u0338= j. Here, i and j is the row number. Here, for a given target\ndata point Yi, Yj cannot be made as a part of independent feature Xi for the following\nreason.\nFor testing, all the target variables Y(m+1) to Yn are unknown. Hence,for test\ndataset between m+1 to n the independent variable X will have certain columns\nempty. Hence, in the test dataset for a given target variable data point Yi on which\n3",
        "d5d9ff3a-745f-41a2-9655-0dfcc8a2f008": "we want to make prediction we cannot have Yj as a part of input matrix, since Yi\nand Yj are both unknown. Consequently, the XGBoost model based on SE will lose\nimportant information pertaining to cannibalization and thus we need an alternative\napproach to incorporate the cannibalization information to the model. In time series\nforecasting to forecast over long term horizon, we depend on the predictions made by\nprevious time periods. Hence, the models are auto-regressive. Unfortunately, this leads\nto the problem of error propagation that become more severe as the prediction time\nhorizon increases. This is true in our study also, since we are forecasting sales for 8 to\n14 weeks into the future respectively.\nTo overcome the two challenges we introduce a sum constraint based modelling\ntechnique that involves predicting for target variable data-pointsY(m+1) to Yn and also\nsimultaneously solving constraint based equation that incorporates cannibalization\ninformation. The available dataset for this work is quite small with individual category\ncontaining approximately 3000 records and large number of independent variables. X.\nHence, feed forward or other deep learning frameworks have not been used to construct\nthe framework. XGBoost is an obvious algorithm of choice for implementing three-\nstage framework mainly because it has superior performance on small datasets and\nimplementing custom objective functions is relatively straight forward.\n4. Algorithm\nWe make a fundamental assumption that, sum of sales over the week for a given\ncategory is easily obtainable from domain knowledge and is already known to us.\nWhat we are left with is to make week over week predictions for individual products\nbelonging to the speci\ufb01c category. Mathematically it can de\ufb01ned as,\nS = Yi + Yi+1 + Yi+2 + ..... + Yi+count (2)\nWhere, S is the aggregate sum of sales over all products in that week for the category.\nThe S can be easily obtained from domain knowledge. Yi is the weekly sale of product\n1, Yi+1 is weekly sale of product 2 and so on. Here, count represents the number of\nproducts in the given week in our dataset and can vary week over week.\nFor the weeki, Yi represents the sales of a high demand product that would suppress\nthe weekly sales of other products accordingly. It is because, S is approximately con-\nstant in a given week. The idea is to exploit this useful information to make predictions\nfor the cannibalized products. Even though the cannibalization information is not a\npart of input matrix X, we construct a three XGBoost models given by f1(X), f2(X),\nf3(X). Each of these models is trained on their corresponding objective functions l1,\nl2, l3 respectively. According to their usage in our framework they have been named\nas follows. XGBoost1 as baseline model, XGBoost2 as constraint model, XGBoost3 as\n\ufb01ne-tune model as shown in \ufb02owchart 4.1.\nIn stage 1, we train XGBoost on 1 to m samples on SE objective function. We refer\nto this model as XGBoost1. The SE objective function is given by,\nl1 =\n\u2211m\ni=1(Yai \u2212Ypi )2\nm . (3)\nWhere m is the number of training samples with actual sales. Yai is the weekly sale of\nan individual product. Ypi is the predicted weekly sale by the model for an individual\n4",
        "2edc00c7-f6e2-41ce-aabf-0a0178ae03a1": "product. The trained XGBoost1 model is then used to predict for test set m+1 to n\nsamples. The predictions on test set for samples m+1 to n in the dataset are used\nto update the columns sales(t-3), sales(t-2), sales(t-1) and sales(t). Here, sales(t-3),\nsales(t-2), sales(t-1) are a part of input feature set X. This process of using previous\nweek sales to make current prediction is called back-padding. The sales sales(t) is the\noutput label Y that we are predicting.\nThe prediction made at stage 1 are of poor accuracy as it does not take into account\nthe cannibalization of sales due to launch of new products. These predicted values yp\nare appended to ya column which initially only contained m training samples. This\nupdated train dataset containing n samples is used as an input to stage 2 (stage 2 is\nexplained in the next paragraph) of the algorithm. Also, XGBoost1 acts as a baseline\nmodel that gives the upcoming stages of the model a general guideline to make a better\nprediction.\nIn stage 2, we train XGBoost on entire dataset containing n samples based on the\nupdated dataset obtained from stage 1. We refer to it as XGBoost2. Here, 1 to m\ndata-points contains actual sales and m+1 to n contain the predicted weekly sales by\nXGBoost1. XGBoost2 is trained on the below objective function,\nl2 =\n\u2211n\ni=1(Yai \u2212Ypi )2\nn +\n\u2211n\ni=1(YcategoryActuali \u2212YcategoryPrediction i )2\ncounti \u2217n (4)\nwhere Yai is the actual weekly sales of each product for i in [1,2,...m]. It also includes\nthe predicted sales on test set by XGBoost1 model for i in [ m+1, m+2,...,n]. Ypi is\nthe weekly sales for each product predicted by XGBoost2 model. YcategoryActuali is the\naggregate sum of sales of all products for a given week. In equation 4 YcategoryActuali\nis given by,\nYcategoryActuali =\ncounti\u2211\ni=1\nYai ; for i=1,2,3,....,m. (5)\nWhere, YcategoryActuali is the aggregate sum of sales for a given category for week i and\nis assumed to be known. YcategoryPrediction i in 4 is given by,\nYcategoryPrediction i =\ncounti\u2211\ni=1\nYpi (6)\nwhere counti is the number of products on sale in the given week i. The XGBoost2\nmodel makes predictions and updates the columns sales(t-3),sales(t-2), sales(t-1) for\nrecords m+1 to n. Here, we do not update the label column Y, i.e. sale(t). We only\nupdate the input feature columns of X.\nAs evident from equation 4, the model in stage 2 is trained simultaneously on\nan objective function consisting of two parts namely, SE as well as categorical sum\nconstraint. The yp from stage 1 that are appended to ya means that the new prediction\ntries to get close to ya as per SE. But it is forced to increase or decrease the prediction\nof all the products within the category for the week based on the categorical sum\nconstraint part of the equation 4 which has been expanded in 5. This ensures that if\nthe products are over-forecasting or under forecasting then it self adjusts due to the\nsum constraint term in the objective function of equation 4.\n5",
        "949a4ed3-9ba1-4303-a517-c867d4cef9fa": "In stage 3, we train XGBoost on entire dataset containing 1 to n samples with the\nobjective function given below and refer to it as XGBoost3.\nl3 =\n\u2211n\ni=1(YcategoryActuali \u2212YcategoryPrediction i )2\ncounti \u2217n +\n\u2211n\ni=1(Ypi \u2212YpredRatioi \u2217YcategoryActuali )\nn\n(7)\nwhere YpredRatioi is obtained from output of stage 1. It is given by,\nYpredRatioi = Ypi\nYcategoryPrediction i\n. (8)\n6",
        "6123b9cb-0b4b-4d63-b9a2-57dea9344e18": "4.1. Flow Chart\nStart\nTrain on historical\ndata 1 to m records\nXGBoost1\nAppend predictions m+1 to n\nand back pad to the dataset\nGenerated prod-\nuct prediction ratio\nXGBoost2\nGenerate predictions and only\nback pad them to dataset\nXGBoost3\nFinal prediction\nStop\nThe algorithm consists of three stages, with each stage utilizing the information\nfrom previous stages and gets trained on a di\ufb00erent objective function. In stage 2\nthe model increased or decreased the predictions of all products based on categorical\nsum. Stage 3, is to guide the prediction of each product within the category based\non equation 7. To do this we use the feature device prediction ratio, which provides\ninformation of how much sale is contributed by individual device within category sum\nS. The objective function in stage 3 is given by equation 7 and consists of two terms.\nThe \ufb01rst term is same as sum constraint in equation 4. This ensures that the sum of\npredictions of all the devices in the category is still adhering to the category actuals\nobtained from domain knowledge. The second term in the objective function is to\nmake sure that individual device sale prediction within category increases or decreases\n7",
        "c8a2a07d-7293-49b7-8411-fb5ffe6f1b86": "based on the information obtained from stage 1, respectively. The stage 3 is the \ufb01nal\nstage in our framework that insures that the \ufb01nal prediction obtained for each device\nsums upto the categorical sum obtained from domain knowledge and also adjusting\nthe individual device level predictions according to their input features.\n5. Experimentation and Results\nForecasting experiment has been carried out on A, B and C categories. Each category\nhas set of devices where forecasting has been done upto 14 weeks. The experiments\nhave been performed on devices of A, B and C categories once NPI devices have been\nlaunched in their respective category.\nTable 2 enlists the details about the devices and the experiments performed.\nThe category column lists the devices belonging to a certain category based on their\npricing and device features. As discussed above in equation 2, it is considered that total\nsales volume of each category is known. The objective is to predict the sales for existing\ndevices of each category. The Date column tells us the starting date of the forecasts\nmade by the models. Regular XGBoost Accuracycolumn is the weighted accuracy over\nthe entire forecasting horizon of 8 weeks or 14 weeks depending on the category based\non the prediction made by XGBoost using SE as the objective function in equation 3.\nSimilarly, the Three-Stage XGBoostcolumn is the weighted accuracy over the entire\nforecasting horizon for the given category based on the prediction made by our three-\nstage XGBoost framework given by equation 7. Lags represent the number of weeks\nfrom the staring date for which forecasting has been carried out. Existing Devices are\nthe devices for which sales forecasting has been carried out. Sales for these devices\nare impacted by launch of new devices belonging to the same category. New devices\nare the devices that cause cannibalization of existing devices. The forecasting of new\ndevices is out of scope for this study and is a good direction for future research.\nThe train data consists of historical data of about 4 years depending on the\nexperimental setup. We have used weighted accuracy to calculate the performance of\nthe models over di\ufb00erent experiments. The weighted accuracy is given by,\nWeightedAccuracy = 1 \u2212\n\u2211m\ni=1 abs(y actual \u2212y predict)\nsum . (9)\nWhere, sum = \u2211m\ni=1 y actual. The important input features to the model are indica-\ntors to showcase holiday seasonality, promotional features, device speci\ufb01cations and\nother features such as weeks from device launch and pricing, etc.\nFor the \ufb01gures in this section, the horizontal axis represents the time horizon over\nwhich sales and forecast have been plotted. The vertical axis represents the corre-\nsponding actual sales and forecasts generated by the model, the sales and forecasts\nhave been normalized. Figure 1 represents the weekly normalized sales belonging to\ncategory A. We see that three-stage forecast from XGBoost model is much more closer\nto actual sales compared to SE based XGBoost forecast. We also observe that after\nweek 8 there is reduction in actual sales as well as in the forecast produced by three-\nstage XGBoost model. However, the SE XGBoost model is unable to capture drop in\nsales correctly and thus we see that long term forecast has been accurately captured.\n8",
        "e264403c-3f0e-4789-bd16-39bea5601256": "Figure 1. A1 Sales and Forecasts\nWe can observe that prediction with our framework yields better results compared\nto existing machine learning model. It is because it bears a much closer resemblance\nwith the actual trend. We have identi\ufb01ed week 3 and week 4 as Thanksgiving dates\nwhere we expect to see a spike in sales. Similarly, the newer model has the ability to\ncapture spike on week 8 which is the Christmas week as seen in \ufb01gure 1.\nFigure 2. B1 Sales and Forecasts\n Figure 3. B2 Sales and Forecasts\nFigures 2 and 3 represent the results for category B. Actual sales in B1 gradu-\nally decreases over 14 week horizon. The reason being, NPI devices B11, B12, B13 are\nlaunched on Aug 17, 2020 respectively. However, SE objective function based XGBoost\nmodel is unable to capture the same as it produces an increase in forecast instead of\ndecreasing. The three-stage XGBoost model captures the dynamics very well espe-\ncially over longer time horizon. More importantly, the performance of the three-stage\nframework outperforms the performance of SE XGBoost after 5th week.\nFigure 4. C1 Sales and Forecasts\n Figure 5. C2 Sales and Forecasts\nActual and forecasts for devices C1 and C2 belonging to category C are shown\nin \ufb01gure 4 and 5, respectively. We see that overall accuracy is greater in the three-\nstage algorithm compared to standard SE objective function based XGBoost. We\nobserve that for product C1 the SE and the three-stage XGBoost framework both\nperform poorly in capturing the market dynamics, especially after week 7. Both models\n9",
        "b30d68e4-286a-48f7-bf2b-5a95e4ed5750": "tend to signi\ufb01cantly under-forecast the sales after week 7 compared to actuals. For\nproduct C2 belonging to category C we see three-stage XGBoost model performs much\nbetter compared to SE XGBoost model. The three-stage XGBoost for C3 product has\nforecasts very close to actuals. On the contrary, the SE XGBoost performs quite poorly.\nIt can be observed, the three-stage framework based XGBoost consistently performs\nbetter than SE based XGBoost in all of our experiments.\nIt can be seen that our model exploits categorical sales that is a input to the\nthree-stage framework. Hence, the three-stage framework uses the total categorical sale\nto adjust the sales of individual devices within the category. This helps to overcome the\nerror propagation problem when making long term forecasts. Hence, in all the above\ncases we observe that the proposed three-stage framework consistently outperforms\nexisting state of the art XGBoost model signi\ufb01cantly. The performance of the models\nhas been calculated by using the weighted accuracy given by equation 9.\nTable 2. Experiment Results\nCategory Date Regular XGBoost\nAccuracy\nThree-Stage\nXGBoost Lags Existing Devices New Devices\nCategoryA 02 Nov, 202038.65% 67.09% 8 A1, A2, A3,\nA4, A5 A6, A7, A8, A9\nCategoryB 17 Aug, 202044.60% 51.50% 14 B1, B2, B3, B4, B5, B6,\nB7, B8, B9, B10 B11, B12, B13\nCategoryC 17 Aug, 20206.70% 52.90% 14 C1, C2, C3, C4,\nC5, C6, C7, C8, C9, C10, C11C12, C13, C14\n6. Conclusion\nIn this work, we have developed an algorithm to improve the sales forecasting accuracy\nof older devices that are impacted by cannibalization due to launch of new devices.\nThe other problem statement that has been addressed is to improve week over week\nlong term forecasting accuracy of the old devices. To address the above two issues we\ndeveloped a three-stage framework using XGBoost algorithm that consists of three\nstages. We compared the three-stage XGBoost-based framework with the regular XG-\nBoost model that uses SE as the objective function. Our experiments show that the\nproposed three-stage framework performs consistently better for long term forecasts.\nWe used weighted accuracy as a metric to quantify the performance comparison. From\nour experiments, we observe a signi\ufb01cant increase in overall prediction accuracy for old\nproducts from 38% in the baseline model, to 67% after using the proposed framework.\nA good direction for future work is to extend the framework to make accurate forecasts\nfor newly launched devices that cause the cannibalization of sales of old devices.\nReferences\n[1] Carlos Aguilar-Palacios. \u201dcausal quanti\ufb01cation of cannibalization during promo-\ntional sales in grocery retail\u201d. IEEE Xplore, 2019.\n[2] Jason Brownlee. \u201d4 strategies for multi-step time series forecasting\u201d. 2019.\n[3] Hing Kai Chan. \u201ddemand forecasting in the context of intra-cannibalization\u201d. MIT\nLibraries, 2019.\n10",
        "ab3b95c9-7f6d-4822-bc6e-602585a9aa26": "[4] R\u00b4 emy Garnier. \u201dconcurrent neural network : A model of competition between\ntimes series\u201d. 2019.\n[5] Evelyne L. Kong. \u201dcannibalization e\ufb00ects of products in zara\u2019s stores and demand\nforecasting\u201d. dspace, 2019.\n[6] Katherine Gail Nowadly. \u201dusing machine learning approaches to improve long-\nrange demand forecasting\u201d. MIT Libraries, 2019.\n[7] Jason Runge. \u201da review of deep learning techniques for forecasting energy use in\nbuildings\u201d. energies, 2019.\n[8] Hossein Sangrody. \u201dlong term forecasting using machine learning methods\u201d. Re-\nsearch Gate, 2021.\n11",
        "4e26a6e0-d781-4bb2-93c4-942b178d1b8e": "Application of Predictive Analytics \nto Sales Forecasting in Fashion Business \nJulia Erhard \nPeter Bug\nReutlingen University\nReutlingen, 2016\nbrought to you by COREView metadata, citation and similar papers at core.ac.uk\nprovided by Repositorium und Bibliografie der Hochschule Reutlingen",
        "664b3ca3-eb19-416a-b0dc-dbcce5aa662b": "1 \nABSTRACT \nLike many others, fashion companies have to deal with a global and very competitive envi-\nronment. Thus companies rely on accurate sales forecasts - as key success factor of an eff i-\ncient supply chain management. However, forecasters have to take into account some spec i-\nficities of the fashion industry. To respond to these constraints, a variety of different for e-\ncasting methods exists, including new, computer-based predictive analytics. After the evalu a-\ntion of different methods, their application to the fashion industry is investigated through \nsemi-structured expert interviews. Despite several benefits predictive analytics is not yet \nfrequently used in practice. This research does not only reflect an industry profile, but also \ngives important insights about the future potential and obstacles of predictive analytics. \n1. Introduction\n1.1 The apparel supply chain \nThe fashion industry includes many companies from spinning to distribution \nwhich are required to fulfill all the manufacturing steps to transform the fibre into \nthe final garment hanging in the store. This leads to quite a long and complex supply \nchain shown in figure 1. Additionally the apparel market is regarded as a highly co m-\npetitive one due to a relatively constant oversupply produced by the many brands of \nclothing, so that consumers are very disloyal and base their choices quite often on \nthe price. Consequently the fashion companies have to reduce their production \ncosts in order to stay efficient, so that most of the manufacturing processes elab o-\nrated in figure 1 are realized in far away and low cost countries in Asia. This global i-\nzation increases the lead time again (Thomassey, 2010). \nFigure 1: The apparel supply chain (Thomassey, 2014, p. 10)",
        "f70a61a1-c1e0-4e96-a604-5f653916144d": "2 \nBy contrasting the long time- to-market with the short life cycle of fashion \nproducts it becomes obvious that the apparel supply chain must be absolutely opt i-\nmized. In this context, sales forecasting as \u201cthe projection of expected demand given \na set of environmental conditions\u201d (Brannon, 2010, p. 308) becomes the key success \nfactor of the supply chain management (Thomassey, 2010). \nAccurate sales forecasting does not only result in lower costs but also in i n-\ncreased sales and higher levels of customer satisfaction: \n\uf0b7 It reduces the Bullwhip effect caused by demand volatility by optimizing the\ninventory levels and consequently the inventory costs of the companies\nalong the apparel supply chain (Zhang, 2004).\n\uf0b7 It prevents the distributor from markdowns, when unwanted, remaining\nitems must be sold at a loss of the profit margin at the end of the season. At\nthe same time it avoids stockouts, so that customers can buy the ir desired\nitems in the locations and in the quantities demanded, and therefore i m-\nproves the customer service level (Brannon, 2010).\n\uf0b7 Moreover it enables all the up\nstream suppliers to optimize their resources\nand generally improve their sourcing strategy, which leads to higher eff i-\nciency and lower costs (Zhao, Xie, & Lau, 2001).\nAs the distributor however is the main actor within the apparel supply chain, \nplacing the orders for the upstream suppliers and providing the garments for the \ncustomers, it is the driver of the whole flows within the supply chain (Thomassey, \n2010). Consequently distributors have to apply sales forecasting methods to ensure \nan efficient supply chain and make the best of the benefits described above. \nBut in order to perform suitable sales forecasts, it is essential to know the \nspecificities of the fashion business, which should be taken into account by building \na sales forecasting model for this industry. \n1.2 Specificities of apparel sales and related forecasts \nFor most of the products, the life cycle can generally be summarized in six \nstages: the product development, the introduction, the market development, the \nmaturity, the saturation and finally the decline (Brannon, 2010). Still it is commonly \nknown that the life cycle of fashion products is quite short,\n especially compared to \ntheir long time-to-market, and looks different among the products according to their \nfashion level: Basic items are offered throughout the year or at least every year,",
        "0fb1e234-72f9-477e-abd8-abc57e286ca6": "3 \nwhereas fashion items are only sold punctually in a short period. Best selling items \nusually appear every year with slight modifications according to the fashion trends \nand can be replenished during the season. Of course every category has a distinct \nsales behavior, which should be taken into consideration in the forecasting process \n(Thomassey, 2014). \nConsidering the different life cycles and the long lead-time for design, pro-\nduction, transportation, quality control and distribution, two horizons of sales for e-\ncasting have to be distinguished: On the one hand the system has to be able to carry \nout a long-term forecasting (horizon of one season or year) to plan the sourcing and \nproduction. But on the other hand short-term forecasting (horizon from one week) is \nessential in order to replenish best-selling items or readjust the forecasts (Thoma s-\nsey, Happiette, & Castelain, 2005). \nAnother constraint of the fashion industry is the product variety. The fashion \ntrends contain many styles and colors, so that in combination with variations in size \nthe number of stock keeping units to be managed becomes very large (Vaagen & \nWallace, 2008). This is the reason why an aggregation of the data up to the level of \nthe product family is often made. Thus the number of sales forecasts gets reduced \nand at the same time the amount of historical data for every SKU increases. Still each \ncollection includes a lot of new items with no historical data available. Consequently \ndata mining techniques for clustering and classification have to be applied to make \nsales forecasting possible in this special case (Thomassey, 2014). \nThe nature of clothing itself leads to an additional specificity of the fashion \nindustry: high seasonality. Corresponding to the climatic changes the sales volume of \nsome product categories like swimwear or pullovers varies a lot, whereas other \nitems are logically not that impacted by seasonal variation. Nevertheless seasonality \ngives an overall trend for the sales, which should definitely be integrated into the \nforecasting system (Thomassey, 2010). \nLike other industries the apparel market finally gets influenced by so called \nexplanatory factors, which can be more or less controlled, sometimes even u n-\nknown, but with an impact on the sales. The variables under-control involve the \nmarketing strategy, realizing promotional and advertising actions to boost sales, \nretailing strategy, controlling the number of stores, and the feature of the products \nitself with respect to their match with the fashion trends. On the contrary changes \nwithin the competitive environment, macro-economic developments and the cale n-\ndar or rather weather data are regarded as uncontrollable factors (Little, 1998). Alt-\nhough the impact of these explanatory variables is difficult to quantify, they have to \nbe considered in the sales forecasts (Thomassey, 2010).",
        "2a27f39c-a501-414f-b151-e9924ce81180": "4 \n1.3 Big Data and new forecasting opportunities arising from it \nBesides the specificities of the fashion industry, which make sales forecas t-\ning quite complex, the amount of data collected by the retail point- of-sale technolo-\ngies as well as by web site traffic and e-commerce has been exploding (Kim, Fiore, & \nKim, 2011). As a result, Big Data, referring not only to the quantity but also to the \nquality, velocity and variety of different data types collected, is the buzzword and \nchallenge of the moment. However, in connection with analytical techniques from \nstatistics to data mining and machine learning it can be regarded as a huge op por-\ntunity to better predict future sales, summarized under the term predictive analy t-\nics. In this context, the use of predictive analytics will soon become a major differen-\ntiator among fashion companies (Liu, 2015). \n From the discussion mentioned above, it is very clear that fashion sales \nforecasting is a truly important topic in practice. Nevertheless fashion companies are \nconfronted with such a great variety of forecasting methods, from the traditional \nforecasting systems to advanced predictive analytics, each one responding different-\nly to the constraints of the fashion industry. Thus it is the objective of this paper to \ncompare the existing sales forecasting methods and to investigate their application \nto the fashion business. In this context interviews with experts of the apparel indu s-\ntry are conducted in order to get an overview on which forecasting methods are \napplied in practice and why. \nThe organization of this paper is given as follows. In section 2 commonly \nused as well as advanced sales forecasting methods are detailed and evaluated. Se c-\ntion 3 describes the methodical approach of this paper \u2013 the consultation of experts. \nThe results of the interviews are presented in section 3, followed by their discussion, \nsummary and implications in section 4. \n2. Literature Review\n2.1 Traditional sales forecasting methods \nGenerally two major categories of forecasting techniques have to be disti n-\nguished here: quantitative and qualitative methods. Traditionally, fashion sales for e-\ncasting is accomplished by quantitative, statistical methods like time series and r e-\ngression models, which are the most commonly used techniques for prediction of \nsales data according to literature (Makridakis, Wheelwright, & Hyndman, 1998).",
        "743144c7-22a3-4d1d-a9c1-0ad2b9effb2b": "5 \nTime series forecasting looks only at patterns in sales history reflected by the \nlevel, overall trend, seasonality and noise of the sales data and projects those pa t-\nterns into the future. The statistical methods have developed from simply using the \naverage of all past sales as a forecast, to moving average, where only the most r e-\ncent periods of sales data are included up to exponential smoothing, weighting the \nmost recent sales period more heavily in the forecast than older ones (Mentzer & \nMoon, 2005). As time series methods are simple to understand and inexpensive to \nuse, they are very popular in fashion sales forecasting. Nevertheless the approach of \nusing the average neutralizes any fluctuations like noise, trend as well as seasonality \nand additionally ignores all the exogenous factors that might have influenced d e-\nmand like promotions, competitive actions, economic activity and so on. This is the \nreason why time series should in any case be applied for some short term forecas t-\ning within the fashion industry (Brannon, 2010). \nAnother common forecasting method is the regression technique, which f o-\ncuses on the correlations between sales and all the explanatory, outside factors di s-\ncussed above. Regression models investigate how sales will develop if the exogenous \nfactors change, for example if a marketing action such as a sales promotion program \nis carried out. Consequently they provide a broad environmental perspective for \nforecasting sales (Mentzer & Moon, 2005). However regression models need huge \ndata sets including past history on each factor and sales volume and therefore are \nmost useful when the time horizon is more than six months (Brannon, 2010). \nIn addition to that some further, more complex developments of time series \nand regression models do exist as well as their combination like in the Box & Jenkins \nARIMA model. Nevertheless more sophisticated forecasting models need not nece s-\nsarily perform better than the simple ones. In general all the statistical methods are \nnot efficiently implemented in the fashion industry, because they require a big \namount of historical data, a complex optimization of their parameters and they are \nlimited to linear structure and the assumption that past pattern will continue in the \nfuture (Thomassey, 2010).  \nAs it is commonly known that pure statistical methods are insufficient to \nproduce satisfactory forecasting results for fashion sales (Choi, Hui, & Yu, 2011) , \nthere are many examples in literature, where additionally qualitative methods are \napplied in order to adjust the forecasting results. Qualitative techniques make use of \nthe expertise of people in the organization. This is especially useful for predicting \nchanges that cannot be seen in the sales pattern or for sales forecasting of new \nproducts where no historical data is available. Here the intuition, experience and \njudgement of experienced executives represent an extremely rich data source that",
        "809458dd-bea5-42f0-af35-bd20a1521da7": "6 \nshould be incorporated into the forecasting process (Mentzer & Moon, 2005). \nThere are several qualitative methods: Sometimes adjusting quantitative \nforecasts depends on a key employee \u2013 an in-house expert \u2013 with the skill and expe-\nrience to see beyond the numbers into the dynamics of the apparel market. An al-\nternative to the single in-house expert is to involve groups of executives from var i-\nous corporate functions, like sales, marketing, finance, buying and design. Especially \nthe salespeople closest to the customers can provide important sales information. \nThe executive committee should then jointly work on a sales forecasting adjustment. \nAnother technique called the Delphi method makes use of a polling process. Internal \nas well as external experts provide written forecasting recommendations, which \nthen are summarized and shared again, so that the participants can refine and clarify \ntheir responses until a best guess scenario is achieved. As this procedure needs \nsome time, the Delphi method is mainly used for the prediction of long-term sales \nforecasting (Brannon, 2010). \n2.2 Advanced methods \u2013 predictive analytics \nWith the advance of computer technology and the increased availability of \ndata the field of business intelligence, as taking raw data and turning it into \nknowledge used to support business decisions, yields a new analytical, three-stage \napproach: the evolution from descriptive to predictive to finally prescriptive analy t-\nics (Koch, 2015). \nDescriptive analytics refers to the question what is happening or rather what \nhappened in the organization and provides a basic understanding of the data. Thus \nas a first analytical step key statistical characteristics about the collected data are \nexamined and visualized,\n in order to gain essential insights and to resolve imperfe c-\ntions or problems in the data (Sharda, Asamoah, & Ponna, 2013). \nThen predictive analytics aims to determine what will happen in the future. \nThis prediction is based on advanced mathematical models and algorithms as well as \non more recently developed techniques that fall under the general category of artifi-\ncial intelligence and data mining, which will be detailed later (Sharda, Asamoah, & \nPonna, 2013). Using these advanced, computational methods predictive analytics \ntries to discover meaningful patterns in the data and project them into the future. \nLastly prescriptive analytics attempts to positively influence the forecasting \n(how can we make it finally happen?) applying various optimization tools. Thus it \nidentifies the actions needed to be taken to drive predicted outcomes and gives \nrecommendations on different alternatives. Prescriptive analytics utilizes advanced",
        "0a9dec01-cf05-41a2-970c-b3f4479e986c": "7 \nautomated data-driven decision making techniques. It finally converts the comp a-\nnies into more proactive market players (Koch & Hamilton, 2015). \nIn the context of sales forecasting predictive analytics with its various types \nof models is in the center of attention. A predictive model can be defined as a mech-\nanism that predicts an outcome or a behavior of an individual taking descriptive \nattributes as input and factoring them together for prediction. The predictive model \nin turn is built by an automated computer machine learning process utilizing histor i-\ncal data (Siegel, 2013). In the following the most common types of predictive models \nwill be described: \n\uf0b7 Fuzzy inference systems model non-linear and incomplete data. As fuzzy log-\nic is based on linguistic rules, human knowledge and consequently the co m-\nplex influences of explanatory variables on sales can be included. Using e x-\npert knowledge and machine learning the model is able to automatically r e-\nproduce the expert approach when performing intuitive forecasts (Thoma s-\nsey, 2010).\n\uf0b7 Neural networks also describe complex, non-linear relationships without any\nprior assumption about the underlying data-generating process. Generally\nthe neural network consists of three layers: In the first layer weighted vari a-\nbles get introduced as input to the system. The hidden, middle layer then\nbuilds the model, so that the forecast will be presented in the output layer.\nTo create the model the neural network has to be trained with some histor i-\ncal data (Alon, Qi, & Sadowski, 2001).\n\uf0b7 As the training phase of a neural network based on a learning algorithm r e-\nquires quite a long time, the extreme learning machine models have\nemerged as a very fast alternative (Sun, Choi, Au, & Yu, 2008).\n\uf0b7 When it comes to classification, decision trees are among the most popular\npredicting modeling techniques. All the trees can be read as a series of \u201cif -\nthen-else\u201d rules that ultimately generate a predictive value  in terms of the\nassignment to a certain category using a segmentation algorithm (Abbott,\n2014).\n\uf0b7 Last but not least clustering is a prominent data mining technique whereby\nitems with similar attributes are grouped together in order to reveal stru c-\nture in the data. Moreover data clustering helps to identify, learn or predict\nthe nature of new data items. As clustering algorithm K-means or K-nearest\nneighbor are used (Bari, Chaouchi, & Jung, 2014).",
        "1757a188-51b7-41e6-8b11-b85ecc638e4e": "8 \nIn the literature fuzzy logic based techniques as well as neural networks u s-\ning historical sales, color, size, price and many other variables, including explanatory \nfactors, as inputs exhibit a superior fashion sales forecasting performance in co m-\nparison to traditional forecasting methods and are especially useful for short-term \nforecasts. Due to their speed extreme learning machine models are regarded as very \ngood candidates for sales forecasting in fast fashion companies (Liu, Ren, Choi, Hui, \n& Ng, 2013). Regarding the forecasting of new apparel items with no historical data, \nthe clustering technique can be applied : First the historical items with similar sales \nprofiles are grouped together, then links between the sales behavior and descriptive \ncriteria are established, so that in the third step each new item can be classified to \none sales profile according to its descriptive attributes. This procedure is detailed in \nfigure 2 (Thomassey & Happiette, 2007): \nFigure 2: The Clustering and Classification Forecasting Model (Thomassey & Happiette, 2007, p. 1178) \nIn contrast to the application of predictive analytics to fashion sales forecast-\ning, a lot more literature and applications can be found in the area of advertising, \nincluding target marketing and recommender systems, finance and risk analytics \n(Miller, 2014),  fashion trend forecasting and design as well as manufacturing issue s \n(Guo, Wong, Leung, & Li, 2011).",
        "e5682f5b-f8a9-4125-92ec-542c6a2d336b": "9 \nAlthough predictive models make about 20 \u2013 30 % more accurate forecasts \nand overcome the limitations of traditional methods including speed, misspecific a-\ntion, biased outliers, assumption of linearity and the need of a huge amount of hi s-\ntorical data (Finlay, 2014) , Thomassey estimates that the sales forecasting in the \nmajority of the fashion companies still consists of a baseline forecast, which is bas i-\ncally the sales of last year and which then get reworked and adjusted by the pract i-\ntioner according to explanatory factors. Even though the result can be very accurate, \nthis procedure has some important drawbacks: The number of explanatory variables \ntaken into account as well as the number of items to be forecasted are limited; ot h-\nerwise the analysis becomes too complex and imprecise. Generally qualitative a d-\njustments are quite expensive, because they require a lot of time. Finally the results \ndepend very much on the experience of the practitioner (Thomassey, 2014). \nHaving all the benefits of predictive analytics in mind the following questions \narise: In which extent do fashion companies already make use of these advanced \ntechniques for their sales forecasting and why have others not applied them so far? \n3. Methodology\nTo obtain a richer, deeper understanding of the procedures of sales forecast-\ning in practice and the reasons behind the different approaches telephone semi-\nstructured interviews with experts from the fashion industry have been conducted. \nThis is regarded as a very economical research technique, which allows to get direct \nand quick access to the insider-knowledge, experience and opinion of the practitio n-\ners and to reveal even deeper insights and practical problems directly from the field. \nIn this context, an expert is defined as someone who has specific expertise in a par-\nticular area due to practical experience and at the same time carries certain respo n-\nsibility. Consequently he or she is of less interest as a person than in his or her capac-\nity of being an expert for sales forecasting (Bogner, Littig, & Menz, 2005). \n3.1 Data collection \nThe study participants were purposively sampled based on the latest ranking \nof the biggest textile and clothing manufacturers and retailers in Germany \u2013 a list \nthat contains all fashion companies with more than 50 million Euro turnover in Ge r-\nmany provided by the professional journal TextilWirtschaft in 2014. After eliminating \ndiscounters, furniture stores, small distributors without private label or without any \ncontact to the end customer and generally companies with no headquarter in a Ger-",
        "b33b57e6-fb46-417e-8e14-6cb495a6f602": "10 \nman speaking country, to make sure that all the interviews can be uniformly and \nunproblematically conducted in German, a list with the most important 108 comp a-\nnies remained to represent the German fashion industry. \nAll the 108 fashion companies have been contacted by telephone. After co n-\nvincing the so-called gate-keepers the interview request and some pre-information \nincluding the introduction of the researcher, a short description of the research topic \nand objective, the general organizational framework and the intended use of the \nfindings were sent via email to the potential interview partners (Kaiser, 2014). In \ntotal 24 experts participated in the interviews during October and November 2015, \nwhich leads to a response rate of 22 % confirming the relevance of the topic . The 24 \ncompanies participating represent all type of apparel firms: Most of them, 54 % offer \nclothes for women, men and children, sometimes additionally home textiles, wher e-\nas 25 % are specialized on womenswear, 17 % are specialized on menswear and one \nkidswear company. Among the participants were also two traditional department \nstores. Regarding the business model 50 % concentrate on their own stores in co n-\nnection with an online shop, 42 % base their distribution mainly on wholesale par t-\nners and 8 % focus on mail order business and online shops. 43 % of the interview \npartners \u2013 19 men and 5 women \u2013 were from purchasing and merchandise ma n-\nagement and at 57 % from the sales and sales controlling department, especially at \nwholesale driven companies. Moreover the interviewees had a minimum of five \nyears, in the average eleven years of experience within the area of sales forecasting ; \n71 % of them have an executive position. \nEach telephone interview took between 25 and 50 minutes. A guideline was \ndeveloped and pretested, in order to achieve certain comparability, a thematic \nstructure and a co-expert status of the interviewer. Nevertheless the guideline was \nused in a very flexible way according to the conversational situation (Kaiser, 2014). \nIn the very beginning the interviewer once again introduced the research \nand the objective of the interview and expressed her thanks for the participation. As \nthe opening question the participants should then introduce their positions and e x-\nperience in the area of sales forecasting. During the interview each expert was asked \nseveral open questions on two main topics: First he or she should describe the cu r-\nrent sales forecasting procedure, including if necessary responding to detailed que s-\ntions of the interviewer regarding the explicit characteristics of the method, the \nhandling of the specificities of the fashion industry and the satisfaction with the a c-\ntual process. In the second block the expert was confronted with predictive analytics \nas a new forecasting method and asked to state his or her opinion on their potential \nin the future and the current obstacles to their application. On closing each inte r-",
        "244d8390-3c98-4487-b416-1df75e7d915c": "11 \nview, the possibility of additional questions as well as the further proceedings were \nclarified, including the promise of the results (Bogner, Littig, & Menz, 2014). \n3.2 Data analysis \nAll the interviews were audiotaped with the expert\u00b4s agreement and literally \ntranscribed for analysis purposes. As the interviews were mainly aimed at gathering \ninformation and estimations of the experts, the transcription as well as the analysis \nfocused principally on the content of what is being said (Dresing & Pehl, 2013). \nThus qualitative content analysis according to Mayring was used in order to \nsystematically structure, summarize and finally analyze the data, comprising of the \nfollowing steps (Mayring, 2007): \n\uf0b7 Analysis unit: The textual material was divided into units for further analysis.\nIn this study the respective answers to each question, consisting of ether se-\nveral, coherent sentences or just one word, represented the analysis unit.\n\uf0b7 Category system: Categories were deductively developed based on the ques-\ntion categories of the interview guideline and were therefore organized in a\ncertain, thematic system. Then step by step every analysis unit was assigned\nto the corresponding category \u2013 every answer to the corresponding que s-\ntion. Although the categories were defined a priori, they were inductively\nchecked and adjusted on the textual material.\n\uf0b7 Analysis technique: The primary focus and objective of the analysis was the\nsummarization of the material. Consequently, after categorizing the stat e-\nments, the material was summarized and reduced to the essential points ,\nparaphrasing the content supporting passages of each statement . Para-\nphrases with similar meaning again were bundled together.\n\uf0b7 Interpretation technique:  Finally the paraphrases of the statements of the\ndifferent interview partners regarding each question category were co m-\npared. In order to get an industry profile, the paraphrases were summarized\nusing quantitative evaluation measures like frequency distributions. The rea-\nsons behind distinct approaches and opinions were of course also analyzed.\nThe described, systematic content analysis approach should ensure the \ntraceability of this research and at the same time achieve the quality criteria of reli-\nability and validity (Mayring, 2007).",
        "8ae92c42-32f8-4b26-a844-1126f6cf6eda": "12 \n4. Results\n4.1 Applied sales forecasting methods and used data \nAs the key objective of this research was to get an overview of the sales \nforecasting methods applied to the fashion business, the description of the current \nsales forecasting procedures by the experts was a fundamental first step. In general \nthe sales forecasting process begins with the overall planning of the annual turnover \nfor the next year, which in 54 % of cases is set top-down in advance, whereas 46 % \ntake more of a bottom-up approach. At a surface level, all experts claimed to have a \nunique approach. Yet at a deeper level 23 out of the 24 interviewed companies base \ntheir sales forecasting on the same method: a combination of time series analysis \nbased on historic data and qualitative adjustments. \nInformant 7:  \u201cYes, precisely - in our company, it is a very retrospective consider a-\ntion. This means that we operate strongly based on historical data. \u201d \nInformant 21: \u201cThe very first and easiest thing we do is a comparison with the prev i-\nous year, which means that sales figures from the past year get married in a certain \nway with sales trends from this year.\u201d \nInformant 11: \u201cWhat we do is a mixture between historic data, if available, this has \nalways been the issue too, and then there is also a meeting with buyers and me r-\nchandise planners. This meeting is about finding out the most important future \ntrends in fashion business and consequently adjusting the forecast with a rather o p-\ntimistic or pessimistic trend. So we are mostly int erested in data from the past (\u2026) \nand then we add a coefficient \u2013 the trend.\u201d \nHaving a closer look at the time series analysis 82 % use only the data of the \nprevious year or season on an either monthly or weekly base, because of the short \nlife cycles and therefore the low comparability of the articles over several seasons. \nInformant 11: \u201cNo, this refers only back to the last year. The problem is, we always \nhave a lot of modifications for our customer, we do not have many basic articles, \nwhich we are offering regularly every year. Of course we create some similar art i-\ncles, but most of them have a very short lifespan. (\u2026) Only if there is a super similar \nproduct, we go three, four years back, but otherwise it is quite risky for a live cycle. \u201d",
        "3c5c92e6-c8a3-4ad4-a5c0-f0101d1b7a3c": "13 \nNevertheless 9 % make use of the data of the last two years, in order to \nsmooth out extreme deviations within one year. Another 9 % even refer back to the \nlast three years to gain a clear tendency. \nThe historic data represent the input of the time series analysis. 63 % of the \ninterviewed fashion companies work with real sales figures, whereas 37 % - only or \nmainly wholesale driven companies - utilize the order figures of their wholesale \npartners. Half of these companies complain ed that they do not have access to EDI \n(electronic data interchange) sales information of their wholesale partners, while the \nother half receive the real sales figures at least partly from some wholesale partners \nvia EDI connection. Still this transmitted sales information is incomplete, which is the \nreason why it primarily serves as input for the development of new collections or the \nanalysis of bad-best-seller lists rather than for the sales forecasting. Only one of the \nwholesale driven companies has the entire sales figures from their partners via EDI \nconnection, so that they can base their sales forecasting on the real sales figures to \nthe final customers. \nInformant 9:  \u201cUnfortunately not all of our customers, even though we are in the \nyear 2015, are well connected. And in addition to that there are 20 \u2013 30 % who do \nnot want an EDI connection and 10 \u2013 20\n % who have their own system, which is not \ncompatible with our system. The interconnectedness is definitely expandable. \u201d \nInformant 22: \u201cThis is exactly the point; we do not have the real sales figures of our \nwholesale partners. This is not yet included in our sales forecasting.\u201d \n21 out of the 24 interviewed companies additionally run an online shop, \nwhich can be regarded as a huge data source. Still 86 % do not include the (custo m-\ner) data generated therefrom in the sales forecasting. Three companies use the \nonline shop as an indicator of potential bestsellers, but only one company analyses \nthe online purchase history of their customers and the customer journey between \nthe segments for the sales forecasting. Moreover none of the companies who offer a \ncustomer card integrate the sales and customer data therefrom into their sales for e-\ncasting. The result of the time series analysis depending on the aggregation level of \nthe input data is in 25 % of the cases a forecast per label or segment, in 42 % a for e-\ncast on product group level and in 33 % a forecast of the quantities on item level. \nAs time series analysis - a purely  statistical method - is not satisfactory for \nsales forecasting in fashion business, all experts emphasized that it only provides \nbasic data. The forecast additionally gets qualitatively adjusted . This way on the",
        "a1a6de1d-13a8-4da9-a677-11a4ad655b91": "14 \ncompanies add strategic corporat e objectives as well as explanatory factors to their \nforecasting. The qualitative adjustments happen in consideration of bad-best-seller \nlists, fashion trends, the current collection, the latest turnover trend, assortment \nand area management, price ranges and stocks. Several interviewees indicated that \nbuyers and sales personnel adjust the forecast in accordance to their experience, fair \nvisits, trips abroad, exchange with other companies, information from market r e-\nsearch institutes and feedback from retailers. Consequently the expertise of these \nemployees plays an essential role within the forecasting process. \nInformant 21: \u201cBasically [the forecast] is also strongly based on the experience of \nthe people (\u2026) There is definitely more information than just the sales history. There \nare mega trends influencing the whole fashion market. Therefore we have co l-\nleagues who are away on business fairs or are working in the stores. From here the \ninformation is bundled and rewinded to the product development.\u201d \nInformant 15: \u201cAdditionally there are three factors which I take into account. First \npublic holidays, secondly I have to clean up the data from marketing actions in the \npast causing abnormalities in the data, and the third factor are mega trends. \u201d \nIn addition most of the mainly or partly wholesale driven companies wait for \nthe first orders of their wholesale partners as an indicator, in order to project them \ninto the future and again adjust the forecast. A grossing-up factor is built to bring the \nactual order trend down to the individual items. The same projection logic is applied \nwithin the catalogue business, after the first order week. \n30 % of the experts also mentioned that the sales forecasting process for the \nNOS (never out of stock) articles differs in that an automatic replenishment system \nexists, which can rely on historic data from several years. Therefore the NOS articles \ncan be easier systemized and in the end better forecasted. This is also the reason \nwhy one company with a significant percentage of NOS items renounces time series \nanalysis and qualitative adjustments. Instead a standard order rhythm is employed, \nprojecting the incoming orders through algorithms into the future. \nAs a first and direct finding it should be noted that apart from this NOS dri v-\nen company none of the remaining interviewed companies already applies predi c-\ntive analytics as a sales forecasting method. Though several experts expressed their \ninterest in the implementation of predictive analytics in the near future, only one \ncompany will actually soon go live with a new software including predictive analytics \nas sales forecasting module.",
        "e2b52fe9-3b34-43b5-b2b8-e7e5fa7df693": "15 \n4.2 Characteristics of the applied sales forecasting methods \nThe majority, 46 % of the interviewed companies carry out two forecasts per \nyear, which cover respectively the time horizon of half a year. 30 % just realize one \nforecast for the whole year; however this usually gets updated half-yearly. It became \nquite clear that most of the fashion companies still think in the traditional two se a-\nsons: spring/summer and fall/winter. Consequently 70 % offer two (main) collections \nper year, sometimes with two smaller pre or post season programs in between. In \norder to include short-term developments and trends, some updates are necessary. \nThus half-yearly, monthly or most frequently weekly updates are made, depending \non the reporting cycles of the company. Some, mainly wholesale driven companies \nupdate their forecasts even daily, especially during the order phase. In general, the \nmore collections and the more new, innovative fashion items, the more forecasts \nand updates are conducted. This ranges up to six, ten or even twelve collections per \nyear with respectively as many forecasts and shorter time horizons according to the \nsales period of the collections. \n The fashion industry is known for its long lead times. Likewise, the sales \nforecasting is done in 22 % of the interviewed companies from three-quarters up to \none year in advance, in 57 % half a year in advance; 21 % have a lead time of only \nthree to five months between the sales forecasting and the delivery of the products. \nThis is the reason why the experts several times compared the sales forecasting to \n\u201ctaking a look into the crystal ball\u201d. \nThe distinction of the sales forecasting regarding different distribution cha n-\nnels or regions plays a subordinate role. One third of the companies do not treat the \nonline shop differently, so that despite the particular processes and data it is for e-\ncasted like a store or a customer. On the contrary two third have a separate fore-\ncasting for each channel, whereby above all the data base and the projection met h-\nod between the own retail and wholesale business differs. Another prominent topic \namong the interviewed experts was that the forecast for the ir own stores has to be \nfinished earlier in comparison to the wholesale business and that the assortment in \nthe online shop is broader, deeper and more fashionable, consistent with a younger \ntarget group. The regional adjustment of the forecasting happens in 78 % automat i-\ncally, as the quantities and assortments are planned on store or wholesale customer \nlevel. Especially the national forecast differs from the export countries. In contrast \n22 % do not have any regional distinction regarding their sales forecasting.",
        "5e64f834-ca5e-488a-bb63-e99521762ef3": "16 \n4.3 Sales forecasting management \nTo investigate the functional integration the experts were asked which d e-\npartments are usually involved in the sales forecasting process. Every participant \nfirst named the buying followed by the sales division, together representing the key \nresponsible departments. About half of the experts mentioned a collaboration with \nthe design and controlling, far less with marketing, production and merchandise \nmanagement. Although sales forecasting became very analytic, only one company \nreported the involvement of an IT data analyst. Generally 79 % described a close \ncollaboration, having the input of all important functional areas in the forecast. Still \nin 21 % each functional department independently develops its own forecast. \nInformant 1: \u201cIn the end everybody works separately and the machine then brings it \ntogether to an overall plan. We have once tried coordination meetings, but until \nthey are ready, we are running out of time.\u201d \nThe role of the top management mainly consists in approving the developed \nforecasts or setting the annual overall turnover in case of a top-down management \napproach. In terms of the significance, the majority (75 %) emphasized the great \nimportance of sales forecasting, as it affects the whole supply chain. Nevertheless \n25 % stated that sales forecasting may be necessary, but because precise forecasts \nhave never come true, a huge effort is not worth it. \nAs for the performance measurement, 75 % of the interviewed companies \nevaluate the accuracy of their sales forecasting as a plan-actual-comparison on item \nlevel, which is principally realized monthly, but with an increasing tendency to week-\nly or even daily, in order to react as quickly as possible on market trends. On the \ncontrary 25 % do not measure the forecasting performance. These companies try to \ngather a learning effect through bad-best-seller lists. \n4.4 Difficulties and satisfaction of the current sales forecasting process \nWhen it co mes to the specificities of the fashion industry the interviewed \nexperts agreed that one of the biggest challenges regarding the forecasting consists \nof the extreme variations in sales primary caused by external factors, with the result \nthat a few best-sellers become very strong and on the other hand bad-sellers remain \nvery poor. Another frequently mentioned problem was the conflict between the fast \npace of trends and the long lead times, which makes short-term reactions on the",
        "9958311d-394b-482c-8528-c8a34d2c1075": "17 \nmarket extremely difficult. In addition to that the fashion companies complained \nabout constant discount battles and a high competitive pressure. Many interviewees \nalso see a major problem of the fashion industry in the management of the goods, \nwhich is not orientated to the needs of the customer. At the same time customer \nbehavior gets more unpredictable and spontaneous. \nTo handle extreme fluctuations and fast-moving trends most of the inte r-\nviewed companies keep an open-to-buy budget for the reorder of best-sellers or for \ntrendy flash programs and carry out in-season management. Therefore some e x-\nperts also stressed their close cooperation with suppliers or wholesale partners. A \nrarely mentioned action was the strategic choice of the production countries, not \ntoo far away, so that the reaction and transportation time get reduced. \nInformant 11: \u201cWe always try to produce as lately as possible. Therefore we produce in cou n-\ntries like Morocco, where we can react quickly. (\u2026) But on the other side we have to achieve \na greater flexibility with our far-east suppliers, by only fabric or capacity blocking or by pr o-\nducing white items and making the color in the very last step of the process (\u2026) sometimes \nnot before it comes to Europe.\u201d \nMore popular among the interviewed experts were price reductions as well \nas spontaneous marketing actions in response to market variations. Moreover the \nexpertise of buyers and sales persons adjusting the forecasts to external factors and \ntrends plays an essential role . Still some companies - wholesale-driven specialists - \ndid not see themselves much affected by the specificities of the fashion industry, ha-\nving a stable product range and consequently steady sales and stock volumes. When \nit comes to new items without historical sales data \u2013 another specificity of the fas h-\nion industry \u2013 88 % of the interviewed companies refer to the sales data of comp a-\nrable articles. 42 % additionally indicated the importance of experience and fee d-\nback from wholesale partners and finally 29 % used to initially plan small quantities \nand simply accept a certain degree of risk. \nWhen asked for their satisfaction with the current sales forecasting, 29 % of \nthe participative experts stated their discontent, whereas 71 % were satisfied, even \nthough mainly with the process, not yet with the system-based support. This was \nconsequently the most frequent answer (83 %) regarding potential for improvement. \nThe fashion companies want a better system-based support, in order to bring t o-\ngether sales data, product data, customer data and marketing data and to conduct \nmore detailed analysis on item and store level. This leads to a better overview and \ndata base for the decision makers, superior customer orientation and less depen d-",
        "a34a549a-f349-4d90-a938-9d5d5f56fdf2": "18 \nence on subjective assessments. Besides several experts complained about availabil-\nity and validity of sales and customer data, expandable EDI connection with whol e-\nsale partners, acceptance of the sales forecasts and the lead times. Interestingly, a \nfew respondents wish to simplif y the sales forecasting process instead of the intr o-\nduction of computer-based systems. \n  \n4.5 Application and potential of predictive analytics to sales forecasting \n \n First of all  the familiarity with predictive analytics was queried. 42  % of the \nexperts were not at all familiar, 58 % knew the basic idea, but emphasized that they \nare just at the beginning of the change process towards Big Data. Regarding the ap-\nplication only one company already worked with predictive analytics within the NOS \narea and another one was about to implement it as a forecasting tool. The math e-\nmatical model behind it probably is a neural network with self-learning algorithms. \n Nevertheless, when asked about the future potentia l of predictive analytics \nas sales forecasting method in fashion business, only two out of 24 interviewed com-\npanies \u2013 a wholesale driven specialist and one department store \u2013 denied its huge \npotential. All the others saw a great opportunity in predictive analytics, as it could \nrepresent the desired syste m-based support. Many of these experts identified the \nmost potential in the forecasting of NOS items, as they have large historic data sets.  \nAdditionally they would like to use predictive analytics within t he marketing area, \nanalyzing customer data for target marketing and shopping basket analysis. 58 % of \nthe interviewed companies already applied  predictive analytics for this purpose to \nthe marketing, but not yet to the forecasting process. The wholesale driven compa-\nnies mainly restricted the potential on the business with end customers, since pr e-\ndictive analytics is based on the recognition of patterns in a huge data set. \n As another objective of this research was to find out the reasons why predic-\ntive an alytics is not yet widespread in the fashion industry, the companies were \nasked to describe obstacles to the implementation. The most frequently mentioned \nproblem relates to the data and its availability, quality as well as variety. But as a \ncondition for the application of predictive analytics sales, customer and article data \nmust first be maintained and united, which requires great effort and certain IT-infra-\nstructure. Another relevant topic was the comp lexity of predictive analytics, leading \nto a lack of comprehensibility and therefore to a lack of confidence  and acceptance \nby the users. At the same time many experts indicated that they currently do not \nhave the resources regarding time, costs and qualified personnel  to introduce pr e-\ndictive analytics. In the end it is always a cost-benefit issue. Some experts also com-",
        "7a1e151e-6002-46cd-8c29-249fd4424162": "19 \nplained that there are too many external, soft factors causing extreme variations, \nwhich cannot be modeled by any system. Finally, when it came to the clustering \ntechnique in order to forecast new items, the company already using predictive ana-\nlytics for NOS articles mentioned the difficulty to find comparable attributes among \nthe items and the problem that customer behavior is still not corresponding to these \nattributes and therefore unpredictable. Examples of dominant obstacles are listed \nwith participant comments in table 1. \nObstacle Participant Comment \nData availability, \nvariety and    \nquality \n\u201cIn my opinion the problem is the quality of the data, as we need to \ncorrelate a lot of data in various constellations with each other. (\u2026) \nThen we have to make huge efforts to enter as a first step all the basic \nproduct data, attributes and cluster assignments into the system.\u201d \n\u201c(\u2026) we have not yet structured data from all involved departments.\u201d \n\u201cAt the moment the biggest obstacle is the IT -infrastructure. As a first \nstep we have to bring together the different data sources - sales data, \ncustomer data and marketing information.\u201d \n\u201cAt first we have to introduce a customer card and collect data. (\u2026) \nWith this the desire to analyze all the information will grow. This will \nfacilitate the investments for predictive analytics.\u201d \n\u201cIn our case it is the EDI con nection with our wholesale partners  and \ntherefore the availability of the sales data.\u201d \n\u201cIf you want to do it properly, you need the sales data from everybody, \nworldwide. I consider this utopia.\u201d \nLack of compre-\nhensibility and \ntrust because of \nthe complexity \n\u201cI am opposed to things passing automatically through machines (\u2026) In \nthe end there are successful buyers behind it and without them you can \nforget any system.\u201d \n\u201cThe challenge is to connect pred ictive analytics with the employees, so \nthat the system pre -defines a forecast, which is understood and adjus t-\ned by the operatives. (\u2026) The operational level will not accept it, if it is \nnot understood by them.\u201d",
        "b0e00c06-c7c3-46f2-ab85-c0084e387f98": "20 \n\u201cThe biggest problem is the acceptance of this method by the decision -\nmakers, who sometimes make the decisions driven by vanity. A long -\nserving buyer should now count on an automatic data analysis? It  is not \nthat simple.\u201d \n\u201cI experienced that, as soon as a system or  algorithms calculate the \nforecast, the user cannot interpret it anymore and tends to either copy \nit to 100 percent or totally mistrust the system, which leads to an a c-\nceptance rate of zero percent.\u201d \n\u201cIt always involves  change. This needs  time and people have to be will-\ning. (\u2026) First you have to explain them the cost-benefit effect.\u201d \nLack of resources \u201cWe have enough data from the POS, but sometimes simply not the \nresources to analyze them respectively.\u201d \n\u201cPredictive analytics has no longer been pursued because of the costs, \ncapacities and resources.\u201d \nExternal factors \u201cI have to say quite clearly that the systems can only be as good as the \nuser, who feeds it. A lot of information has to be provided for a good \nresult. The system has to know all the factors, influencing the sales.\u201d \n\u201cEmpirically the more variables, the more complex  and imprecise the \nforecast. With all the factors of the fashion industry, it is not calculable, \nforget it. Some logic, common sense and a good data base are much \nmore worth.\u201d \nUnpredictable \ncustomer \nbehavior \n\u201cOf course we can get some more insights, but this should not be ove r-\nestimated. For this our industry is too fast and difficult to predict. (\u2026) It \nis incredibly hard to grab the end consumer, as he has so many oppor-\ntunities nowadays due to omni-channel strategies.\u201d \n\u201cThe problem is that the attributes of the items are not generalizable. \nFirst it is very difficult to find comparable attributes. Secondly the cu s-\ntomer does not behave according to these attributes. For the customer \nthe first impression counts, which sometimes has nothing to do with the \nattributes.\u201d \nTable 1: Obstacles for the application of predictive analytics with participant comments",
        "191a4eb0-c36f-40aa-8f33-dcdab36f220f": "21 \n5. Discussion and implications\nInitially it has to be stated that regarding the application of different sales \nforecasting methods to the fashion industry t he research results clearly confirm \nwhat Thomassey assumed in former studies: The majority of the fashion companies \nstill make use of simple time series analysis, basically sales of last year, combined \nwith qualitative adjustments in order to incorporate the specificities of the fashion \nindustry. Thus the interviewed companies give enormous weight on the employee\u00b4s \npersonal estimation and expertise. As already evaluated above this approach is quite \nsimple, comprehensible and long-established, but unfortunately not suitable for big \ndata sets and less analytic. Likewise also many experts, mainly in controlling pos i-\ntions, showed themselves unsatisfied and therefore willing for a change in the sales \nforecasting method . The top answer to the question \u201cWhat would you do, if you \ncould change one thing within your curren t sales forecasting process?\u201d was to intr o-\nduce a better system-based support, in order to become more analytical and achieve \ncertain security in times of extreme fluctuations, fast pace and unpredictable cu s-\ntomer behavior. Predictive analytics could serve not only as a computer-based sup-\nport system, but also as an automatic forecasting tool. \nThe fact that almost half of the interviewees, above all wholesale-driven \ncompanies, have never heard about predictive analytics so far and only one comp a-\nny already applies it (even though only to the NOS items) shows, that the fashion \nindustry is just at the beginning of its transformation towards computer-based for e-\ncasts. Interestingly predictive analytics enjoys a broader application in the marketing \narea, when it comes to the analysis of customer data and the targeted selection of \ncustomers for a certain marketing action. The interest in the topic definitely exists . \nRegardless of the size, segment and distribution strategy of the companies as well as \nregardless of the department or experience of the interviewed experts 92 % see \ntheoretically great potential and the future in predictive analytics, especially in the \narea of the more stable NOS items. But because of missing empirical knowledge \nabout the practical success of predictive analytics within the fashion industry the \ninterviewed experts are still very skeptical. As shown in table 1 there are a lot of \nobstacles that have to be overcome first, so that the companies did not seem to be \nready yet for such a big change. Besides the described problems concerning the d a-\nta, complexity, acceptance, resources, explanatory factors and customer behavior, \nthe mere description of the current sales forecasting process by the experts revealed \nthat generally their mentality has to be changed.",
        "bb80f00b-72fc-46f6-a47b-777ad90f379b": "22 \nIn the following some practical implications are presented, in order to finally \nprovide the right analytics culture in terms of an organization\u00b4s attitude to data, \nanalytics and automated decision making, driven by predictive analytics, as base for \nthis transformation (Finlay, 2014). \nConcerning the number, horizons, update intervals and lead times of the \nsales forecasts the fashion companies still think too rigidly in the long wholesale \nrhythms of spring/summer and fall/winter season . However, given the nature of the \n(fast) fashion industry, very short-term forecasting and quick lead times are ever \nmore important. Therefore the interviewed fashion companies have to become fas t-\ner and realize more, short-term orientated forecasts taking advantage of the speed \nof predictive analytics. This would also lead to an improvement of the ir flexibility \nand reaction time. \nData as the input of every forecasting and its availability has always been a \nbig issue in the interviews. Of course, the first step consists of obtaining the real \nsales figures from the wholesale partners, of collecting some customer da ta and of \nmerging all the data from the different departments, but at the same time the fash-\nion companies must recognize the value of that data. Too many companies indica t-\ned,\n that they have an online shop as well as a customer card, but they do not use or \nintegrate this information in the sales forecasting. But how can they benefit from it , \nif not analyzed and integrated? To value the data is also an initial step to become a \ndata driven company - another condition for the successful implementation of pr e-\ndictive analytics. Decisions will become more evidence-based, drawn from what \npredictive analytics gathers from the data. This does not mean that experience and \nhuman judgement are unimportant, but they will lose weight compared with the \nbasic sales forecast delivered automatically by the predictive analytics system. \nRegarding the sales forecasting management it is essential to have the \ncommitment of senior managers who highlight the importance and the big impact of \nsales forecasting on the whole supply chain, which is currently not rated high ly \nenough. In addition to that the top management must support the use of predictive \nanalytics. This will also facilitate the allocation of sufficient financial and human r e-\nsources. Up to now only one of the interviewed companies mentioned the involv e-\nment of data analysts in the sales forecasting development. In the future a skilled \nanalytical team is needed to support, explain, train and convince the users of predic-\ntive analytics. Thus the understanding and consequently a better acceptance rate \namong the users should be realized, too. In general the functional integration not \nonly of the business intelligence team, but of the whole sales forecasting process \ncould be improved, so that the input from all the different areas is recognized and",
        "ca802da6-caf1-47db-8aab-c910f0ca88b5": "23 \nforecasting is seen as a cross-functional process that serves the entire company. As \nthe sales forecasting will become more analytic al, the accuracy of the forecast on \nitem level must be measured, in order to constantly check and improve the system. \nAt the moment the performance measurement of the forecasts only plays a subo r-\ndinate role and should not be confused with the sales performance of the items.  \nFinally the widespread top-down approach setting overall sales targets i n-\nstead of basing the business plan upon the sales forecasts, leads to frustration \namong the forecasting personnel. On the other side a bottom-up approach emph a-\nsizes the importance of an accurate sales forecasting and should consequently be \nintegrated in the top-down target setting procedure. \nDuring the transformation process the three-stage model described before \ncan serve as an orientation. In the very first step the companies should concentrate \non the generation of a clean as well as complete data base and the application of \ndescriptive analytics. In doing so essential basic analyses of the data are performed \nand first insights can be discovered. Only then and after establishing the necessary \nanalytics culture predictive analytics can be introduced. In the last step prescriptive \nanalytics should convert the fashion companies finally into proactive rather than \nreactive market players. Potential for improvement can be found within the handling \nof market fluctuations, which often consists of very reactive price reductions and \nmarketing actions. \nAs the sample size of this research was considerably large including a great \nvariety of different fashion companies in terms of the size, segments, distribution \nchannels and strategies the findings can be regarded as significant, reflecting a broad \nindustry profile. Nevertheless,\n future research should be conducted including the \npure online-players or fast fashion companies, who are probably more advanced in \nthe topic of predictive analytics and could report about their experience in practice. \nThough this study makes a substantial contribution to the literature, since the status \nof predictive analytics within the fashion industry has never been investigated \nthrough expert interviews before. In this way difficulties could be identified, which \nfor the moment prevent the fashion companies from the application of predictive \nanalytics on their sales forecasting.",
        "2d8c541d-f9df-4346-9089-a31d9fece198": "24 \nReferences \nAbbott, D. (2014). Applied Predictive Analytics: Principles and Techniques for the \nProfessional Data Analyst. Indianapolis: John Wiley & Sons. \nAlon, I., Qi, M., & Sadowski, R. J. (2001). Forecasting aggregate retail sales: a compar-\nison of artificial neural networks and traditional methods. Journal of Retail-\ning and Consumer Services , 8(3), 147 \u2013156. http://doi.org/10.1016/S0969-\n6989(00)00011-4 \nBari, A., Chaouchi, M., & Jung, T. (2014). Predictive Analytics for Dummies . New Jer-\nsey: John Wiley & Sons. \nBogner, A., Littig, B., & Menz, W. (2005). Das Experteninterview: Theorie, Methode, \nAnwendung (2nd ed.). Wiesbaden: VS Verlag. \nBogner, A., Littig, B., & Menz, W. (2014). Interviews mit Experten: Eine praxisorie n-\ntierte Einf\u00fchrung. Wiesbaden: VS Springer. \nBrannon, E. L. (2010). Fashion Forecasting (3rd ed.). New York: Fairchild Books. \nChoi, T.-M., Hui, C.-L., & Yu, Y. (2011). Intelligent time series fast forecasting for fash-\nion sales: A research agenda (pp. 1010 \u20131014). IEEE. \nhttp://doi.org/10.1109/ICMLC.2011.6016870 \nDresing, T., & Pehl, T. (2013). Praxisbuch Interview, Transkription & Analyse: Anle i-\ntungen und Regelsysteme f\u00fcr qualitativ Forschende  (5th ed.). Marburg: E i-\ngenverlag. \nErlinger, M. (2014, November 19). TW-Rangliste: Die Gr\u00f6\u00dften werden gr\u00f6\u00dfer. Tex-\ntilWirtschaft. Retrieved from http://www.textilwirtschaft.de/business/TW-\nRangliste-Die-Groessten-werden-groesser_94638.html?a=1 \nFinlay, S. (2014). Predictive Analytics, Data Mining and Big Data: Myths, Misconce p-\ntion and Methods. Hampshire: Palgrave Macmillan.",
        "f3a3153b-9384-4ce8-aa3c-5da457f50906": "25 \nGuo, Z., Wong, W., Leung, S., & Li, M. (2011). Applications of artificial intelligence in \nthe apparel industry: a review. Textile Research Journal, 81(18), 1871\u20131892. \nhttp://doi.org/10.1177/0040517511411968 \nKaiser, R. (2014). Qualitative Experteninterviews: Konzeptionelle Grundlagen und \npraktische Durchf\u00fchrung. Wiesbaden: Springer. \nKim, E., Fiore, A. M., & Kim, H. (2011). Fashion Trends: Analysis and Forecasting. New \nYork: Berg. \nKoch, R. (2015). From Business Intelligence to Predictive Analytics. Strategic Finance, \n96(7), 56\u201357. \nKoch, R., & Hamilton, B. (2015). From predictive to prescriptive analytics. Strategic \nFinance, 97(6), 62\u201363. \nLittle, J. D. C. (1998). Integrated measures of sales, merchandising, and distribution. \nInternational Journal of Research in Marketing , 15(5), 473 \u2013485. \nhttp://doi.org/10.1016/S0167-8116(98)00015-9 \nLiu, N., Ren, S., Choi, T.-M., Hui, C.-L., & Ng, S.-F. (2013). Sales Forecasting for Fas h-\nion Retailing Service Industry: A Review. Mathematical Problems in Engineer-\ning, 2013, 1\u20139. http://doi.org/10.1155/2013/738675 \nLiu, Y. (2015). Big Data and Predictive Business Analytics. The Journal of Business \nForecasting, 33(4), 40\u201342. \nMakridakis, S., Wheelwright, S. C., & Hyndman, R. J. (1998). Forecasting: Methods \nand Applications (3rd ed.). New York: John Wiley & Sons. \nMayring, P. (2007). Qualitative Inhaltsanalyse: Grundlagen und Techniken  (9th ed.). \nWeinheim: Beltz Deutscher Studien Verlag. \nMentzer, J. T., & Moon, M. A. (2005). Sales Forecasting Management: A Demand \nManagement Approach (2nd ed.). London: Sage Publications. \nMiller, T. W. (2014). Modeling Techniques in Predictive Analytics: Business Problems \nand Solutions with R. New Jersey: Pearson Education.",
        "f1f592fc-43b0-4ba2-aa9d-972bdd75febd": "26 \nProbe, A. (2014, October 1). TW-Rangliste: Luxus legt zu, Sport schw\u00e4chelt. Textil-\nWirtschaft. Retrieved from http://www.textilwirtschaft.de/business/TW-\nRangliste-Luxus-legt-zu-Sport-schwaechelt_93917.html?a=2 \nSharda, R., Asamoah, D. A., & Ponna, N. (2013). Business Analytics: Research and \nTeaching Perspectives (pp. 3 \u20138). University Computing Centre - SRCE. \nhttp://doi.org/10.2498/iti.2013.0589 \nSiegel, E. (2013). Predictive Analytics: The power to predict who will click, buy, lie or \ndie. New Jersey: John Wiley & Sons. \nSun, Z.-L., Choi, T.-M., Au, K.-F., & Yu, Y. (2008). Sales forecasting using extreme \nlearning machine with applications in fashion retailing. Decision Support Sys-\ntems, 46(1), 411\u2013419. http://doi.org/10.1016/j.dss.2008.07.009 \nThomassey, S. (2010). Sales forecasts in clothing industry: The key success factor of \nthe supply chain management. International Journal of Production Econo m-\nics, 128(2), 470\u2013483. http://doi.org/10.1016/j.ijpe.2010.07.018 \nThomassey, S. (2014). Sales Forecasting in Apparel and Fashion Industry: A review. In \nT.-M. Choi, C.-L. Hui, & Y. Yu (Eds.), Intelligent Forecasting Systems: Models \nand Applications (pp. 9\u201327). Heidelberg: Springer. \nThomassey, S., & Happiette, M. (2007). A neural clustering and classification system \nfor sales forecasting of new apparel items. Applied Soft Computing , 7(4), \n1177\u20131187. http://doi.org/10.1016/j.asoc.2006.01.005 \nThomassey, S., Happiette, M., & Castelain, J.-M. (2005). A global forecasting support \nsystem adapted to textile distribution. International Journal of Production \nEconomics, 96(1), 81\u201395. http://doi.org/10.1016/j.ijpe.2004.03.001 \nVaagen, H., & Wallace, S. W. (2008). Product variety arising from hedging in the fash-\nion supply chains. International Journal of Production Economics , 114(2), \n431\u2013455. http://doi.org/10.1016/j.ijpe.2007.11.013 \nZhang, X. (2004). The impact of forecasting methods on the bullwhip effect. Interna-\ntional Journal of Production Economics , 88(1), 15 \u201327. \nhttp://doi.org/10.1016/S0925-5273(03)00128-2",
        "806de7af-3a9a-460d-9fbe-f7c74ce99a74": "27 \nZhao, X., Xie, J., & Lau, R. S. M. (2001). Improving the supply chain performance: Use \nof forecasting models versus early order commitments. International Journal \nof Production Research , 39(17), 3923\u2013 3939. \nhttp://doi.org/10.1080/00207540110072236",
        "f5900889-ad88-4ee9-aa60-f1f913993a39": "ARTICLE  IN  PRESS\nInternational Journal of Forecasting ( ) \u2013\nwww .elsevier.com/locate/ijforecast\nEffective forecasting and judgmental adjustments: an empirical\nevaluation and strategies for improvement in supply-chain\nplanning\nRobert Fildes a,\u2217, Paul Goodwin b, Michael Lawrence c, Konstantinos Nikolopoulos d\na Department of Management Science, Lancaster University, LA1 4YX, UK\nb School of Management, University of Bath, BA2 7AY , UK\nc Australian School of Business, University of New South W ales, Sydney, Australia\nd Manchester Business School, Manchester M15 6PB, UK\nAbstract\nDemand forecasting is a crucial aspect of the planning process in supply-chain companies. The most common approach to\nforecasting demand in these companies involves the use of a computerized forecasting system to produce initial forecasts and\nthe subsequent judgmental adjustment of these forecasts by the company\u2019s demand planners, ostensibly to take into account\nexceptional circumstances expected over the planning horizon. Making these adjustments can involve considerable management\neffort and time, but do they improve accuracy , and are some types of adjustment more effective than others? T o investigate this,\nwe collected data on more than 60,000 forecasts and outcomes from four supply-chain companies. In three of the companies,\non average, judgmental adjustments increased accuracy . However, a detailed analysis revealed that, while the relatively larger\nadjustments tended to lead to greater average improvements in accuracy , the smaller adjustments often damaged accuracy . In\naddition, positive adjustments, which involved adjusting the forecast upwards, were much less likely to improve accuracy than\nnegative adjustments. They were also made in the wrong direction more frequently , suggesting a general bias towards optimism.\nModels were then developed to eradicate such biases. Based on both this statistical analysis and organisational observation, the\npaper goes on to analyse strategies designed to enhance the effectiveness of judgmental adjustments directly .\nc\u20dd 2008 International Institute of Forecasters. Published by Elsevier B.V . All rights reserved.\nKeywords: Forecasting accuracy; Judgment; Heuristics and biases; Supply chain; Forecasting support systems; Practice; Combining; Forecast\nadjustment\n\u2217 Corresponding author.\nE-mail addresses:\nR.Fildes@lancaster.ac.uk (R. Fildes),\nP .Goodwin@bath.ac.uk (P . Goodwin),\nmichael.lawrence@unsw .edu.au (M. Lawrence),\nKostas.Nikolopoulos@mbs.ac.uk (K. Nikolopoulos).\n1. Introduction\nSupply chain planning is usually reliant on demand\nforecasts at the stock keeping unit (SKU) level.\nThe accuracy achieved for these forecasts has\nconsequences for companies at all levels of the supply\n0169-2070/$ - see front matter c\u20dd 2008 International Institute of Forecasters. Published by Elsevier B.V . All rights reserved.\ndoi:10.1016/j.ijforecast.2008.11.010\nPlease cite this article in press as: Fildes, R., et al. Effective forecasting and judgmental adjustments: an empirical evaluation and strategies\nfor improvement in supply-chain planning. International Journal of Forecasting (2009), doi:10.1016/j.ijforecast.2008.11.010",
        "00e11d7c-f189-41cb-adc4-fd827046f561": "ARTICLE  IN  PRESS\n2 R. Fildes et al. / International Journal of F orecasting ( ) \u2013\nchain, from the retailer to the raw materials supplier,\nand even for companies whose \ufb01nal product is \u2018make-\nto-order\u2019 (\nY elland, 2006 ). Errors at each stage of\nthe chain are potentially ampli\ufb01ed, resulting in poor\nservice or excess inventory levels. The forecasting\ntask is dif\ufb01cult due to the inter-related nature of the\ndata series, the presence of outliers, level and trend\nshifts (\nFildes & Beard, 1992 ), and the impacts of the\nmarket and general economic environment. These data\ndif\ufb01culties are compounded by the huge number of\nSKUs that often need to be forecast each period.\nIn order to plan and manage their supply chain,\norganisations typically set up a unit responsible for\nforecasting. Because of the size and complexity\nof the forecasting task, it is generally impossible\nfor all SKUs to be given individual attention by\ndemand planners. The most common approach to\nforecasting demand in support of supply chain\nplanning involves the use of a statistical software\nsystem which incorporates a simple univariate\nforecasting method, such as exponential smoothing,\nto produce an initial forecast. For key products,\nthese initial forecasts (hereafter called the \u2018system\u2019\nforecasts) are reviewed, and may be adjusted by\nthe company\u2019s demand planners to take into account\nexceptional circumstances expected over the planning\nhorizon, or possibly to correct perceived inadequacies\nin the system forecast. This \u2018Sales and Operations\nPlanning\u2019 process is usually carried out in a committee\nsetting, where representatives from marketing, sales,\nproduction and logistics agree on the \u2018\ufb01nal forecast\u2019:\na combination of a statistical forecast and managerial\njudgment.\nImproved demand forecasting accuracy can lead\nto signi\ufb01cant monetary savings, greater competi-\ntiveness, enhanced channel relationships, and cus-\ntomer satisfaction (\nMoon, Mentzer, & Smith, 2003 ).\nHowever, the topic has generated relatively little\norganisationally-based research, and little is known\nabout the effects of the types of judgmental adjustment\n(e.g. large or small, positive or negative) on accuracy ,\nor the extent to which the resulting forecasts are unbi-\nased and ef\ufb01cient (i.e. make optimal use of the avail-\nable information).\nThis paper uses extensive data gathered from four\nsupply-chain companies to address these questions\nand to consider how the process of adjustment can\nbe made more effective. Section\n2 considers the\nliterature on adjustment and proposes hypotheses\nabout the adjustment process. Section\n3 describes\nthe forecasting processes in the four companies and\ngives details of the frequency and nature of the\njudgmental interventions. Section\n4 examines the\ndetailed hypotheses developed in the literature review ,\nwhile Section\n5 evaluates some potential solutions\nto the problems that have been identi\ufb01ed. The \ufb01nal\nsection offers our recommendations for improvements\nin both forecasting support systems and organisational\nforecasting processes.\n2. Literature review and hypotheses\nThere is substantial evidence from the economic\nforecasting literature that statistical forecasts can be\nmade more accurate when experts judgmentally adjust\nthem to take into account the effects of special\nevents and changes that were not incorporated into\nthe statistical model (\nDonihue, 1993; McNees, 1990;\nTurner, 1990). However, few studies have investigated\njudgmental adjustment in the context of company\nforecasts of the demand for SKUs. The exceptions\nwere six studies, all based in the same company ,\nby\nMathews and Diamantopoulos (1986, 1989, 1990,\n1992, 1994) and Diamantopoulos and Mathews\n(1989). These showed that judgmental adjustments\ntend to improve accuracy , though sometimes only\nmarginally , but that they may also introduce bias.\nExperimental evidence on the same question\ngenerally suggests that forecasters often make\nunnecessary judgmental adjustments to statistical\nforecasts (\nLawrence, Goodwin, O\u2019Connor, & Onkal,\n2006). In particular, they make adjustments even\nwhen they do not possess additional information\nabout special events.",
        "62be9298-7f8d-4ae2-aa0e-5e252af4a290": "However, few studies have investigated\njudgmental adjustment in the context of company\nforecasts of the demand for SKUs. The exceptions\nwere six studies, all based in the same company ,\nby\nMathews and Diamantopoulos (1986, 1989, 1990,\n1992, 1994) and Diamantopoulos and Mathews\n(1989). These showed that judgmental adjustments\ntend to improve accuracy , though sometimes only\nmarginally , but that they may also introduce bias.\nExperimental evidence on the same question\ngenerally suggests that forecasters often make\nunnecessary judgmental adjustments to statistical\nforecasts (\nLawrence, Goodwin, O\u2019Connor, & Onkal,\n2006). In particular, they make adjustments even\nwhen they do not possess additional information\nabout special events. There is evidence that this\noccurs because forecasters see patterns in the noise\nassociated with random \ufb02uctuations in the time\nseries (\nHarvey, 1995; O\u2019Connor, Remus, & Griggs,\n1993). In addition, it could be due to the widely\nobserved illusion of control effect, where users\nwho make adjustments exhibit greater con\ufb01dence in\ntheir forecasts (\nKottemann, Davis, & Remus, 1994 ;\nLanger, 1975). Lim and O\u2019Connor (1995) even found\nthat a tendency by forecasters to make damaging\nadjustments persisted, despite a computer display\nshowing that they were reducing the accuracy .\nPlease cite this article in press as: Fildes, R., et al. Effective forecasting and judgmental adjustments: an empirical evaluation and strategies\nfor improvement in supply-chain planning. International Journal of Forecasting (2009), doi:10.1016/j.ijforecast.2008.11.010",
        "a817832b-89dc-4eca-8830-5f62a61cd72c": "ARTICLE  IN  PRESS\nR. Fildes et al. / International Journal of F orecasting ( ) \u2013 3\nHowever, experimental evidence also suggests\nthat when an adjustment is made on the basis of\nevents not re\ufb02ected in the statistical forecast (e.g. a\nforthcoming sales promotion), it is likely to improve\naccuracy , as long as the information about the event is\nreliable (\nGoodwin & Fildes, 1999 ; Lim & O\u2019Connor,\n1996). Forecast adjustments made by experts in a\ncompany environment with access to reliable market\nintelligence, are likely to yield greater bene\ufb01ts than\nthose obtained in experiments by student subjects.\nMost importantly , organisations employ substantial\nresources in the forecast adjustment activity , and\neconomic rationality argues that they must view it as\nvaluable. Thus, we hypothesize:\nH1 : Judgmental forecast adjustments improve\nforecast accuracy .\nThe study by\nMathews and Diamantopoulos (1990)\nfound that managers were able to select the most\ninadequate system forecasts and then to adjust them\nin the correct direction. In addition,\nDiamantopoulos\nand Mathews (1989) found that larger judgmental\nadjustments were more effective in improving forecast\naccuracy than smaller ones. However, as this study\nwas based on a single company , and the statistical\nmethod (Holt\u2019s method) had been \ufb01tted to only\neight quarterly observations, it is possible that these\n\ufb01ndings will not hold for the different companies and\nforecasting systems analysed here. Nevertheless, there\nare a number of reasons why the result relating to\nthe size of adjustments might apply more generally .\nLarge adjustments are likely to be associated with\nreliable information about events which will have\nlarge anticipated effects not re\ufb02ected in the system\nforecast. W e would expect such adjustments to be\nassociated with substantial improvements in accuracy .\nSmaller adjustments are likely to be less effective, for\nthe reasons outlined below .\n(a) If the information on which the adjustment is\nbased is viewed as unreliable, the forecasters are\nlikely to hedge their bets by reducing the size of\nthe adjustment.\n(b) Research on human decision making has shown\na tendency across a number of decision environ-\nments for people to ignore or modify good advice,\nand to demonstrate excessive trust in their own\njudgement. This tendency has been observed in\nthe illusion of control literature (\nKottemann et al. ,\n1994; Langer, 1975), the advice taking literature\n(Bonaccio & Dalal, 2006 ; Y aniv, 2004), where\nusers typically discount good advice, and for com-\nputer mediated advice (\nW aern & Ramberg, 1996 ),\nwhere users are reluctant to trust this form of ad-\nvice compared to human mediated advice. Further-\nmore,\nLim and O\u2019Connor (1995) showed that even\nwhen an impossibly good forecast was provided, 1\nusers still made small adjustments to it, which nat-\nurally reduced its accuracy . This literature consis-\ntently suggests a likelihood for forecasters to make\nsmall modi\ufb01cations to a provided forecast which\nhave no sound basis, and therefore diminish its ac-\ncuracy .\n(c) Based on our observations of forecasting in\ncompanies and discussions with forecasters, we\nsuggest that forecasters often tinker with the\nsystem forecasts merely to demonstrate that they\nhave reviewed the forecasts, and are attending to\nthe task.\nW e characterise an adjustment as either large or\nsmall on a relative, within company , basis, where the\nadjustment is expressed as a percentage of the system\nforecast. Thus we hypothesise:\nH2a : The forecasts selected for adjustment are those\nmost in need of improvement.\nH2b : When adjustments are made, the sizes of the\njudgmental adjustments are positively associated with\nan improvement in accuracy .\nThe relationship between the volatility of a time\nseries (as measured by the coef\ufb01cient of variation of\nthe raw data) and the relative accuracy of judgmental\nand statistical forecasts was examined by\nSanders and\nRitzman (1992). Their study concluded that when\nseries had low coef\ufb01cients of variation, statistical time\nseries methods outperformed judgmental forecasters\nwho had expertise relating to the variables to be\nforecast.",
        "c3c228d2-e7c7-4e08-b9cf-30a669d42a8b": "W e characterise an adjustment as either large or\nsmall on a relative, within company , basis, where the\nadjustment is expressed as a percentage of the system\nforecast. Thus we hypothesise:\nH2a : The forecasts selected for adjustment are those\nmost in need of improvement.\nH2b : When adjustments are made, the sizes of the\njudgmental adjustments are positively associated with\nan improvement in accuracy .\nThe relationship between the volatility of a time\nseries (as measured by the coef\ufb01cient of variation of\nthe raw data) and the relative accuracy of judgmental\nand statistical forecasts was examined by\nSanders and\nRitzman (1992). Their study concluded that when\nseries had low coef\ufb01cients of variation, statistical time\nseries methods outperformed judgmental forecasters\nwho had expertise relating to the variables to be\nforecast. However, the opposite was true when series\nhad high coef\ufb01cients of variation (over about 30%).\nMoreover, the experts increasingly outperformed\nstatistical time series methods as the volatility of\nthe series increased. This suggests that, for the\nseries they examined, higher levels of volatility were\nmore the result of the effects of special events, of\nwhich the judgmental forecasters had some prior\n1 The \u2018impossibly good forecast\u2019 was developed by averaging an\nexponential smoothing forecast with the next actual.\nPlease cite this article in press as: Fildes, R., et al. Effective forecasting and judgmental adjustments: an empirical evaluation and strategies\nfor improvement in supply-chain planning. International Journal of Forecasting (2009), doi:10.1016/j.ijforecast.2008.11.010",
        "76f23785-ff0b-433b-8948-797b0929ba71": "ARTICLE  IN  PRESS\n4 R. Fildes et al. / International Journal of F orecasting ( ) \u2013\nknowledge, than of noise. When the volatility in a\nseries re\ufb02ects noise or unanticipated discontinuities,\nthere is evidence that judgmental forecasters will\nperform poorly relative to statistical methods as the\nvolatility increases (\nO\u2019Connor et al., 1993 ). This\nis because the forecasters\u2019 propensity to overreact\nto noise is exacerbated in conditions of high noise\n(\nGoodwin & Fildes, 1999 ; Harvey, 1995). Because\nour company time series were regularly affected by\nspecial events, like sales promotions, about which\nthe forecasters usually had advance information, we\nhypothesise that:\nH3 : Judgmental forecast adjustments improve the\nforecast accuracy more under high volatility than low\nvolatility conditions.\nWhile accuracy is the most important property for\na forecast, two further properties are also important:\nbias and ef\ufb01ciency . Bias, itself, can be decomposed\ninto two components (\nTheil, 1966 ). Mean bias is a\nsystematic tendency for the forecast to be either less\nor greater than the actual. Regression bias is the\nextent to which the forecasts systematically fail to\ntrack the actual observations. For example, forecasts\nmay tend to be too high when outcomes are low and\ntoo low when outcomes are high. Ef\ufb01ciency is the\nproperty that forecasts optimally incorporate relevant\nnew information as it becomes available. While\nadjustments might improve accuracy , they may not be\neither unbiased or ef\ufb01cient.\nLawrence, O\u2019Connor, and\nEdmundson (2000) found that judgmental forecasts\nmade by 13 large manufacturing organisations were\ngenerally neither unbiased nor ef\ufb01cient. They found\nthat many forecasters faced a situation of asymmetric\nmanagement incentives (depending on the sign of the\nforecast error), which resulted in biased forecasts.\nIn particular, differences between over-stocking costs\nand under-stocking costs may lead to forecast\nbias, though the direction seems to depend on the\norganisational circumstances (\nSanders & Manrodt,\n1994, and Stewart in Fildes et al., 2003 ). In addition,\nit has been found, both in the UK ( Fildes & Hastings,\n1994) and in the US ( Galbraith & Merrill, 1996 ), that\naround 60% of forecasts were negatively affected by\norganisational politics (also see\nDeschamps, 2004 ).\nThe literature also provides plenty of evidence for\nover-optimism in management judgment, and has\nbeen shown to impact a wide variety of forecasts\nincluding security analysts\u2019 forecasts (\nHelbok &\nW alker, 2004 ), project time prediction ( Buehler\n& Grif\ufb01n, 2003 ) and capital budgeting ( Flybjerg,\nBruzelius, & Rothengatter, 2003 ). Moreover, in their\nstudy in a health products company , Mathews and\nDiamantopoulos (1989) found evidence suggesting an\noptimism bias in managers\u2019 revisions of forecasts,\nthough in this case the adjustments may have been\npartly a reaction to systematic underestimation by the\nstatistical forecast. W e therefore hypothesise:\nH4 : The judgmentally adjusted forecasts are biased.\nThere are a number of reasons why forecasts\nbased, in part, on management judgment are likely\nto be inef\ufb01cient; that is, where the forecasts could\nbe improved by modifying them to take into account\ninformation available to the forecaster at the time.\nHuman cognitive limitations mean that people will\nstruggle to optimally incorporate in their forecasts the\neffects of information from multiple sources (\nFildes,\n1991). As a result, they may restrict their attention to\nonly one or two sources. Moreover, when estimating\nthese effects, forecasters may over-rely on the recall\nof single analogies from the past, and they may anchor\ntoo closely to these recalled effects (\nLee, Goodwin,\nFildes, Nikolopoulos, & Lawrence, 2007 ). In addition,\nthe \u2018escalation of commitment\u2019 literature ( Staw , 1976)\ndemonstrates a strong reluctance in human judgment\nto modify a view already held about the future.",
        "fa5d50f2-9db0-4fd2-b906-4d89acd9120c": "Human cognitive limitations mean that people will\nstruggle to optimally incorporate in their forecasts the\neffects of information from multiple sources (\nFildes,\n1991). As a result, they may restrict their attention to\nonly one or two sources. Moreover, when estimating\nthese effects, forecasters may over-rely on the recall\nof single analogies from the past, and they may anchor\ntoo closely to these recalled effects (\nLee, Goodwin,\nFildes, Nikolopoulos, & Lawrence, 2007 ). In addition,\nthe \u2018escalation of commitment\u2019 literature ( Staw , 1976)\ndemonstrates a strong reluctance in human judgment\nto modify a view already held about the future. Thus,\nevidence from actual sales of a lack of response to a\npromotion, for example, is likely not to be believed.\nNo studies of both bias and ef\ufb01ciency have been\ncarried out on forecast adjustments to SKU data.\nHowever, based on the limited evidence available to\ndate, we hypothesise:\nH5 : The judgmentally adjusted forecasts are\ninef\ufb01cient.\nWhile these hypotheses are of theoretical interest,\nmore excitingly they also open up the prospect of\nimproving the accuracy of companies\u2019 forecasting\nprocesses by eradicating any consistent biases,\ninef\ufb01ciencies and size effects. W e examine these\npossibilities later in the paper.\n3. The data base and preliminary analysis\nData have been collected at SKU level from four\ncompanies, three in manufacturing with monthly fore-\ncasts (pharmaceuticals (A), food (B), and household\nproducts (C)), and one retailer (D) forecasting weekly .\nPlease cite this article in press as: Fildes, R., et al. Effective forecasting and judgmental adjustments: an empirical evaluation and strategies\nfor improvement in supply-chain planning. International Journal of Forecasting (2009), doi:10.1016/j.ijforecast.2008.11.010",
        "1dd0fc41-f675-40ce-b5e9-bf94bec004af": "ARTICLE  IN  PRESS\nR. Fildes et al. / International Journal of F orecasting ( ) \u2013 5\nFor companies A\u2013C, all SKUs in the company were\nexamined. For the retailer, data on two product groups\neach supplied by an individual manufacturer were\nmade available (D1 and D2). The data included one-\nstep-ahead statistical systems forecasts, the \ufb01nal fore-\ncasts, and the corresponding actual outcomes. SKUs\nwithout the required continuous forecast history were\nexcluded from the analysis, as were low volume\nSKUs, de\ufb01ned as those with actual outcomes or sys-\ntem forecasts of less than 10 units. In addition, SKUs\nwhere the \ufb01nal forecast is zero were not analysed,\nas these were thought to result from special circum-\nstances such as the particular SKU being withdrawn\nfrom the market. These low volume items have long\nbeen seen as special cases, ideally requiring their own\nparticular statistical forecasting models, and would\nnot typically require the same attention as higher vol-\nume items. As a consequence, the \u2018intermittent de-\nmand\u2019 case has been examined elsewhere, focussing\non inventory effects (\nSyntetos, Nikolopoulos, Boylan,\nFildes, & Goodwin, in press ). However, we refer to the\nmain \ufb01ndings of this study later, to allow comparisons\nbetween the two types of series.\nIn all four organisations the forecasting process was\nobserved and discussions were held with the principal\nforecasters. Each organisation uses a broadly similar\nprocess to estimate their \ufb01nal forecasts. At the start\nof each forecasting period, the statistical \u2018system\u2019\nforecasts are produced using the computer software.\nThe software offers the user options to change the\ndata base or the statistical model. Three companies\nuse systems that are based on variants of exponential\nsmoothing (two being commercial and one developed\nin-house), whilst one uses a commercial system\nrelated to Focus Forecasting (\nGardner & Anderson,\n1997). A forecasting meeting follows, generally\ninvolving forecasting, marketing, production and sales\npersonnel, which then examines the system forecasts\nin the light of various pieces of marketing and other\ninformation, and agrees on the \ufb01nal forecast. Particular\naspects of the recent forecasting performance, such as\na large error, might be drawn to the group\u2019s attention.\nAll company forecasters interviewed af\ufb01rmed that\nthe principal objective of the forecasting process was\nproducing accurate forecasts, as opposed to targets or\npolitically acceptable predictions, and said that their\n\ufb01nal forecasts were not subsequently changed by more\nsenior management.\nT able 1 summarises the full data base, showing the\nnumber of observations containing (i) the statistical\nforecast, (ii) the \ufb01nal forecast, and (iii) the actual\noutcome. The table also shows the percentage of\nforecasts adjusted and the number of SKUs contained\nin the data set. Companies A\u2013C, all manufacturers\nwhich make monthly forecasts, adjust a substantially\ngreater percentage of forecasts than Company D1\nand D2, the retailer which makes weekly forecasts.\nW e group the organisational data in this way\nbecause analysis shows that the groups have common\ncharacteristics; however, any substantive individual\ncompany differences will also be noted.\nV arious measures have been proposed by\nMathews\nand Diamantopoulos (1987) to measure forecast\nadjustment. They argue for a \u2018symmetric measure\u2019,\nwhere the adjustment is measured relative to an\naverage of the system and \ufb01nal forecasts; however,\nas\nGoodwin and Lawton (1999) have shown, their\nproposed measure does not have these properties,\nand we have therefore chosen their more intuitive\nmeasure, de\ufb01ned as 100 \u00d7 (Final forecast \u2013 System\nforecast)/System forecast.\nFig. 1 shows the size of the\nrelative adjustments in the two groups of companies.\nBoth distributions are right skewed, but the retailer\n(D) tends to make larger adjustments.\nT able 2 shows\nthe mean and median sizes of the relative adjustments\ncategorised by the direction of adjustment (a positive\nadjustment is made when the \ufb01nal forecast exceeds\nthe system forecast). It can be seen that positive\nadjustments for both groups of organisations are very\nmuch larger than the negative adjustments (which are\nbounded by zero).",
        "ab4094ed-1a3d-4623-92ff-e4f670f92e37": "They argue for a \u2018symmetric measure\u2019,\nwhere the adjustment is measured relative to an\naverage of the system and \ufb01nal forecasts; however,\nas\nGoodwin and Lawton (1999) have shown, their\nproposed measure does not have these properties,\nand we have therefore chosen their more intuitive\nmeasure, de\ufb01ned as 100 \u00d7 (Final forecast \u2013 System\nforecast)/System forecast.\nFig. 1 shows the size of the\nrelative adjustments in the two groups of companies.\nBoth distributions are right skewed, but the retailer\n(D) tends to make larger adjustments.\nT able 2 shows\nthe mean and median sizes of the relative adjustments\ncategorised by the direction of adjustment (a positive\nadjustment is made when the \ufb01nal forecast exceeds\nthe system forecast). It can be seen that positive\nadjustments for both groups of organisations are very\nmuch larger than the negative adjustments (which are\nbounded by zero).\n4. Accuracy, unbiasedness and ef\ufb01ciency in the\nadjusted forecasts\n4.1. Accuracy\nAlthough the measurement of forecasting accuracy\nis controversial (\nArmstrong & Fildes, 1995 ; Clements\n& Hendry , 1995 ), the use of absolute percentage\nerror measures is now general within company\nsettings (\nFildes & Goodwin, 2007 ). However, the\ndisadvantages of such measures are well known.\nIn particular, they suffer from being sensitive to\nextremes (\nArmstrong & Collopy , 1992 ), which would\nbe problematic in our database, despite our large\nPlease cite this article in press as: Fildes, R., et al. Effective forecasting and judgmental adjustments: an empirical evaluation and strategies\nfor improvement in supply-chain planning. International Journal of Forecasting (2009), doi:10.1016/j.ijforecast.2008.11.010",
        "72efe35b-0c09-48c1-8155-a71ec9e315cd": "ARTICLE  IN  PRESS\n6 R. Fildes et al. / International Journal of F orecasting ( ) \u2013\nT able 1\nDatabase of SKU forecasts by company (3.7% of triples fell into the low volume category and were omitted from further analysis).\nCompanies Data A vailable data T otal complete triples a % adjusted No. of SKUs\nA Monthly 1/2003\u201312/2005 5,428 65 213\nB 5/2004\u201312/2005 2,856 91 296\nC 3/2004\u201312/2005 3,012 63 244\nD1 W eekly 1/2004\u201352/2005 12,789 14 191\nD2 1/2004\u201352/2005 44,899 10 592\nT otal 68984 21 1536\na A triple consists of the actual observation, the system forecast and the \ufb01nal forecast.\nFig. 1. Size of relative adjustments to the forecasts in two groups of companies.\nT able 2\nMean and median relative adjustment by organisational group, and the direction of adjustment.\nOrg. group Direction of adjustment N Relative adjustment\nMean (Trimmed) (%) Median (%) 90th Percentile (%)\nA\u2013C Positive adjustment 4013 57.2 (46.1) 20.1 132\nNegative adjustment 3392 19.0 (18.6) 13.1 46.9\nD1 & D2 Positive adjustment 3049 60.3 (54.4) 32.5 144\nNegative adjustment 2409 22.6 (22.3) 20.2 44.2\nnumber of observations. In evaluating the accuracy\nand bias of the statistical forecasts (SFC) and the\n\ufb01nal forecasts (FFC), we therefore report two more\nrobust variants, the trimmed mean absolute percentage\nerror (MAPE) and the median absolute percentage\nerror (MdAPE). For the MAPE, we have removed\noutlying values using a 2% trim (a value chosen\nto include as many observations as possible, while\nstill being compatible with removing outliers). Other\naccuracy measures such as the symmetric MAPE and\nthe relative absolute error of FFC compared to SFC\nhave been calculated in preliminary work, but tell the\nsame tale and are therefore not reported.\nA summary of accuracy for the groups of\norganisations is given in\nT able 3 , where the errors of\nthe system forecast, the \ufb01nal forecasts, and a na \u00a8\u0131ve\nPlease cite this article in press as: Fildes, R., et al. Effective forecasting and judgmental adjustments: an empirical evaluation and strategies\nfor improvement in supply-chain planning. International Journal of Forecasting (2009), doi:10.1016/j.ijforecast.2008.11.010",
        "c4a3c71f-1cc7-41a0-87df-c170a45127ea": "ARTICLE  IN  PRESS\nR. Fildes et al. / International Journal of F orecasting ( ) \u2013 7\nT able 3\nForecast error by organisational group and the direction of adjustment.\nOrg. Group No. of Observations,\nN (na\u00a8\u0131ve)\nTrimmed MAPE MdAPE\nNa\u00a8\u0131ve System\nforecast\nFinal\nforecast\nNa\u00a8\u0131ve System\nforecast\nFinal\nforecast\nA\u2013C No adjustment 3,174 (3,068) 37.5 60.5 60.5 22.2 13.6 13.6\nPositive adjust 4,013 (3,735) 39.8 29.1 39.6 25.8 20.2 17.6\nNegative adjust 3,392 (3,136) 41.4 46.9 26.6 25.2 20.7 15.7\nD1& D2 No adjustment 50,427 (49,794) 17.5 19.3 19.3 12.7 13.5 13.5\nPositive adjust 3,049 27.6 32.1 64.9 20.9 21.2 43.2\nNegative adjust 2,409 24.5 40.9 28.5 14.7 25.0 20.9\n(random walk) forecast are compared for each type\nof adjustment. The differences between the MAPEs\nand MdAPEs indicate that the absolute percentage\nerrors are right skewed, with some extremely large\nerrors present in the data, even after the 2% trimming.\nBecause of the skew in the trimmed MAPE as a result\nof these extremes, we will focus on the interpretation\nof the MdAPE, noting any contrasting results.\nFor both groups of companies, a comparison of\nthe MdAPEs of the unadjusted system forecasts with\nthose of the system forecasts that were subsequently\nadjusted suggests that forecasters were able to\nidentify the system forecasts that were most in\nneed of adjustment, a result which is consistent\nwith \ufb01ndings of\nWillemain (1991) and Mathews\nand Diamantopoulos (1990), and supports hypothesis\nH2a . The poor performance of those forecasts left\nunadjusted for companies A\u2013C, as measured by the\ntrimmed mean, suggests that there were, however, a\nnumber of forecasts ( > 1%) where the forecasters\nfailed to recognize the need to adjust.\nWilcoxon\u2019s signed paired rank tests were used\nto compare the APEs of the system and adjusted\n\ufb01nal forecasts for each group of companies, both\noverall and for the different directions of adjustment.\nWhilst there was a signi\ufb01cant improvement overall\nfor companies A\u2013C (supporting H 1 ), there was no\nsigni\ufb01cant improvement where the adjustment was\npositive. Moreover, for the retailer (D1 & D2),\nadjustments did not signi\ufb01cantly improve the accuracy\nat all. Indeed, in this case both the system and\n\ufb01nal forecasts were less accurate than the na \u00a8\u0131ve\nforecasts.\nLawrence et al. (2000) also found that\nthe na \u00a8\u0131ve forecasts were often the most accurate\nin their study of judgemental sales forecasting in\nAustralian companies. Thus, while the forecasters\nin all companies could apparently identify situations\nwhere judgmental adjustments were most needed,\nonly negative adjustments in the A\u2013C companies\nsigni\ufb01cantly improved accuracy . The \ufb01nding that\nnegative adjustments are more effective is consistent\nwith the results of the study of products subject\nto intermittent demand conducted by\nSyntetos et al.\n(in press ).\n4.1.1. Effects of size of adjustments and series\nvolatility\nW e now investigate whether the accuracy resulting\nfrom judgmental adjustments is affected by either the\nsize of the adjustments (H 2b ) or the volatility of the\nseries (H 3 ). W e measured the size of the adjustment by\nits absolute size relative to the system forecast, de\ufb01ned\nas:\n100 \u2217 |Final forecast \u2212 System forecast |\nSystem forecast .\nSanders and Ritzman (1992) measured volatility using\nthe coef\ufb01cient of variation of the raw data.",
        "bcc01933-bd64-495d-af2b-1ff81dc1f158": "The \ufb01nding that\nnegative adjustments are more effective is consistent\nwith the results of the study of products subject\nto intermittent demand conducted by\nSyntetos et al.\n(in press ).\n4.1.1. Effects of size of adjustments and series\nvolatility\nW e now investigate whether the accuracy resulting\nfrom judgmental adjustments is affected by either the\nsize of the adjustments (H 2b ) or the volatility of the\nseries (H 3 ). W e measured the size of the adjustment by\nits absolute size relative to the system forecast, de\ufb01ned\nas:\n100 \u2217 |Final forecast \u2212 System forecast |\nSystem forecast .\nSanders and Ritzman (1992) measured volatility using\nthe coef\ufb01cient of variation of the raw data. However,\nwe are particularly interested in volatility which arises\nas a result of special circumstances, as this is where the\njudgemental adjustment is most needed. Further, we\nwish to eliminate volatility which is capable of being\nforecasted, as the adjustment process starts with the\nstatistical forecast. Thus the volatility measure used\nis based on the coef\ufb01cient of variation of the system\nforecast absolute error. Each series was categorised\nas belonging to either a high or low volatility group,\ndepending on whether this measure was less or greater\nthan the median value for that organisation. Finally ,\nPlease cite this article in press as: Fildes, R., et al. Effective forecasting and judgmental adjustments: an empirical evaluation and strategies\nfor improvement in supply-chain planning. International Journal of Forecasting (2009), doi:10.1016/j.ijforecast.2008.11.010",
        "32be630f-90be-45e8-97d8-abf5a357ad6c": "ARTICLE  IN  PRESS\n8 R. Fildes et al. / International Journal of F orecasting ( ) \u2013\nwe de\ufb01ne our measure of the effect of judgmental\nadjustments on improving accuracy as\nFCIMP = 100\n\u2217 ( |Actual \u2212 System forecast | \u2212 |Actual \u2212 Final forecast |)\nActual ,\ni.e. the difference between the absolute percentage\nforecast error from the system forecast and the\nabsolute percentage \ufb01nal forecast error. Note that this\nvariable is positive when the \ufb01nal forecast is more\naccurate than the system forecast (i.e. the adjustment\nhas improved the system forecast) and negative when\nthe adjustment has reduced the accuracy .\nMathews and\nDiamantopoulos (1987) proposed certain alternative\nmeasures, where they normalise the numerator above\nby , alternatively , the system forecast or the adjustment.\nThe measure adopted above has the advantage that it\nmeasures the improvement or degradation in MAPE\nintroduced by the adjustment directly . For \ufb01rms A\u2013C,\nan ANOV A with the size of adjustment (split into\nquartiles), volatility and \ufb01rm as explanatory variables\nshowed \ufb01rm as signi\ufb01cant ( p = 0.023), volatility as\nhighly signi\ufb01cant ( p = 0.002), and size of adjustment\nas highly signi\ufb01cant ( p < 0.001). For the retailer\n(D1 & D2) the ANOV A results showed that \ufb01rm,\nvolatility and size were all highly signi\ufb01cant. The\nresults held both when extreme adjustments were\neliminated and when volatility was measured by the\nstandard deviation of the absolute system error relative\nto the mean actual.\nAs hypothesised in H 2a , the size of judgmental\nadjustments is positively associated with the size\nof accuracy improvements, and\nFig. 2 shows the\nmedian improvement in absolute percentage error\nfor different sizes of relative adjustment. For the\npositive adjustments, the smallest 25% of adjustments\non average lead to lower forecast accuracy for\ngroups A\u2013C, but for the retailer (D1 & D2) positive\nadjustments reduce accuracy , on average, irrespective\nof their size. Negative adjustments pay off for all\ncompanies (apart from the largest adjustments by the\nretailer). Examining the means tells a similar story ,\nalthough positive adjustments for the manufacturers\nhave negative consequences for all sizes. Interestingly ,\nthese results are not entirely consistent with those\nof\nSyntetos et al. \u2019s ( in press ) study of products\nsubject to intermittent demand, which found that small\nadjustments to forecasts of zero demand are likely to\nbe bene\ufb01cial.\nThe results do not support H 3 . While there is a\nsigni\ufb01cant association between volatility and forecast\nimprovement, the improvements are greater for low\nvolatility series, contrary to H 3 . It seems that volatile\nseries are more dif\ufb01cult to forecast, either because\ntheir \u2018jumpiness\u2019 is not associated with foreseeable\nevents or because the forecaster has dif\ufb01culty in\naccurately assessing the effect of events which are\nknown to be occurring in the future.\n4.2. Unbiasedness\nAll of the organisations studied aim to minimize the\nforecast error. An equal propensity to over- and under-\nforecast would suggest that the (trimmed) means\nand medians of the percentage \ufb01nal forecast error\n(PE) are zero (hypothesis H 4 ). These measures are\ndisplayed in\nT able 4 . For both groups, whatever the\ndirection of the adjustment, the distribution was not\ncentred at zero. This also applies to the individual\ncompanies. There is a clear contrast between negative\nand positive adjustments. In all of the companies,\nnegative adjustments are effective in that they tend to\nreduce the mean bias in system forecasts that are too\nhigh. It appears that, once they have decided to make\na negative adjustment, forecasters are realistic about\nthe likely levels of demand. Positive adjustments,\non the other hand, tend to lead to \ufb01nal forecasts\nthat overestimate (i.e., they are too optimistic).",
        "9d0ea767-eecf-4a89-a888-986b1dfd8d2d": "An equal propensity to over- and under-\nforecast would suggest that the (trimmed) means\nand medians of the percentage \ufb01nal forecast error\n(PE) are zero (hypothesis H 4 ). These measures are\ndisplayed in\nT able 4 . For both groups, whatever the\ndirection of the adjustment, the distribution was not\ncentred at zero. This also applies to the individual\ncompanies. There is a clear contrast between negative\nand positive adjustments. In all of the companies,\nnegative adjustments are effective in that they tend to\nreduce the mean bias in system forecasts that are too\nhigh. It appears that, once they have decided to make\na negative adjustment, forecasters are realistic about\nthe likely levels of demand. Positive adjustments,\non the other hand, tend to lead to \ufb01nal forecasts\nthat overestimate (i.e., they are too optimistic). This\ntendency can also be seen in\nT able 5a , which shows\nthat 66% of positive adjustments for companies A\u2013C\nand 83% of those made by D1 and D2 led to forecasts\nthat were too high. The retailer\u2019s positive adjustments\nare applied to system forecasts that are already , on\naverage, too high (according to the mean percentage\nerror), and therefore serve only to exacerbate the bias.\nOnly 46% of the negative adjustments made by the two\ngroups resulted in \ufb01nal forecasts that were too high\n(\nT able 5b).\nOverall, for both groups of companies, the forecasts\nsuffer from mean bias (using a binomial test) with the\nmagnitude of bias often being large, supporting H 4 .\nT o test for both mean and regression bias (H 4 ), we\nestimated the following regression model:\nYi j , t \u2212 Fi j , t \u22121 ( 1) = \u03b1 i + \u03b2 i Fi j , t \u22121 ( 1) + \u03bd i j t , (M1)\nwhere Yi j , t represents the actual sales in the i th\ncompany for the j th SKU in period t , Fi j , t (1) is\nPlease cite this article in press as: Fildes, R., et al. Effective forecasting and judgmental adjustments: an empirical evaluation and strategies\nfor improvement in supply-chain planning. International Journal of Forecasting (2009), doi:10.1016/j.ijforecast.2008.11.010",
        "bc9e7cca-c5ad-4e20-ae12-445edbfae19e": "ARTICLE  IN  PRESS\nR. Fildes et al. / International Journal of F orecasting ( ) \u2013 9\nFig. 2. Effect of size on improvements in forecast accuracy .\nT able 4\nMean bias in adjusted forecasts by organisational group and direction of adjustment.\nOrg. group Direction of\nadjustment\nMean percentage errors Median percentage errors\nNa\u00a8\u0131ve System forecast Final forecast Na \u00a8\u0131ve System forecast Final forecast\nA\u2013C No adjustment 3,174 \u22129.7 \u221244.3 \u221244.3 2.9 \u22121.4 \u22121.4\nPositive adjust 4,013 \u22129.0 5.7 \u221229.6 2.5 9.8 \u22129.4\nNegative adjust 3,392 \u221215.5 \u221238.1 \u22124.5 1.0 \u221214.0 2.2\nD1 & D2 No adjustment 50,427 3.2 \u22124.6 \u22124.6 \u22120.1 \u22121.4 \u22121.4\nPositive adjust 3,049 10.1 \u22127.4 \u221257.5 12.1 0.0 \u221239.2\nNegative adjust 2,409 \u22121.8 \u221235.5 \u22123.5 4.8 \u221222.3 3.6\nT able 5a\nEvidence of optimism bias in positive adjustments.\nOrg. group No. of observations % of times positive\nadjustment is too large\n% of times positive adjustment\nis in wrong direction\nT otal % of positive adjustments\nthat are overoptimistic\nA\u2013C 4013 32.0 34.4 66.4\nD1 & D2 3049 32.4 50.6 83.0\nT able 5b\nEvidence of optimism bias in negative adjustments.\nOrg. group No. of\nobservations\n% of times negative\nadjustment is too large\n% of times negative adjustment is\nin wrong direction\nT otal % of negative adjustments\nthat are overoptimistic\nA\u2013C 3392 25.2 28.3 46.5\nD1 & D2 2409 33.2 20.5 46.3\nthe one period ahead \ufb01nal forecast made at period\nt , and \u03bd i j t is random error. If the company forecasts\nare unbiased in aggregate, \u03b1 i = \u03b2 i = 0 for each\ncompany i . The normal regression assumptions cannot\nbe expected to hold, in that the data have very different\nlevels and the errors can therefore be assumed to\nPlease cite this article in press as: Fildes, R., et al. Effective forecasting and judgmental adjustments: an empirical evaluation and strategies\nfor improvement in supply-chain planning. International Journal of Forecasting (2009), doi:10.1016/j.ijforecast.2008.11.010",
        "90678aa4-28b3-40c7-bf35-c25f8e5777d1": "ARTICLE  IN  PRESS\n10 R. Fildes et al. / International Journal of F orecasting ( ) \u2013\ndepend on the level of Yi j , t , as well as the noise. The\nvariables have therefore been normalised using the\nstandard deviation of sales for each SKU. In addition,\nthe data have been ordered by size of adjustment (in\npercentage terms) into three groups, and models have\nbeen estimated for each sub-group.\nThe sub-sample of forecasts that have been ad-\njusted contains many extreme observations. For exam-\nple, 1.9% of observations have forecast adjustments\ngreater than 250%. W e have removed all observa-\ntions with such large values from the model build-\ning. In addition, when estimating model\n( M1) we have\nremoved outliers (with absolute studentized residu-\nals greater than 2.5) and high leverage points, to en-\nsure that such points do not exert \u2018undue\u2019 in\ufb02uence\non the regression coef\ufb01cients. Sensitivity testing was\ncarried out on these data \ufb01ltering decisions, and the\nresults we present appeared robust. After the adjust-\nment procedure had been applied, the residuals proved\nwell-behaved (with no evidence of non-normality and\nheteroscedasticity for most models), demonstrating\nthe effectiveness of the normalisation process.\nInitially the model was estimated jointly , with the\nparameters (and error distribution) assumed to be\nconstant across companies. Using a general linear\nmodel, with the companies as factors, and Fi j , t \u22121 as\nthe covariate, together with an interaction between\nthem, leads to a rejection of the hypothesis that the\nbiases are independent of company . The two separate\nsets of forecasts available for company D were\nalso tested for equality of the regression parameters,\nwhile the other three companies were examined in a\npairwise fashion, again leading to a formal rejection\nof statistical equivalence, despite face similarities.\nFrom our analysis of the summary statistics on bias,\nthere was also evidence that the direction of the\nadjustment in\ufb02uences the magnitude of bias in the\nadjustments. W e have therefore estimated models both\nfor the direction of adjustment and, separately , for the\nindividual companies as well as the groups.\nT able 6 shows the results of estimating the\nequations for the two groups. For both groups (and all\ncompanies), whatever the direction of the adjustment,\nH4 (the forecasts are biased) is supported. The small\nvalues of R2 and \u03b2 for companies A to C suggest\nthat bias makes a relatively small contribution to the\nforecast errors, though its elimination is still likely to\nbe worthwhile. The table shows that, in general, the\nT able 6\nRegression and mean forecast bias by organisational group and\ndirection of adjustment.\nOrg. group Coef\ufb01cient R2\n(%)\nTheil\u2019s\ndecomposition\nMean Bias/\nRegression (%)\n\u03b1 \u03b2\nA\u2013C\n- Positive adjust 0.103 \u22120.118 6.5 55\n- Negative adjust 0.259 \u22120.059 2.4 319\nD1&D2\n- Positive adjust 0.264 \u22120.366 40 17\n- Negative adjust 0.497 \u22120.188 10 49\nAll coef\ufb01cients signi\ufb01cant at the <0.01% level.\nconstant term is positive, but that all the \u03b2 coef\ufb01cients\nof the \ufb01nal forecasts are negative. At the mean level\nof the \ufb01nal forecasts this leads to an upwards bias,\nin that the \ufb01nal forecast tends to be too high. The\nrelative importance of the mean bias compared to\nthe regression bias can be established using Theil\u2019s\ndecomposition of the Mean Squared Error,\nMSE = ( \u00afY \u2212 \u00afF ) 2 + ( SF \u2212 \u03c1 Y, F SA ) 2\n+ ( 1 \u2212 \u03c1 Y, F SY ) 2\nwhere F is the forecast of Y , \u00afF and \u00afY are the\nrespective means, with standard deviations SF and\nSY , and \u03c1 Y, F is the correlation between the two. The\n\ufb01rst term represents the mean bias, whilst the second\nmeasures the regression bias.",
        "13611d95-bfff-4fbc-b8fe-c8915f04aca9": "constant term is positive, but that all the \u03b2 coef\ufb01cients\nof the \ufb01nal forecasts are negative. At the mean level\nof the \ufb01nal forecasts this leads to an upwards bias,\nin that the \ufb01nal forecast tends to be too high. The\nrelative importance of the mean bias compared to\nthe regression bias can be established using Theil\u2019s\ndecomposition of the Mean Squared Error,\nMSE = ( \u00afY \u2212 \u00afF ) 2 + ( SF \u2212 \u03c1 Y, F SA ) 2\n+ ( 1 \u2212 \u03c1 Y, F SY ) 2\nwhere F is the forecast of Y , \u00afF and \u00afY are the\nrespective means, with standard deviations SF and\nSY , and \u03c1 Y, F is the correlation between the two. The\n\ufb01rst term represents the mean bias, whilst the second\nmeasures the regression bias.\nT able 6 also shows the\nratio of these two biases. There is no evidence of\na consistent pattern where one type of bias is more\nprevalent than the other. Overall, the results con\ufb01rm\nthat bias contributes more to forecast inaccuracy when\nadjustments are positive.\nWhy were the forecasters applying positive adjust-\nments so poorly? Discussions with the companies sug-\ngested that the reasons differed between companies\nA\u2013C and the retailer (D1 & D2). In the latter case it\nemerged that, for many products, managers were con-\nfusing forecasts of demand with decisions on the levels\nof inventory required to meet customer service levels.\nFor example, a forecast of 200 units might be adjusted\nupwards to 250 units so that the probability of a stock\nout was reduced to a level perceived to be acceptable.\nThe managers were unaware of this confusion. Clearly\nthere was a danger that their estimates, labelled as\n\u2018forecasts\u2019, were liable to misinterpretation, or even to\nfurther upward adjustments by others.\nPlease cite this article in press as: Fildes, R., et al. Effective forecasting and judgmental adjustments: an empirical evaluation and strategies\nfor improvement in supply-chain planning. International Journal of Forecasting (2009), doi:10.1016/j.ijforecast.2008.11.010",
        "a6cf2ccf-3ad9-42ef-b989-eabd4924f6a2": "ARTICLE  IN  PRESS\nR. Fildes et al. / International Journal of F orecasting ( ) \u2013 11\nThere was no evidence that this confusion applied\nto companies A\u2013C. Here, there were two plausible\nexplanations for the bias associated with positive\nadjustments: over-optimism and the mistiming of\nestimates of promotion effects. T o identify the most\nlikely explanation, we need to consider the two\nconditions under which positive adjustments can lead\nto forecasts that are too high. Either they can be made\nwhen a negative adjustment was actually required\n(i.e., the adjustment will be in the wrong direction), or\nthey can be adjustments which are too large, despite\nbeing in the right direction.\nA high percentage of positive adjustments were\nmade in the wrong direction (\nT able 5a ). These\nadjustments add substantially to the forecast errors,\nparticularly when they are large. They might be caused\nby the effects of some promotion campaigns occurring\nlater than the forecaster expected, so that an expected\nupward movement in demand does not materialise\nin the early stage of the campaign. However, if\nthis is the case then one would expect subsequent\npositive adjustments to be in the correct direction.\nOur data provides little support for this. For example,\nfor companies A\u2013C, 43.3% of upwards adjustments\nthat were made following a wrong side adjustment\nwere also wrong sided. This suggests that unwarranted\noptimism is the cause of many positive adjustments,\nrather than the mistiming of adjustments.\n4.3. Ef\ufb01ciency\nWith bias established, the next hypothesis to\nexamine is that of ef\ufb01ciency . The most immediate\ndata the forecaster can bring to bear in making the\nadjustment are the time series history , the latest system\nforecast and the most recent forecast errors. If the\nforecast is ef\ufb01cient, the forecast error should not be\npredictable by variables known to the forecaster. For\nall of the companies, the latest observation is known\nprovisionally at the time of making the forecast. A\nsuitable test of ef\ufb01ciency with this information set is\nthe following model:\nDe\ufb01ne\nei j , t = Yi j , t \u2212 Fi j , t \u22121 ( 1)\n= \u03b1 i SFCi j , t \u22121 ( 1) + \u03b2 i, 1 Yi j , t \u22121 + \u03b2 i, 2 Yi j , t \u22122\n+ \u03b3 1 ei j , t \u22121 + \u03b3 2 ei j , t \u22122 + \u03bd i j , t , (M2)\nwhere SFCi j , t is the system forecast made at t .\nThe constant term has been suppressed, as the\nmodel\u2019s objective is to re-weight and combine\nthe available information to explain the observed\nerror. In the model, a signi\ufb01cant coef\ufb01cient for an\nexplanatory variable indicates that the forecaster has\nnot used the information represented by that variable\nef\ufb01ciently , and that by re-weighting that variable,\neither judgmentally or statistically , the forecast could\nbe improved. Before the \ufb01nal estimated equation can\nbe established, the outliers must again be removed, and\nthe errors rendered homoscedastic via normalisation\n(where, as before, the standard deviation of the actuals\nhas been used). There is some limited evidence of\nseasonality in the errors, but its removal through the\ninclusion of dummy variables affects the results only\nslightly .\nDue to space limitations, we present summary\nregressions for the two groups of \ufb01rms A\u2013C and D1\n& D2. Both groups and all companies show signs\nof inef\ufb01ciency , in that their forecast errors can be\nreduced by better weighting the available information\non the past observations and forecast errors (see\nT able 7 ). T ypically , the current forecast assigns too\nlittle weight to the latest observed error, possibly due\nto the fact that it is not always known exactly at the\ntime the forecast is made. For \ufb01rms A\u2013C, there is\nsome slight evidence that the most recent observation\nis mis-weighted. In addition, the system forecast is\nover-weighted.",
        "58239f41-5cc6-40ef-bd59-58f4bfe0adec": "There is some limited evidence of\nseasonality in the errors, but its removal through the\ninclusion of dummy variables affects the results only\nslightly .\nDue to space limitations, we present summary\nregressions for the two groups of \ufb01rms A\u2013C and D1\n& D2. Both groups and all companies show signs\nof inef\ufb01ciency , in that their forecast errors can be\nreduced by better weighting the available information\non the past observations and forecast errors (see\nT able 7 ). T ypically , the current forecast assigns too\nlittle weight to the latest observed error, possibly due\nto the fact that it is not always known exactly at the\ntime the forecast is made. For \ufb01rms A\u2013C, there is\nsome slight evidence that the most recent observation\nis mis-weighted. In addition, the system forecast is\nover-weighted. Interestingly , this result differs from\nthat of\nGoodwin and Fildes (1999), who found in a\nlaboratory study that forecasters ignored the statistical\nforecast when making judgmental interventions to\ntake promotion effects into account. Overall, the\nresults con\ufb01rm those derived from examining bias:\nthere is more inef\ufb01ciency shown by the forecasters\nwhen they have made positive adjustments to the \ufb01nal\nforecasts.\nW e also attempted to pool estimates using a\ngeneral linear model, but this leads to rejection\nof the hypothesis that the error model is constant\nacross companies. No uniform pattern of response\nis observed across the companies. For example,\ncompany B fails to take into account both most recent\nsales and its previous forecast error. Company C shows\nthe least sign of signi\ufb01cant inef\ufb01ciencies. The retailer\nD shows major inef\ufb01ciencies, particularly with regard\nto the most recent errors.\nPlease cite this article in press as: Fildes, R., et al. Effective forecasting and judgmental adjustments: an empirical evaluation and strategies\nfor improvement in supply-chain planning. International Journal of Forecasting (2009), doi:10.1016/j.ijforecast.2008.11.010",
        "14b2b90d-2ad4-4129-b601-53bfdb43ada4": "ARTICLE  IN  PRESS\n12 R. Fildes et al. / International Journal of F orecasting ( ) \u2013\nT able 7\nThe ef\ufb01ciency of the \ufb01nal forecasts by company and adjustment direction.\nOrganisational\ngroup\nNo.\nobservations\nModel coef\ufb01cients R2 (%)\nSystem forecast Lag 1 Actual Lag 2 Actual Lag 1 Error Lag 2 Error\nA\u2013C Positive\nadjust\n2868 \u22120.120 (8.4) 0.028 (2.0) n.s. 0.135 (8.5) 0.072 (4.7) 20\nNegative\nadjust\n2427 \u22120.106 (10.2) 0.107 (7.6) 0.029 (2.2) n.s. n.s. 7\nD1 & D2 Positive\nadjust\n2566 n.s. n.s. \u22120.108 (4.5) 0.585 (31.7) n.s. 76\nNegative\nadjust\n2101 n.s. n.s. 0.104 (5.9) 0.369 (19.4) 0.046 (3.6) 41\nt statistics are in parentheses: n.s. implies that the coef\ufb01cient is not signi\ufb01cant at the 5% level.\nFrom the analysis so far presented, we have\nfound that \ufb01nal forecast errors (and the judgmental\nadjustments) are biased, inef\ufb01cient, and affected by\nthe direction and size of the adjustments. This leads\nto the core question of whether these features can be\nexploited to achieve more accurate forecasts.\n5. Reaping the bene\ufb01ts of expert adjustment\nIn this section we evaluate four possible methods\nfor improving the accuracy of the adjustments, based\non the biases and inef\ufb01ciencies we have identi\ufb01ed.\nThe \ufb01rst two methods use a statistical procedure\nto correct the adjustments. The others require\ntraining for forecasters, better market information, or\nrestrictiveness within the forecasting software.\n5.1. The Blattberg\u2013Hoch approach\nThe \u201850% model, 50% manager\u2019 heuristic proposed\nby\nBlattberg and Hoch (1990) involves taking the\nmean of independent management judgmental and\nstatistical forecasts. Blattberg and Hoch found that\nthis simple strategy was successful because of the\ncomplementary strengths and weaknesses of statistical\nmodels and human judges. For example, models can\nconsistently attach optimal weights to large volumes\nof data, while managers can recognise and interpret\nabnormal situations. In our case, application of the\nheuristic will involve taking a mean of the system\nand \ufb01nal forecasts. However, because the managers\nsaw the system forecasts (indeed, they typically over-\nweight them) before making their adjustments, the\nsystem and \ufb01nal forecasts are not independent. This\nmeans that the heuristic will lead to a forecast which\nis equal to:\n0.5 (System forecast) + 0.5 (System forecast\n+ Adjustment) = System forecast + 0.5 Adjustment.\nThus, the method will simply act as a damper on\nthe adjustments, restricting them to 50% of the change\nindicated by the managers. When there is a propensity\nto over-adjust forecasts (e.g. as a result of an optimism\nbias), this damping might improve accuracy . However,\nif the forecasters are anchoring on the statistical\nforecast and conforming to the anchor and adjustment\nheuristic (\nTversky & Kahneman, 1974 ), they will be\nunder-adjusting from the statistical forecast, and the\ndamping is likely to be too severe.\n5.2. Error bootstrap rules\nBootstrap rules, which model the relationship\nbetween the judgmental forecast and the available\ncue variables, have outperformed unadjusted raw\njudgments in many studies (\nDawes, Faust, &\nMeehl, 1989 ). This is because these models average\nout the inconsistencies in the raw judgments.\nHowever, the evidence is primarily based on cross-\nsectional forecasting problems where the cues are\nnot autocorrelated, as in time series data, and,\neven in these situations, bootstrapping has not\nalways outperformed raw judgments.",
        "8f6fbc4b-8c57-4633-bb84-4f0ebd046956": "However,\nif the forecasters are anchoring on the statistical\nforecast and conforming to the anchor and adjustment\nheuristic (\nTversky & Kahneman, 1974 ), they will be\nunder-adjusting from the statistical forecast, and the\ndamping is likely to be too severe.\n5.2. Error bootstrap rules\nBootstrap rules, which model the relationship\nbetween the judgmental forecast and the available\ncue variables, have outperformed unadjusted raw\njudgments in many studies (\nDawes, Faust, &\nMeehl, 1989 ). This is because these models average\nout the inconsistencies in the raw judgments.\nHowever, the evidence is primarily based on cross-\nsectional forecasting problems where the cues are\nnot autocorrelated, as in time series data, and,\neven in these situations, bootstrapping has not\nalways outperformed raw judgments. For example,\nAstebro and Elhedhli (2006) found that their linear\nadditive bootstrap model performed worse than\nexperts\u2019 forecasts of whether R&D projects would\nsubsequently be commercialised because it was not\nable to represent their non-compensatory use of cues.\nPlease cite this article in press as: Fildes, R., et al. Effective forecasting and judgmental adjustments: an empirical evaluation and strategies\nfor improvement in supply-chain planning. International Journal of Forecasting (2009), doi:10.1016/j.ijforecast.2008.11.010",
        "b21cc981-3972-46f1-a86b-31f8ccc52300": "ARTICLE  IN  PRESS\nR. Fildes et al. / International Journal of F orecasting ( ) \u2013 13\nThe time series evidence is much more limited\n(\nLawrence & O\u2019Connor, 1996 ; Lawrence et al. , 2000).\nThis may be partly because the serial correlation in the\ncues implies that there is a high degree of redundancy\nin the available information, a situation which is likely\nto enhance the quality of forecasts based on raw\njudgments relative to those of the bootstrap model.\nAn alternative way in which the cue information\ncan be used to improve on judgment in forecasting\nwas demonstrated by\nFildes (1991). He showed that\nmodelling the relationship between forecast errors and\ncues enabled forecasts to be corrected for the mis-\nweighting of both a causal driver (GDP) and past\nactuals. This resulted in improved accuracy in out-\nof-sample data. Given that the \ufb01nal forecasts in our\ncompanies are inef\ufb01cient and biased, this approach\n(which we will refer to as \u2018error bootstrapping\u2019) seems\nmuch more likely to result in improvements than\nconventional bootstrapping.\nW e have developed various models in order to\nexamine whether the observed inef\ufb01ciencies can be\nused to improve accuracy . Although lags of two\nperiods were shown to be signi\ufb01cant in the earlier\nef\ufb01ciency analysis shown in\nT able 7 , the longer lags\ntypically have a low impact on the models\u2019 standard\nerrors, once the most recent data is forced into\nthe model, and the additional complexity of such\nrules seemed likely to limit their operational impact.\nThe model estimated here combines the various raw\ninformation sources using only one period lags \u2013 the\nmost recent data available to the forecasters.\nYi j , t = \u03bb 1 j SFCi j , t \u22121 ( 1) + \u03bb 2 j Ad j i j , t \u22121\n+ \u03b2 1 j Yi j , t \u22121 + \u03b3 1 j ei j , t \u22121 + \u03bd i j , t , (M3)\nor alternatively\nei j , t = ( 1 + \u03bb 1 j ) SFCi j , t \u22121 ( 1) + ( 1 + \u03bb 2 j ) Ad j i j , t \u22121\n+ \u03b2 1 j Yi j , t \u22121 + \u03b3 1 j ei j , t \u22121 + \u03bd i j , t ,\nwhere SFCi j , t \u22121 (1) is the one-period ahead system\nforecast for the i th product, and the j th company ,\nmade in period t \u2212 1; Ad j t \u22121 is the size of the\njudgmental adjustment made by the forecaster; and\net \u22121 is the last period\u2019s error. Once\n( M3) is estimated\n(for each company), it can be used to produce forecasts\nof demand.\nT o ensure a rigorous evaluation of the proposed\nmodels, the database was split into an estimation\nset of approximately 80% of the total data set for\neach company , and a test set of the remainder. For\nexample, for Company A, the last 7 months constituted\nthe test data. This design of a hold-out sample is\nmore demanding for the model than the alternative\nof selecting 20% of the data at random as the hold-\nout sample. Model performance has been evaluated\nagain using a trimmed mean (trimmed by removing\n2% of the extreme \ufb01nal forecast errors). The test\nof equality of the model coef\ufb01cients for positive\nand negative information was highly signi\ufb01cant, and\ntherefore the two classes of data have again been\nmodelled separately .\nV arious different models were estimated, based on\nthe size of the adjustments made, since preliminary\nanalysis had shown that the size of the adjustment af-\nfected the model coef\ufb01cients (e.g., set Adjustment = 0\nfor small values). However, \ufb01tting the default model to\nthe entire data set (apart from outliers) proved the most\nsuccessful in terms of improved accuracy .",
        "fb2304a6-e8ef-4008-8fee-3641ddada0c0": "This design of a hold-out sample is\nmore demanding for the model than the alternative\nof selecting 20% of the data at random as the hold-\nout sample. Model performance has been evaluated\nagain using a trimmed mean (trimmed by removing\n2% of the extreme \ufb01nal forecast errors). The test\nof equality of the model coef\ufb01cients for positive\nand negative information was highly signi\ufb01cant, and\ntherefore the two classes of data have again been\nmodelled separately .\nV arious different models were estimated, based on\nthe size of the adjustments made, since preliminary\nanalysis had shown that the size of the adjustment af-\nfected the model coef\ufb01cients (e.g., set Adjustment = 0\nfor small values). However, \ufb01tting the default model to\nthe entire data set (apart from outliers) proved the most\nsuccessful in terms of improved accuracy . W e have\nanalysed two models: (1) the full model, incorporat-\ning the system forecasts, size of adjustment, past val-\nues of the errors and recent observations as cues; and\n(2) the optimal adjustment model, which uses only the\nsystem forecasts and size of adjustment. Error boot-\nstrap models are shown in the\nAppendix for the two\norganisational groups. Full details of all of the indi-\nvidual company models are available from the corre-\nsponding author. They show that the companies differ,\noften quite substantially , in their ability to use informa-\ntion in their environment to achieve optimally accurate\nforecasts. However, a number of traits were common\nto all companies; in particular, for the A\u2013C group of\ncompanies the past actuals and errors (whilst signi\ufb01-\ncant) were unimportant, in contrast to the retailer (D1\n& D2). Second, negative forecast adjustments were al-\nmost optimal in companies A\u2013C.\n5.3. Avoiding small adjustments\nThe results displayed in\nFig. 2 show that small\nadjustments were often ineffective in reducing forecast\nerror. W e argue that this was because they tended to\nbe made when information was unreliable and/or the\nanticipated effects of special events were small, or\nbecause forecasters were either seeing false patterns\nin noise or were tweaking the forecasts to justify\ntheir role. This suggests that a strategy which stops\nPlease cite this article in press as: Fildes, R., et al. Effective forecasting and judgmental adjustments: an empirical evaluation and strategies\nfor improvement in supply-chain planning. International Journal of Forecasting (2009), doi:10.1016/j.ijforecast.2008.11.010",
        "c2dbe4a9-46af-4d14-98d7-cf4c1d196c7d": "ARTICLE  IN  PRESS\n14 R. Fildes et al. / International Journal of F orecasting ( ) \u2013\nforecasters from making these smaller adjustments\nwould marginally enhance accuracy , but also, more\nimportantly , it would free up the time of those involved\nin the forecasting process. There are a number of\nways in which such a strategy could be implemented,\nincluding the training of forecasters and the use\nof software that prohibits adjustments below a pre-\nset percentage. W e examined the potential accuracy\ngains that could be achieved through a strategy of\navoiding adjustments smaller than 20% (some 42%\nof all adjustments) to see whether it would be worth\nimplementing.\n5.4. Avoiding wrong-sided adjustments\nOur earlier analysis showed that wrong-sided\nadjustments are particularly damaging to forecast\naccuracy . W e examined the potential gains that could\nbe achieved if improvements in market intelligence\ncould be used to eliminate half of these wrong sided\nadjustments. W e did this by randomly allocating each\nwrong sided adjustment to either a no change group\nor a change group. Each member of the no change\ngroup had its adjustment set to zero, so that its forecast\nbecame the system forecast.\n5.5. A comparison of the methods\nW e compared the accuracy of methods on both the\nin-sample and hold-out data using both the MAPE and\nMdAPE. W e also ranked the results for each company\n(1 being most accurate), and summed the ranks. Of\nthe two error bootstraps, the full bootstrap was the\nmore accurate in-sample, so we present these results.\nT able 8 summarises the hold-out sample results for\nthe A\u2013C and D1 & D2 companies. The in-sample\nresults are similar, and demonstrate the robustness of\nthe methods.\nIt can be seen from the hold-out results that there\nare clear differences in the accuracy of the methods\ndepending on whether the adjustment is positive\nor negative. When positive adjustments are made,\nthe strategy of preventing forecasters from making\nsmaller adjustments is ineffective. Because over-large\npositive adjustments are the main problem, preventing\nsmall positive adjustments offers little improvement.\nHowever, the other methods are effective when\nadjustments are positive. Rather than removing the\nsmall adjustments, the Blattberg\u2013Hoch method, as\napplied here, acts by damping all adjustments by 50%.\nThis method therefore improves accuracy by reducing\nthe damaging impact of the large adjustments. Wrong-\nsided adjustments are also frequently associated\nwith positive adjustments (especially for the D1&D2\ngroup\u2014see\nT able 5a ). Not surprisingly , developing\nmeasures to prevent half of these wrong-sided\nadjustments also leads to substantial improvements\nover the \ufb01nal forecasts. However, the optimal\nadjustment model performs best, re\ufb02ecting the serious\nerrors associated with forecasters\u2019 estimates of the\nsizes of required adjustments when the direction of\nadjustment is positive. For the D1&D2 group, the\nimprovements it yields over the \ufb01nal forecasts are\nsubstantial.\nWhen information is negative, a contrasting set\nof results is obtained. In this case, comparison\nbetween the MAPEs and MdAPEs suggests that\nthe judgmental adjustments are particularly effective\nbecause they are reducing many of the extreme errors\nin the system forecasts. However, there is a slight\ntendency to under-adjust: around 54% of negative\nadjustments are too small in both groups of companies\n(\nT able 5b). T able 8 shows that two of the improvement\nmethods are ineffective at best when adjustments are\nnegative. Indeed, if measures are taken to remove the\nsmaller adjustments, accuracy will actually be reduced\n(especially for the D1 & D2 group). This is because\nthe measure would eliminate those small adjustments,\nwhich are nevertheless in the correct direction.\nSimilarly , by damping the size of the adjustments,\nthe Blattberg\u2013Hoch method only exacerbates the\ntendency to under-adjust. (The small improvement in\nthe MdAPE arises from just one of the companies.)\nPreventing half of the wrong-sided adjustments is\nbound, by de\ufb01nition, to lead to improved accuracy .",
        "7e016e18-0809-44d6-9b8e-be38a478ebb3": "However, there is a slight\ntendency to under-adjust: around 54% of negative\nadjustments are too small in both groups of companies\n(\nT able 5b). T able 8 shows that two of the improvement\nmethods are ineffective at best when adjustments are\nnegative. Indeed, if measures are taken to remove the\nsmaller adjustments, accuracy will actually be reduced\n(especially for the D1 & D2 group). This is because\nthe measure would eliminate those small adjustments,\nwhich are nevertheless in the correct direction.\nSimilarly , by damping the size of the adjustments,\nthe Blattberg\u2013Hoch method only exacerbates the\ntendency to under-adjust. (The small improvement in\nthe MdAPE arises from just one of the companies.)\nPreventing half of the wrong-sided adjustments is\nbound, by de\ufb01nition, to lead to improved accuracy .\nHowever, when information is negative there tend\nto be fewer wrong-sided adjustments (see\nT able 5b ),\nso the bene\ufb01ts derived from measures designed to\neliminate half of them are less than those obtained\nwith positive information. The optimal adjustment\nmodel does not yield improvements for companies\nA\u2013C, re\ufb02ecting the fact that forecasters in these\ncompanies made negative adjustments which were\nhighly accurate, but it does lead to improved accuracy\nfor companies D1 & D2, where there was a confusion\nbetween forecasts and inventory decisions.\nPlease cite this article in press as: Fildes, R., et al. Effective forecasting and judgmental adjustments: an empirical evaluation and strategies\nfor improvement in supply-chain planning. International Journal of Forecasting (2009), doi:10.1016/j.ijforecast.2008.11.010",
        "e4a8dee1-4d2a-4e01-8af3-78f6f25576b5": "ARTICLE  IN  PRESS\nR. Fildes et al. / International Journal of F orecasting ( ) \u2013 15\nT able 8\nHold-out sample accuracy of forecast \u2018improvement\u2019 methods.\nOrg. group Adjustment Accuracy\nmeasures\nSystem\nforecast\nFinal\nforecast\nBlattberg-\nHoch\nFull\nmodel\nA void smallest\nadjustments\nRemove 50% of wrong\nside adjustments\nA\u2013C Positive MAPE 32.3% 42.5% 38.1% 32.1% 41.4% 35.0%\n(n = 639) MdAPE 21.0% 21.7% 18.1% 19.9% 19.9% 18.0%\nSum of\nranks\n20 33 19 13 28 14\nNegative MAPE 52.6% 29.8% 49.3% 29.6% 32.7% 28.6%\n(n = 720) MdAPE 19.2% 18.3% 16.9% 18.4% 16.0% 17.0%\nSum of\nranks\n34 17 28 16 22 9\nD1 & D2 Positive MAPE 30.3% 47.7% 38.5% 35.5% 46.2% 40.0%\n(n = 627) MdAPE 26.1% 41.8% 31.1% 30.3% 40.2% 32.9%\nSum of\nranks\n8 24 14 9 20 14\nNegative MAPE 35.2% 23.6% 27.8% 18.9% 25.5% 21.7%\n(n = 612) MdAPE 28.0% 19.4% 23.2% 11.8% 20.9% 18.0%\nSum of\nranks\n18 17 14 5 19 11\nn = number of observations.\nIn summary , preventing smaller adjustments is\nnot likely to lead to improvements in accuracy ,\nwhatever the information direction. The other methods\nare effective when information is positive. When it\nis negative they lead to smaller improvements in\naccuracy or none at all. As a strategy , the optimal\nadjustment procedure works well overall; it generally\nleads to improved accuracy , sometimes substantial,\nand with no damaging effects for either positive or\nnegative adjustments for either group of companies.\n6. Improving forecasting performance in practice\nOur analysis has revealed major differences in the\nforecasting accuracies obtained by the companies. It\nalso showed the potential for improvements that could\nbe achieved by focusing on the more effective use of\nthe available information and the removal of consistent\nbiases. However, the methods of improvement that we\nexamined would face possible obstacles if an attempt\nwas made to implement them in many organisations.\nBecause the Blattberg\u2013Hoch and error bootstrap\napproaches are automatic correction procedures, their\nuse may lead to the demotivation of forecasters, with\nless effort being applied to the original judgments\n(\nBelton & Goodwin, 1996 ). Alternatively , the nature\nof the biases may change over time, possibly as a result\nof training, or the company forecasters may seek to\npre-empt the corrections by distorting their judgmental\ninputs into the process. Improvements in forecasting\nthrough training and better use of market intelligence,\nwhich are required by the other two improvement\nmethods we examined, are also not straightforward\nto achieve, and there are many barriers to adopting\nnew forecasting procedures (\nSchultz, 1984 ). In this\nsection we use our experience of extensive meetings\nwith the companies concerned and observation of\ntheir forecasting processes to provide explanations\nfor the results and to identify how improvements\nmight be achieved. W e held between 5 and 11\nmeetings with staff at each company . These included\nsemi-structured interviews with individual company\npersonnel, formal presentations by managers of\ntheir forecasting processes, forecast review meetings\n(involving at least \ufb01ve staff), and discussions where\nwe debriefed staff on the \ufb01ndings of our research.",
        "ab5f6c1a-cd01-4bb7-abf6-222c38ef34dd": "Improvements in forecasting\nthrough training and better use of market intelligence,\nwhich are required by the other two improvement\nmethods we examined, are also not straightforward\nto achieve, and there are many barriers to adopting\nnew forecasting procedures (\nSchultz, 1984 ). In this\nsection we use our experience of extensive meetings\nwith the companies concerned and observation of\ntheir forecasting processes to provide explanations\nfor the results and to identify how improvements\nmight be achieved. W e held between 5 and 11\nmeetings with staff at each company . These included\nsemi-structured interviews with individual company\npersonnel, formal presentations by managers of\ntheir forecasting processes, forecast review meetings\n(involving at least \ufb01ve staff), and discussions where\nwe debriefed staff on the \ufb01ndings of our research. The\nstaff we met were forecasters, logistics and supply\nchain managers, accountants, and sales and marketing\npersonnel. At least two observers (or interviewers)\nwere involved in the meetings, which were also\nusually tape-recorded.\n6.1. The forecasters\nFildes and Hastings (1994) and Moon et al.\n(2003) identi\ufb01ed motivation and training as potentially\nPlease cite this article in press as: Fildes, R., et al. Effective forecasting and judgmental adjustments: an empirical evaluation and strategies\nfor improvement in supply-chain planning. International Journal of Forecasting (2009), doi:10.1016/j.ijforecast.2008.11.010",
        "f971683e-11a9-451c-828b-522656dd11b5": "ARTICLE  IN  PRESS\n16 R. Fildes et al. / International Journal of F orecasting ( ) \u2013\nimportant in attaining accurate forecasts. Despite such\na conclusion being apparently obvious, in none of\nthe companies were the forecasters knowledgeable\nin the statistical aspects of forecasting such as error\nmeasurement or alternative forecasting methods. Nor\nwere they aware of the many biases associated with\njudgmental interventions in forecasting. All the senior\nforecasters were, however, immersed in process and\nmanagement issues relating to forecasting, with one\nacting as a regular presenter at professional forecasting\nevents. It therefore appears that training and the use\nof appropriately targeted incentives might lead to\nimprovements in accuracy .\n6.2. The forecasting support system (FSS)\nAll four company systems were professionally\ndeveloped, but they had in\ufb02exible interfaces and poor\n(or in one case non-existent) graphics. However, the\nformat of the interface can be important in improving\naccuracy (\nT ashman & Hoover, 2001 ). The system\nforecasts were produced using models far removed\nfrom best practice (see, for example,\nGardner &\nAnderson, 1997 ), nor had the chosen methods been\ntuned to produce the best possible results from the\nsoftware. Standard exponential smoothing models are\nnow known to require just such tuning in the choice\nof smoothing parameters, so this might explain the\ninadequacies (\nGardner, 2006 ). This was underlined in\nthe strong performance of the na \u00a8\u0131ve forecast compared\nto the system forecast for companies B and D. In\ncompany A, perceived inadequacies in the automatic\nsystem forecasts led to a complex model-\ufb01tting\nprocess (see\nGoodwin, Lee, Fildes, Nikolopoulos, &\nLawrence, 2007 ) and, typically , system forecasts that\nwere less accurate than an automatic alternative. An\nanalysis of screen displays shows that the forecasters\ndid not have clear guidance on the previous actuals\nand previous errors. Summary error measures were not\neasily available, and those that were provided were\nsubject to outlier and intermittent demand effects.\nThus, a reliable assessment of the gains or losses in\naccuracy resulting from judgmental adjustments could\nnot be made, and there was therefore little opportunity\nto learn from experience about the appropriateness\nof judgmental intervention in different circumstances.\nImproved statistical forecasting systems to provide\nbetter baseline forecasts, and accuracy monitoring\nsystems using well-designed error measures, are\ntherefore needed.\nAlthough three of the systems had a \u2018notes\u2019\nfacility , whereby the forecaster could explain the\nreasons for the adjustments they made, they had\nnone of the features that might make it easy to\nuse and effective (\nLee et al., 2007 ). Their use was\nspasmodic and incoherent, in that forecasters could\nnot explain the past adjustments they had made to us\nby referring to their \u2018notes\u2019 system. Clearly , requiring\nforecasters to record reasons for their adjustments\nin a standard format (e.g. by selecting a reason\nfrom a list) might serve to reduce the number of\nrelatively small, but damaging adjustments that may\nbe based on misinterpreting noise as signal, or re\ufb02ect\ngratuitous tweaking of the forecasts (\nGoodwin, 2000 ).\nA list of reasons would also allow forecasters to\nunderstand why and how market intelligence is so\noften misinterpreted. In addition, it would assist in\nthe decomposition of market intelligence into key\ndrivers, thereby lessening the likelihood of double\ncounting. Finally , experimental evidence suggests that\nthe incorporation of guidance systems such as those\nwhich allow the formal use of analogies (e.g. past\npromotions and their effects, see\nLee et al., 2007 ),\nwould improve the quality of judgments based on\nmarket intelligence.\n6.3. Communicating and compiling the market\nintelligence\nForecasters in companies B, C and D identi\ufb01ed\npromotions as the most important driver of their\njudgmental adjustments. Other important drivers\nincluded price changes (companies C & D), the\nweather (B & D), and inventories (A & D).",
        "8965224d-5e20-46d0-90c1-9b44598c18c0": "A list of reasons would also allow forecasters to\nunderstand why and how market intelligence is so\noften misinterpreted. In addition, it would assist in\nthe decomposition of market intelligence into key\ndrivers, thereby lessening the likelihood of double\ncounting. Finally , experimental evidence suggests that\nthe incorporation of guidance systems such as those\nwhich allow the formal use of analogies (e.g. past\npromotions and their effects, see\nLee et al., 2007 ),\nwould improve the quality of judgments based on\nmarket intelligence.\n6.3. Communicating and compiling the market\nintelligence\nForecasters in companies B, C and D identi\ufb01ed\npromotions as the most important driver of their\njudgmental adjustments. Other important drivers\nincluded price changes (companies C & D), the\nweather (B & D), and inventories (A & D). Where\nmarket intelligence is strong and the direction of its\neffect is clear, there are major potential improvements\nin accuracy , as seen in the greater accuracy achieved\nthough negative adjustments. However, the process\nby which such intelligence is gained, as\nMoon et al.\n(2003) point out in an examination of 16 case\nstudies, is often very \ufb02awed, primarily through the\nlack of coordination and communication between\nthe different organisational units involved in supply\nchain operations, sales and marketing. Here, while\nall companies apparently consult widely on important\ndrivers, the evidence that is collected is not compiled\nPlease cite this article in press as: Fildes, R., et al. Effective forecasting and judgmental adjustments: an empirical evaluation and strategies\nfor improvement in supply-chain planning. International Journal of Forecasting (2009), doi:10.1016/j.ijforecast.2008.11.010",
        "c4b926cc-2396-467f-b990-b5d16c9ad8d5": "ARTICLE  IN  PRESS\nR. Fildes et al. / International Journal of F orecasting ( ) \u2013 17\nthrough, for example, a database which could lead\nto learning by analogy from earlier exemplars. Each\nforthcoming event is, instead, treated as unique.\nIn addition, different sources of intelligence are\nnot identi\ufb01ed and quanti\ufb01ed separately using a\ndecomposition approach. This can lead to double\ncounting or omission (\nMacGregor, 2001 ). None of\nthe companies attempted to review the reasons for\nwrongly interpreting the direction of an adjustment,\nwhich, as we have seen, is a cause of major forecast\nerror.\n7. Limitations of the study and an agenda for\nfuture research\nDespite the large number of forecasts and\nadjustments that we examined, our study has some\nlimitations. First, the data was collected from just\nfour UK-based companies and were one step ahead\nforecasts. Although they covered a range of industries,\nall of these organisations were similar in the expertise\nof their forecasting staff, the forecasting processes\nthat they adopted, and the type of software that they\nused. In particular, given that the managers in our\norganisations had minimal training in forecasting,\nwe cannot be sure that our results will apply to\ncompanies where forecasts are produced by more\nhighly trained staff. Moreover, the manufacturers\nwere not involved in collaborative forecasting with\neither their suppliers or their customers; the retailer\u2019s\nforecasts were however shared with some of their\nsuppliers. A large body of recent research has\nattempted to assess the bene\ufb01ts of information sharing\nin supply chains (see for example,\nHelms, Ettkin,\n& Chapman, 2000 , for an introduction and Smaros,\n2007, for a description of some of the dif\ufb01culties of\ncollaboration); however, much of this research has\ninvolved simulations or mathematical modelling of\nsimpli\ufb01ed theoretical supply chains (e.g.\nA viv , 2001).\nClearly , future research needs to assess the bene\ufb01ts\nof information sharing in real supply chains. Despite\nall of these reservations, the results of our recent\nsurveys of forecasters (\nFildes & Goodwin, 2007 ), as\nwell as the study by Moon et al. (2003), and our\nown observations in other companies, suggest that the\nforecasting practices we studied are typical of those\ncurrently found in a large number of supply-chain\nbased companies.\nSecond, there is arguably no such thing as an\nobjective statistical forecast. All forecasting involves\nsome judgment, at least in the choice of the statistical\nmethod used and the length of the data history\nto which it is \ufb01tted. In some of our companies\nthe forecasters had often already intervened in the\nstatistical forecasting process to change the length\nof the data history or the smoothing parameters. In\naddition, the process of cleaning past data to remove\nthe effect of unusual events was judgmental. Thus\nthe judgmental adjustments were applied to statistical\nforecasts that already contained an element of the\nforecasters\u2019 judgment. In the future it would be\ninteresting to compare the merits of adjustments to\nstatistical forecasts that have, as far as possible, been\ngenerated automatically to those applied to forecasts\nthat have already been subjected to signi\ufb01cant\nintervention (\n\u00a8Onkal, G \u00a8on \u00a8ul, & Lawrence, 2007 ).\nThird, our \ufb01nding that forecast adjustment re\ufb02ects\na general over-optimism needs further investigation\nto determine its underlying causes. Although the\nforecasters in the non-retail companies assured us that\ntheir objective was to produce as accurate forecasts\nas possible, our results do not allow us to assess the\nextent to which this bias resulted from overt or latent\nincentives to over-forecast, or whether it is a cognitive\nbias associated with the forecasting process.\nOur results point to a number of additional\nkey areas where future research efforts need to be\ndirected. The statistical models we have examined\nhave been based on cross-sectional analysis, pooling\nmodels across companies. With a larger database,\ntime series cross-sectional models could be examined\nwith a view to analysing the performance of products\nindividually and producing even more effective\ncombined forecasts.",
        "dfad6a64-17e2-4ee2-a3cc-cc5db72f686f": "Third, our \ufb01nding that forecast adjustment re\ufb02ects\na general over-optimism needs further investigation\nto determine its underlying causes. Although the\nforecasters in the non-retail companies assured us that\ntheir objective was to produce as accurate forecasts\nas possible, our results do not allow us to assess the\nextent to which this bias resulted from overt or latent\nincentives to over-forecast, or whether it is a cognitive\nbias associated with the forecasting process.\nOur results point to a number of additional\nkey areas where future research efforts need to be\ndirected. The statistical models we have examined\nhave been based on cross-sectional analysis, pooling\nmodels across companies. With a larger database,\ntime series cross-sectional models could be examined\nwith a view to analysing the performance of products\nindividually and producing even more effective\ncombined forecasts. Such an analysis would deliver\nanswers to questions such as, do the adjustments made\nto \u2018important\u2019 products prove more effective? Is there\nlearning taking place from adjustment to adjustment?\nThere is clearly a need for much more research\ninto the design of forecasting software. Software\ndevelopments over recent decades have focussed on\nimproving data handling and input\u2013output processes.\nManagement judgment, which, as we have seen, is\na crucial component of demand forecasts in supply\nchain companies, remains relatively unsupported\n(\nFildes, Goodwin, & Lawrence, 2006 ). In this regard,\nwe think that there is scope for developing and\nPlease cite this article in press as: Fildes, R., et al. Effective forecasting and judgmental adjustments: an empirical evaluation and strategies\nfor improvement in supply-chain planning. International Journal of Forecasting (2009), doi:10.1016/j.ijforecast.2008.11.010",
        "72b450d1-b233-4ebd-badd-18121cffefe1": "ARTICLE  IN  PRESS\n18 R. Fildes et al. / International Journal of F orecasting ( ) \u2013\ntesting software facilities that allow advice and\ninformation obtained from multiple sources to be used\nand combined in a structured way . Our early work\non the use of a database of analogies to support\nestimates of the effects of special events, such as\npast promotions (\nLee et al., 2007 ), could be extended\nto include a database of pro\ufb01les of the week-to-\nweek effects of sales promotions over the course\nof the campaign. In addition, the evidence from the\nretailer (company D) suggests the need for a de-\ncoupled system which clearly distinguishes between\nforecasting and the associated supply chain decisions.\nA system which allows the two to be compounded\nmeans that bene\ufb01cial judgmental interventions are\ndif\ufb01cult to apply because there is no clear view of\neither forecast error or optimal inventories. Moreover,\ngiven the widespread use of forecasting review\nmeetings, there is also scope for the development\nof group forecasting support systems which allow\nmanagers to feed independent estimates of required\nadjustments into the system. Such improvements in\nsystem design might also help to mitigate the pressures\ntowards bias, both personal and organisational, that\nexists in many companies.\nHowever, none of these developments will be\neffective if such facilities in systems are not demanded\nand then used by forecasters, perhaps because of\npolitical or cultural factors, or because of a lack of\ntraining. This indicates the need for organisation-\nbased studies that use interpretive research methods\nto establish, at a deep level, the beliefs and values of\nmanagers engaged in forecasting. Such research would\nneed to explore both the psychological processes\nthat individual managers employ and the effects of\ninteractions between managers within organisational\ncontexts. The results of these studies should encourage\nthe successful implementation of effective company\nforecasting processes.\n8. Conclusions\nJudgmental adjustment of statistical demand fore-\ncasts for SKUs is very common, with up to 80% of\nforecasts adjusted in some companies. The accuracy of\nsuch forecasts and their associated judgmental adjust-\nments is crucial to supply chain operations and plan-\nning, and has a direct impact on pro\ufb01t and service\nlevels. Moreover, without reliable forecasts, collabo-\nration between retailers and manufacturers is likely to\nbe limited. Hitherto the effectiveness of these judg-\nmental adjustments has been moot, with very lim-\nited empirical evidence available. In this paper we\nhave shown conclusively that the value of these adjust-\nments depends on the company context, but where the\nforecasters\u2019 principal motivation is towards improved\naccuracy , they can add substantially to forecast accu-\nracy (hypothesis H 1 ).\nHowever, the company forecasts we observed\nproved to be biased and inef\ufb01cient (hypotheses H 4 and\nH5 ). Positive adjustments were far less effective than\nnegative, and forecasts that overestimated demand\nwere prevalent. The forecasters tended to over-weight\nthe statistical system\u2019s forecast, which for some of\nthe companies was itself de\ufb01cient in comparison to a\nna\u00a8\u0131ve forecast. W e therefore developed various models\nfor capitalising on the biases and inef\ufb01ciencies,\nshowing that the most appropriate model depends\non the circumstances, and in particular the nature\nof the adjustment, positive or negative. For example,\nthe effectiveness of the Blattberg-Hoch heuristic of\n50% model + 50% man, as we applied it, is itself\nlimited to positive adjustments. Based on over 12,000\njudgmentally adjusted forecasts and 1536 SKUs, we\ncan therefore conclude that, at least for the companies\nanalysed here, they could improve their forecasts\nsubstantially by more effective codi\ufb01cation and\nincorporation of available information, such as market\nintelligence \u2014 the basis of most adjustments. In\nparticular, companies should put into place processes\nto avoid \u2018optimism\u2019 bias and the confusion of forecasts\nand decisions, either as part of the FSS or in the\nmotivation, training and monitoring aspects of the\norganisational forecasting process.",
        "ae050f1c-447b-4856-b4f6-8a39a46d09c2": "For example,\nthe effectiveness of the Blattberg-Hoch heuristic of\n50% model + 50% man, as we applied it, is itself\nlimited to positive adjustments. Based on over 12,000\njudgmentally adjusted forecasts and 1536 SKUs, we\ncan therefore conclude that, at least for the companies\nanalysed here, they could improve their forecasts\nsubstantially by more effective codi\ufb01cation and\nincorporation of available information, such as market\nintelligence \u2014 the basis of most adjustments. In\nparticular, companies should put into place processes\nto avoid \u2018optimism\u2019 bias and the confusion of forecasts\nand decisions, either as part of the FSS or in the\nmotivation, training and monitoring aspects of the\norganisational forecasting process. A voiding small\nadjustments (hypothesis H 2b ) would free time to\nfocus on what is undoubtedly the most important\nissue, identifying the direction of the effect of market\nintelligence. In addition, the statistical forecasting\nsystems employed by many of the companies could\nbe improved to deliver a better baseline from which to\nmake the judgmental adjustments.\nForecast adjustment is the only practical way for\nmost organisations to improve their incorporation of\nkey drivers into their disaggregated sales forecasts.\nWhile the evidence we present shows the bene\ufb01ts of\nadjustment, there remain plenty of opportunities for\nPlease cite this article in press as: Fildes, R., et al. Effective forecasting and judgmental adjustments: an empirical evaluation and strategies\nfor improvement in supply-chain planning. International Journal of Forecasting (2009), doi:10.1016/j.ijforecast.2008.11.010",
        "bda9d01d-7727-45b5-92c9-8344f6c91810": "ARTICLE  IN  PRESS\nR. Fildes et al. / International Journal of F orecasting ( ) \u2013 19\nT able A.1\nError bootstrap model coef\ufb01cients and R2 values by organisational group.\nOrg. group Adjustment direction System forecast Past actual Past error Adjustment R2\nA\u2013C M3 0.842 (62.5) 0.169 (13.5) n.s. 0.424 (22.6) 0.97\nPositive n = 1367 Optimal Adjust 0.987 (211) * * 0.450 (25.2) 0.97\nBlattberg\u2013Hoch 1 * * 0.5 *\nM3 0.827 (59.3) 0.181 (13.2) n.s. 0.804 (32.0) 0.97\nNegative n = 1087 Optimal Adjust 0.995 (197) * * 0.989 (43.1) 0.96\nBlattberg\u2013Hoch 1 * * 0.5 *\nD1-D2 M3 0.392 (17.2) 0.590 (24.1) n.s. 0.203 (8.90) 0.97\nPositive n = 2063 Optimal Adjust 0.864 (122) * * 0.272 (17.9) 0.94\nBlattberg\u2013Hoch 1 * * 0.5 *\nM3 0.465 (17.4) 0.573 (18.7) 0.089 (4.14) 0.371 (13.8) 0.97\nNegative n = 1797 Optimal Adjust 0.913 (104) * * 0.622 (21.4) 0.94\nBlattberg\u2013Hoch 1 * * 0.5 *\nt -statistics are in parentheses. Key: M3: The full model using all information; Optimal Adjust: The optimal adjustment model using just the\nadjustment and system forecast; Blattberg-Hoch: The 50-50 model weighting the system forecast and the adjusted \ufb01nal forecast equally .\nboth companies and researchers to understand how\nsuch factors are best included to overcome the biases\nand inef\ufb01ciencies we have identi\ufb01ed. The result should\nbe major improvements in accuracy .\nAcknowledgements\nThis research was supported by Engineering and\nPhysical Sciences Research Council (EPSRC) grants\nGR/60198/ 01 and GR/60181 / 01. Alastair Robertson\ncontributed substantially to the statistical modelling.\nAppendix. Error bootstrap models to make more\nef\ufb01cient use of the judgmental adjustments\nT able A.1 shows the estimated cue models based\non the ( M3) equation, which uses the system forecast,\npast cues and the judgmental adjustment to estimate\nthe actual demand. If the \ufb01nal forecast were the best\nachievable, the coef\ufb01cients of the system forecast\n(SFC) and the judgmental adjustment ( Adj) would\nboth be 1, with zero weight assigned to past actuals\nand past errors. The second model only used the\njudgmental adjustment and the system forecast. The\ndata have been grouped into the manufacturing\ncompanies (forecasting monthly) and the retailer\n(forecasting weekly). For the monthly data the drivers\nof the past actual and past error have limited or\nno predictive power. However, the forecasters over-\nrespond to market intelligence and other information,\nparticularly in the case where information is positive,\nand therefore damping the adjustment improves\naccuracy . For negative information the \ufb01nal forecasts\nare close to optimal (giving a weight close to 1 to\nthe adjustment). For the retailer, neither the system\nforecast nor the adjustment is close to optimal, so\nthe scope for improvement is large. The implied\ncoef\ufb01cients from the Blattberg-Hoch model are shown\nfor comparison.\nReferences\nArmstrong, J. S., & Collopy, F. (1992). Error measures\nfor generalizing about forecasting methods \u2013 empirical\ncomparisons. International Journal of F orecasting , 8, 69\u201380.\nArmstrong, J. S., & Fildes, R. (1995).",
        "d595edc7-f54a-4d61-b0b9-9ffd95909a30": "However, the forecasters over-\nrespond to market intelligence and other information,\nparticularly in the case where information is positive,\nand therefore damping the adjustment improves\naccuracy . For negative information the \ufb01nal forecasts\nare close to optimal (giving a weight close to 1 to\nthe adjustment). For the retailer, neither the system\nforecast nor the adjustment is close to optimal, so\nthe scope for improvement is large. The implied\ncoef\ufb01cients from the Blattberg-Hoch model are shown\nfor comparison.\nReferences\nArmstrong, J. S., & Collopy, F. (1992). Error measures\nfor generalizing about forecasting methods \u2013 empirical\ncomparisons. International Journal of F orecasting , 8, 69\u201380.\nArmstrong, J. S., & Fildes, R. (1995). On the selection of error\nmeasures for comparisons among forecasting methods. Journal\nof F orecasting, 14, 67\u201371.\nAstebro, T., & Elhedhli, S. (2006). The effectiveness of simple\ndecision heuristics: Forecasting commercial success for early-\nstage ventures. Management Science , 52, 395\u2013409.\nA viv, Y. (2001). The effect of collaborative forecasting on supply\nchain performance. Management Science , 47, 1326\u20131343.\nBelton, V., & Goodwin, P. (1996). Remarks on the application\nof the analytic hierarchy process to judgmental forecasting.\nInternational Journal of F orecasting , 12, 155\u2013161.\nBlattberg, R. C., & Hoch, S. J. (1990). Database models and\nmanagerial intuition \u2013 50-percent model + 50-percent manager.\nManagement Science , 36, 887\u2013899.\nBonaccio, S., & Dalal, R. S. (2006). Advice taking and decision\nmaking: An integrative literature review , and implications for the\norganizational sciences. Organizational Behavior and Human\nDecision Processes , 101, 127\u2013151.\nPlease cite this article in press as: Fildes, R., et al. Effective forecasting and judgmental adjustments: an empirical evaluation and strategies\nfor improvement in supply-chain planning. International Journal of Forecasting (2009), doi:10.1016/j.ijforecast.2008.11.010",
        "50aa5daf-fc0d-4b9b-9b5f-9acf5b3ebca2": "ARTICLE  IN  PRESS\n20 R. Fildes et al. / International Journal of F orecasting ( ) \u2013\nBuehler, R., & Grif\ufb01n, D. (2003). Planning, personality , and\nprediction: The role of future focus in optimistic time\npredictions. Organizational Behavior and Human Decision\nProcesses, 92, 80\u201390.\nClements, M. P., & Hendry, D. F. (1995). On the selection of error\nmeasures for comparisons among forecasting methods \u2014 reply.\nJournal of F orecasting , 14, 73\u201375.\nDawes, R. M., Faust, D., & Meehl, P. E. (1989). Clinical versus\nactuarial judgment. Science, 243, 1668\u20131673.\nDeschamps, E. (2004). The impact of institutional change on\nforecast accuracy: A case study of budget forecasting in\nW ashington state. International Journal of F orecasting , 20,\n647\u2013657.\nDiamantopoulos, A., & Mathews, B. P. (1989). Factors affecting\nthe nature and effectiveness of subjective revision in sales\nforecasting: An empirical study. Managerial and Decision\nEconomics, 10, 51\u201359.\nDonihue, M. R. (1993). Evaluating the role judgment plays in\nforecast accuracy. Journal of F orecasting , 12, 81\u201392.\nFildes, R. (1991). Ef\ufb01cient use of information in the formation\nof subjective industry forecasts. Journal of F orecasting , 10,\n597\u2013617.\nFildes, R., & Beard, C. (1992). Forecasting systems for production\nand inventory control. International Journal of Production and\nOperations Management , 12, 4\u201327.\nFildes, R., Bretschneider, S., Collopy, F, Lawrence, M., Stewart,\nD., Winklhofer, H., et al. (2003). Researching sales forecasting\npractice \u2013 commentaries and authors\u2019 response on \u201cConducting\na sales forecasting audit\u201d by M.A. Moon, J.T . Mentzer & C.D.\nSmith. International Journal of F orecasting , 19, 27\u201342.\nFildes, R., & Goodwin, P. (2007). Against your better judgement?\nHow organizations can improve their use of management\njudgement in forecasting. Interfaces, 37, 570\u2013576.\nFildes, R., Goodwin, P., & Lawrence, M. (2006). The design\nfeatures of forecasting support systems and their effectiveness.\nDecision Support Systems , 42, 351\u2013361.\nFildes, R., & Hastings, R. (1994). The organization and\nimprovement of market forecasting. Journal of the Operational\nResearch Society , 45, 1\u201316.\nFlybjerg, B., Bruzelius, N., & Rothengatter, W. (2003). Megapro-\njects and risk: An anatomy of ambition . Cambridge, UK: Cam-\nbridge University Press.\nGalbraith, C. S., & Merrill, G. B. (1996). The politics of forecasting:\nManaging the truth. California Management Review , 38, 29\u201343.\nGardner, E. S. (2006). Exponential smoothing: The state of the art \u2013\nPart II. International Journal of F orecasting , 22, 637\u2013666.\nGardner, E. S., & Anderson, E. A. (1997). Focus forecasting recon-\nsidered. International Journal of F orecasting , 13, 501\u2013508.\nGoodwin, P. (2000). Improving the voluntary integration of\nstatistical forecasts and judgment. International Journal of\nF orecasting, 16, 85\u201399.\nGoodwin, P., & Fildes, R. (1999). Judgmental forecasts of time\nseries affected by special events: Does providing a statistical\nforecast improve accuracy?",
        "51f6fd46-b0f1-4105-a25d-c9426cd0bf07": "The politics of forecasting:\nManaging the truth. California Management Review , 38, 29\u201343.\nGardner, E. S. (2006). Exponential smoothing: The state of the art \u2013\nPart II. International Journal of F orecasting , 22, 637\u2013666.\nGardner, E. S., & Anderson, E. A. (1997). Focus forecasting recon-\nsidered. International Journal of F orecasting , 13, 501\u2013508.\nGoodwin, P. (2000). Improving the voluntary integration of\nstatistical forecasts and judgment. International Journal of\nF orecasting, 16, 85\u201399.\nGoodwin, P., & Fildes, R. (1999). Judgmental forecasts of time\nseries affected by special events: Does providing a statistical\nforecast improve accuracy? Journal of Behavioral Decision\nMaking, 12, 37\u201353.\nGoodwin, P., & Lawton, R. (1999). On the asymmetry of the\nsymmetric MAPE. International Journal of F orecasting , 15,\n405\u2013408.\nGoodwin, P ., Lee, W . Y ., Fildes, R., Nikolopoulos, K., & Lawrence,\nM. (2007). Understanding the use of forecasting systems: An\ninterpretive study in a supply-chain company . Bath University\nManagement School working paper.\nHarvey, N. (1995). Why are judgments less consistent in less\npredictable task situations? Organizational Behavior and\nHuman Decision Processes , 63, 247\u2013263.\nHelbok, G., & W alker, M. (2004). On the nature and rationality\nof analysts\u2019 forecasts under earnings conservatism. The British\nAccounting Review , 36, 45\u201377.\nHelms, M. M., Ettkin, L. P., & Chapman, S. (2000). Supply\nchain forecasting \u2014 collaborative forecasting supports supply\nchain management. Business Process Management Journal , 6,\n392\u2013407.\nKottemann, J. E., Davis, F. D., & Remus, W. E. (1994). Computer-\nassisted decision making: Performance, beliefs, and the illusion\nof control. Organizational Behavior and Human Decision\nProcesses, 57, 26\u201337.\nLanger, E. J. (1975). Illusion of control. Journal of P ersonality and\nSocial Psychology , 32, 311\u2013328.\nLawrence, M., & O\u2019Connor, M. J. (1996). Judgement or models:\nThe importance of task differences. Omega: International\nJournal of Management Science , 24, 245\u2013254.\nLawrence, M., O\u2019Connor, M., & Edmundson, B. (2000). A \ufb01eld\nstudy of sales forecasting accuracy and processes. European\nJournal of Operational Research , 122, 151\u2013160.\nLawrence, M., Goodwin, P., O\u2019Connor, M., & Onkal, D. (2006).\nJudgmental forecasting: A review of progress over the last 25\nyears. International Journal of F orecasting , 22, 493\u2013518.\nLee, W. Y., Goodwin, P., Fildes, R., Nikolopoulos, K., & Lawrence,\nM. (2007). Providing support for the use of analogies in demand\nforecasting tasks. International Journal of F orecasting , 23,\n377\u2013390.\nLim, J. S., & O\u2019Connor, M. (1995). Judgmental adjustment of initial\nforecasts \u2014 its effectiveness and biases. Journal of Behavioral\nDecision Making , 8, 149\u2013168.\nLim, J. S., & O\u2019Connor, M. (1996). Judgmental forecasting with\ntime series and causal information. International Journal of\nF orecasting, 12, 139\u2013153.\nMacGregor, D. (2001). Decomposition for judgemental forecasting\nand estimation. In J. S. Armstrong (Ed.), Principles of\nforecasting (pp. 107\u2013123).",
        "85c8b298-ecb6-4a17-8e36-13bce7cf2c1e": "(2007). Providing support for the use of analogies in demand\nforecasting tasks. International Journal of F orecasting , 23,\n377\u2013390.\nLim, J. S., & O\u2019Connor, M. (1995). Judgmental adjustment of initial\nforecasts \u2014 its effectiveness and biases. Journal of Behavioral\nDecision Making , 8, 149\u2013168.\nLim, J. S., & O\u2019Connor, M. (1996). Judgmental forecasting with\ntime series and causal information. International Journal of\nF orecasting, 12, 139\u2013153.\nMacGregor, D. (2001). Decomposition for judgemental forecasting\nand estimation. In J. S. Armstrong (Ed.), Principles of\nforecasting (pp. 107\u2013123). Norwell, MA: Kluwer.\nMathews, B. P., & Diamantopoulos, A. (1986). Managerial\nintervention in forecasting: An empirical investigation of\nforecast manipulation. International Journal of Research in\nMarketing, 3, 3\u201310.\nMathews, B. P., & Diamantopoulos, A. (1987). Alternative\nindicators of forecast revision and improvement. Marketing\nIntelligence and Planning , 5, 20\u201323.\nMathews, B. P., & Diamantopoulos, A. (1989). Judgemental\nrevision of sales forecasts \u2014 a longitudinal extension. Journal\nof F orecasting, 8, 129\u2013140.\nMathews, B. P., & Diamantopoulos, A. (1990). Judgmental revision\nof sales forecasts \u2014 effectiveness of forecast selection. Journal\nof F orecasting, 9, 407\u2013415.\nPlease cite this article in press as: Fildes, R., et al. Effective forecasting and judgmental adjustments: an empirical evaluation and strategies\nfor improvement in supply-chain planning. International Journal of Forecasting (2009), doi:10.1016/j.ijforecast.2008.11.010",
        "ab70e7a6-b44b-4392-8797-cba0dd6f1178": "ARTICLE  IN  PRESS\nR. Fildes et al. / International Journal of F orecasting ( ) \u2013 21\nMathews, B. P., & Diamantopoulos, A. (1992). Judgmental revision\nof sales forecasts \u2014 the relative performance of judgementally\nrevised versus non revised forecasts. Journal of F orecasting , 11,\n569\u2013576.\nMathews, B. P., & Diamantopoulos, A. (1994). T owards a taxonomy\nof forecast error measures \u2014 a factor-comparative investigation\nof forecast error dimensions. Journal of F orecasting , 13,\n409\u2013416.\nMcNees, S. K. (1990). The role of judgment in macroeconomic\nforecasting accuracy. International Journal of F orecasting , 6,\n287\u2013299.\nMoon, M. A., Mentzer, J. T., & Smith, C. D. (2003). Conducting a\nsales forecasting audit. International Journal of F orecasting , 19,\n5\u201325.\nO\u2019Connor, M., Remus, W., & Griggs, K. (1993). Judgmental\nforecasting in times of change. International Journal of\nF orecasting, 9, 163\u2013172.\n\u00a8Onkal, D., G \u00a8on \u00a8ul, M.S., & Lawrence, M. (2007). Judgmental\nadjustments: Effects of prior adjustments and explanations.\nP aper presented at the 27th International Symposium on\nF orecasting.\nSanders, N. R., & Manrodt, K. B. (1994). Forecasting practices\nin United States corporations \u2014 survey results. Interfaces, 24,\n92\u2013100.\nSanders, N. R., & Ritzman, L. P. (1992). The need for contextual\nand technical knowledge in judgmental forecasting. Journal of\nBehavioral Decision Making , 5, 39\u201352.\nSchultz, R. L. (1984). The implementation of forecasting models.\nJournal of F orecasting , 3, 43\u201355.\nSmaros, J. (2007). Forecasting collaboration in the European\ngrocery sector: Observations from a case study. Journal of\nOperations Management , 25, 702\u2013716.\nStaw, B. M. (1976). Knee-deep in the big muddy: A study\nof escalating commitment to a chosen course of action.\nOrganizational Behavior and Human P erformance , 16, 27\u201344.\nSyntetos, A. A., Nikolopoulos, K., Boylan, J. E., Fildes,\nR., & Goodwin, P . (2008). The effects of integrating\nmanagement judgement on intermittent demand forecasts.\nInternational Journal of Production Economics , in press\n(\ndoi:10.1016/j.ijpe.2008.08.011).\nT ashman, L. J., & Hoover, J. (2001). Diffusion of forecasting\nprinciples through software. In J. S. Armstrong (Ed.), Principles\nof forecasting: A handbook for researchers and practitioners ,\n(pp. 651\u2013676). Norwell, MA: Kluwer.\nTheil, H. (1966). Applied economic forecasting . Amsterdam: North-\nHolland.\nTurner, D. S. (1990). The role of judgment in macroeconomic\nforecasting. Journal of F orecasting , 9, 315\u2013345.\nTversky, A., & Kahneman, D. (1974). Judgment under uncertainty:\nHeuristics and biases. Science, 85, 1124\u20131131.\nW aern, Y., & Ramberg, R. (1996). People\u2019s perception of human\nand computer advice. Computers in Human Behavior , 12,\n17\u201327.\nWillemain, T. R. (1991). The effect of graphical adjustment on\nforecast accuracy. International Journal of F orecasting , 7,\n151\u2013154.\nY aniv, I. (2004). Receiving other people\u2019s advice: In\ufb02uence\nand bene\ufb01t.",
        "71cceb93-c1c5-4ddf-851a-225c691419b2": "Turner, D. S. (1990). The role of judgment in macroeconomic\nforecasting. Journal of F orecasting , 9, 315\u2013345.\nTversky, A., & Kahneman, D. (1974). Judgment under uncertainty:\nHeuristics and biases. Science, 85, 1124\u20131131.\nW aern, Y., & Ramberg, R. (1996). People\u2019s perception of human\nand computer advice. Computers in Human Behavior , 12,\n17\u201327.\nWillemain, T. R. (1991). The effect of graphical adjustment on\nforecast accuracy. International Journal of F orecasting , 7,\n151\u2013154.\nY aniv, I. (2004). Receiving other people\u2019s advice: In\ufb02uence\nand bene\ufb01t. Organizational Behavior and Human Decision\nProcesses, 93, 1\u201313.\nY elland, P. M. (2006). Stable seasonal pattern models for\nforecast revision: A comparative study. International Journal of\nF orecasting, 22, 799\u2013818.\nRobert Fildes is Professor of Management Science in the School of\nManagement, Lancaster University , and Director of the Lancaster\nCentre for Forecasting. He has a mathematics degree from Oxford\nand a Ph.D. in statistics from the University of California. He\nwas co-founder of the Journal of F orecasting in 1981 and of\nthe International Journal of F orecasting in l985. For ten years\nfrom l988 he was Editor-in-Chief of the IJF . He was president of\nthe International Institute of Forecasters between 2000 and 2004.\nHis current research interests are concerned with the comparative\nevaluation of different forecasting methods, the implementation of\nimproved forecasting procedures in organizations and the design of\nforecasting systems.\nPaul Goodwin is Professor of Management Science at the\nUniversity of Bath. He has an MSc in Management Science and\nOperational Research from the University of W arwick and a Ph.D.\nin Management Science from the University of Lancaster. His\nresearch interests are concerned with the role of management\njudgment in forecasting and decision making. He is a Director of\nthe International Institute of Forecasters, an Associate Editor of the\nInternational Journal of F orecasting and a member of the editorial\nboard of the Journal of Behavioral Decision Making . He also\nwrites a column in F oresight, the international journal of applied\nforecasting that is aimed at practitioners.\nMichael Lawrence is Emeritus Professor of Information Systems\nin the Commerce and Economics Faculty at the University of New\nSouth W ales, Sydney , Australia. He has a Ph.D. from the University\nof California, Berkeley (specialising in Operations Research).\nBefore joining the university he worked in the USA for the Ciba-\nGeigy Corporation and Corning Inc. He has held visiting positions,\nincluding Lancaster and Bath Universities, England, during the\nresearch reported on here. He is past President of the International\nInstitute of Forecasters (IIF), past editor of the International Journal\nof F orecasting, and served for many years on the Board of Directors\nof the IIF . He is an associate editor of Managerial and Decision\nEconomics. His research interests centre generally on how to best\nsupport decision making where a signi\ufb01cant component of the\ndecision involves management judgment, with a speci\ufb01c interest in\nhow to support the task of short term forecasting.\nKonstantinos Nikolopoulos is Assistant Professor in Decision\nSciences at Manchester Business School. He received his Ph.D.\nfrom the National T echnical University of Greece in the research\n\ufb01eld of Forecasting Support Systems. His research interests are\nin time series forecasting, forecasting support systems, arti\ufb01cial\nintelligence and geometry . He has published several papers in\nrefereed academic journals including the International Journal of\nProduction Economics , International Journal of F orecasting , and\nEuropean Journal of Operational Research.\nPlease cite this article in press as: Fildes, R., et al. Effective forecasting and judgmental adjustments: an empirical evaluation and strategies\nfor improvement in supply-chain planning. International Journal of Forecasting (2009), doi:10.1016/j.ijforecast.2008.11.010",
        "9f46f9bb-58ff-4881-9ffd-c62631d2d13a": "1\nLa Gu\u00eda Completa sobre \nMachine Learning \nen la Previsi\u00f3n de la \nDemanda en Retail\nJohanna Sm\u00e5ros\nCo-founder \nPh.D. (Tech)\nHenri Kaleva\nHead of RELEX Labs\nM.Sc. (Tech)",
        "02e34e08-6531-4430-844e-3d2fc6db3317": "2\nContenido de la Gu\u00eda\n1 \u00bfQu\u00e9 es el Aprendizaje Autom\u00e1tico y por qu\u00e9 los retailers deber\u00edan  \nadoptarlo?           4\n2 El Aprendizaje Autom\u00e1tico aborda los retos de las previsiones de la  \ndemanda del retail          7\n2.1 D\u00eda de la semana, estacionalidad, y otros patrones de demanda recurrentes  8\n2.2 Cambios de precios, promociones, y otras decisiones de negocio que afectan  \nla demanda           9\n2.3 Meteorolog\u00eda, eventos locales, y otros factores externos que afectan las ventas 13\n2.4 Factores desconocidos que tienen un impacto en la demanda    14\n3 Haz que el machine learning trabaje para ti en la gesti\u00f3n de la demand  16\n3.1 Trabajar con productos de baja rotaci\u00f3n       17\n3.2 Aprovechar la experiencia humana        19\n3.3 La previsi\u00f3n de la demanda es solo una parte de la planificaci\u00f3n y optimizaci\u00f3n  \ndel retail           20",
        "a3fd74b1-d02b-4125-bebf-a07665c4cdb8": "3\nEl Retail es la gesti\u00f3n del detalle a gran \nescala\nLa antigua denominaci\u00f3n ya lo dice, el retail es la venta al detalle, el comercio \nminorista a gran escala. Para garantizar unas operaciones fluidas y altos m\u00e1rgenes, \nlos grandes retailers han de estar pendientes de decenas de millones de flujos de \nproductos cada d\u00eda. Y la previsi\u00f3n de la demanda se sit\u00faa en el centro de toda la \nactividad de planificaci\u00f3n.\nUna previsi\u00f3n de la demanda altamente precisa es la \u00fanica manera que los \nretailers tienen para predecir qu\u00e9 productos se necesitan para cada ubicaci\u00f3n \n(tienda, almac\u00e9n, centro de distribuci\u00f3n) y canal en un d\u00eda determinado, lo que \na su vez, es la \u00fanica manera de asegurar una alta disponibilidad para los clientes \nmientras se mantiene un riesgo de stock m\u00ednimo. Aprovechar un pron\u00f3stico fiable \nen las operaciones de retail puede tambi\u00e9n respaldar la gesti\u00f3n de la capacidad, \ngarantizar la cantidad correcta de empleados en las tiendas y centros de \ndistribuci\u00f3n, o ayudar al equipo de compras a gestionar las complejidades de las \ncompras a largo plazo.\nGenerar pron\u00f3sticos precisos es, de hecho, bastante f\u00e1cil en condiciones estables, \npero todos sabemos que el retail es inherentemente din\u00e1mico, con cientos de \nfactores afectando la demanda de forma continuada. Cada d\u00eda, los planificadores \nde la demanda tienen problemas al haber de tener en cuenta un inmenso n\u00famero \nde variables, que incluyen:\n\u2022 Variaciones recurrentes en la demanda como las relacionadas con los d\u00edas \nde la semana y estaciones del a\u00f1o.\n\u2022 Decisiones internas de negocio dise\u00f1adas para captar la atenci\u00f3n del \nconsumidor y ofrecer una ventaja competitiva, como promociones, ajustes \nen los precios o cambios en los displays de las tiendas.\n\u2022 Factores externos como eventos locales, apertura o cierre de una tienda de \nla competencia en el mismo barrio, e incluso, la meteorolog\u00eda.\nCon todos estos datos, no hay planificador humano que pueda tener en cuenta \ntodos estos factores potenciales. Sin embargo, el aprendizaje autom\u00e1tico hace \nposible tener en cuenta el impacto a nivel detallado, por tienda espec\u00edfica o \ncanal de venta. No es sorprendente pues, que actualmente haya tantos retailers \ntransformando sus estrategias tecnol\u00f3gicas hacia una previsi\u00f3n de la demanda \nbasada en el machine learning.",
        "9ea3d45e-5442-49fe-91a2-2976b4f82474": "4\n1  \n\u00bfQu\u00e9 es el Aprendizaje \nAutom\u00e1tico y por qu\u00e9 \nlos retailers deber\u00edan \nadoptarlo?",
        "73ba7aea-65cc-4cfb-92a4-8643a3372670": "5\nEl aprendizaje autom\u00e1tico le da al sistema la capacidad de aprender \nautom\u00e1ticamente y mejorar sus recomendaciones usando solo los datos, sin \nnecesidad de programar. Como los retailers generan enormes cantidades de datos, \nla tecnolog\u00eda machine learning justifica sobradamente su valor. Cuando un sistema \nde machine learning se alimenta de datos \u2013 cuantos m\u00e1s, mejor \u2013 busca patrones. \nY a\u00fan va m\u00e1s all\u00e1, puede utilizar los patrones que identifica en los datos para \ntomar mejores decisiones.\nEl aprendizaje autom\u00e1tico posibilita que al pron\u00f3stico de retail se le pueda \nincorporar la gran variedad de factores y relaciones que afectan la demanda a \ndiario. Esto tiene una gran importancia, ya que s\u00f3lo los datos meteorol\u00f3gicos por \ns\u00ed solos pueden estar compuestos por cientos de factores que potencialmente, \npueden impactar en la demanda. Los algoritmos de machine learning generan \nautom\u00e1ticamente modelos de mejora continua usando solamente los datos que \nse les proporciona, ya sean de nuestro negocio o de flujos de datos externos. El \nprincipal beneficio es que un sistema como \u00e9ste puede procesar conjuntos de \ndatos a escala retail desde una variedad de fuentes, todo sin intervenci\u00f3n humana.\nImagen 1: Machine learning permite que los retailers capten el impacto de los \npatrones recurrentes, sus decisiones de negocio internas, y factores externos \npara previsiones de la demanda a corto y largo plazo m\u00e1s precisas, granulares y \nautomatizadas.",
        "a9a3cc98-4bd8-40c9-8f53-49913012ca37": "6\nLos algoritmos de machine learning no son nuevos \u2013 han estado ah\u00ed durante \nd\u00e9cadas. Pero estos algoritmos nunca antes hab\u00edan tenido acceso a tantos \ndatos o a un poder de procesamiento de datos tan grande. Aunque en el pasado \nlos retailers hayan podido tener dificultades para actualizar las previsiones \nr\u00e1pidamente, ahora la tecnolog\u00eda in-memory y el procesamiento de datos a gran \nescala permiten millones de c\u00e1lculos de pron\u00f3stico por minuto.",
        "17c02e5f-eb32-4595-ab92-40b88133541c": "7\n2  \nEl Aprendizaje Autom\u00e1tico \naborda los retos de las \nprevisiones de la demanda \ndel retail",
        "002ff489-9ba9-4de0-85f8-8211e8041c1b": "8\nEl aprendizaje autom\u00e1tico es una herramienta s\u00faper potente en un entorno como \nel de retail, tan rico en datos. Deber\u00eda aprovecharse en cualquier contexto en el \nque se puedan usar los datos para anticipar o explicar cualquier cambio en la \ndemanda. En algunos casos, incluso puede completar los datos que faltan.\n2.1 D\u00eda de la semana, estacionalidad, y otros patrones \nde demanda recurrentes\nEl modelo de series temporales es un enfoque probado y v\u00e1lido que puede \nentregar buenos pron\u00f3sticos para patrones recurrentes, como los cambios en \nla demanda relacionados con los d\u00edas de la semana o las estaciones. En nuestra \nexperiencia, los pron\u00f3sticos de demanda basados en el aprendizaje autom\u00e1tico \nentregan consistentemente un nivel de precisi\u00f3n m\u00e1s alta que los modelos de \nseries temporales. Mientras que los modelos de series temporales simplemente \naplican patrones pasados a la demanda futura, el aprendizaje autom\u00e1tico va m\u00e1s \nall\u00e1 al intentar definir la relaci\u00f3n real entre las variables (como los d\u00edas de la \nsemana) y sus patrones de demanda asociados.\nImagen 2: Machine Learning aborda todos los requerimientos t\u00edpicos de la \nprevisi\u00f3n de la demanda en retail",
        "3fe020d7-543c-4c54-bb8a-28befaa1807f": "9\nEl Machine Learning simplifica el pron\u00f3stico de demanda del retail. Al usar \nmodelos de series temporales, los retailers deben manipular los resultados del \npron\u00f3stico de ventas de referencia para adaptar el impacto de, por ejemplo, las \npr\u00f3ximas promociones o cambios de precios. Sin embargo, el Machine learning \ntiene en cuenta todos los factores autom\u00e1ticamente.\nAdem\u00e1s de tener en cuenta muchos factores, el machine learning tambi\u00e9n \npuede captar el impacto cuando interact\u00faan m\u00faltiples factores, por ejemplo: la \nmeteorolog\u00eda y un d\u00eda de la semana. D\u00edas c\u00e1lidos y soleados pueden hacer que \naumente mucho la demanda de los productos para barbacoas si coinciden en fin \nde semana.\n2.2 Cambios de precios, promociones, y otras \ndecisiones de negocio que afectan la demanda\nLas propias decisiones del negocio como retailer, son tambi\u00e9n una importante \nfuente de variaciones en la demanda, desde promociones y cambios de precio a \najustes en c\u00f3mo se muestran los productos en las tiendas. A parte del hecho de \nque las empresas normalmente planifican y controlan ellas mismas estos cambios, \nmuchos en el sector son incapaces de predecir su impacto con exactitud.\nEl aprendizaje autom\u00e1tico permite que los retailers puedan modelar con precisi\u00f3n \nla elasticidad de precio de producto, es decir, cu\u00e1nto afectar\u00e1 un cambio de \nprecio en la demanda de ese producto. Esta prestaci\u00f3n es muy valiosa como \nImagen 2: Machine Learning aborda todos los requerimientos t\u00edpicos de la \nprevisi\u00f3n de la demanda en retail",
        "ee7980fb-341d-4bc5-aef6-4a4abc954be3": "10\nparte del pron\u00f3stico de la promoci\u00f3n y al optimizar los precios de las rebajas para \nliquidar stock antes de un cambio de surtido o al final de una temporada. Adem\u00e1s, \nlos retailers tienen que ajustar los precios de forma regular para que reflejen los \nprecios de los proveedores y otros cambios en sus costes.\nEn cualquier caso, la elasticidad de precio por s\u00ed sola no recoge el impacto \ncompleto de los cambios de precios. Un cambio de precio en relaci\u00f3n a productos \nalternativos dentro de la misma categor\u00eda a menudo tambi\u00e9n tiene un fuerte \nimpacto. En muchas categor\u00edas, el producto con el precio m\u00e1s bajo captura una \ncuota de mercado desproporcionadamente grande. Los pron\u00f3sticos de demanda \nbasados en el aprendizaje autom\u00e1tico facilitan considerar la posici\u00f3n del precio de \nun producto, tal y como se muestra en la siguiente tabla:\nEn un estudio de 2020 sobre Supermercados de Norteam\u00e9rica, el 70% de \nlos encuestados indic\u00f3 que no podr\u00edan tener en cuenta todos los aspectos \nimportantes de una promoci\u00f3n \u2013 precio, tipo de promoci\u00f3n, o el display en tienda \n\u2013 al pronosticar las subidas promocionales. Pero desear\u00edan poder hacerlo.\nImagen 4: La demanda de este producto aumenta cuando su precio baja mucho, \npero el aumento es mayor cuando su precio es el m\u00e1s bajo de su categor\u00eda.",
        "518cf108-850c-4442-b991-c07fab6dc407": "11\nAqu\u00ed tambi\u00e9n puede ayudar el aprendizaje autom\u00e1tico. Al usar previsiones de \ndemanda basadas en machine learning, los retailers pueden predecir con precisi\u00f3n \nel impacto de las promociones teniendo en cuenta factores que incluyen, entre \notros:\n\u2022 Tipo de promoci\u00f3n, como la reducci\u00f3n de precio o multi compra.\n\u2022 Actividades de m\u00e1rketing, como los anuncios o la se\u00f1alizaci\u00f3n en tienda.\n\u2022 Reducciones de precios de los productos.\n\u2022 Presentaci\u00f3n de los productos promocionados, como los displays en tienda.\nLa canibalizaci\u00f3n de ventas, el fen\u00f3meno que hace que el aumento por \npromoci\u00f3n de un producto cause una disminuci\u00f3n en las ventas de otros productos \nde la misma categor\u00eda, es bastante com\u00fan y tambi\u00e9n ha de tenerse en cuenta al \nhacer las previsiones, especialmente para los productos frescos. Por ejemplo, si \nun supermercado tiene dos marcas de carne de ternera picada org\u00e1nica \u2013 Vaca \nFeliz y La Ternera Verde \u2013 deber\u00eda esperar que una promoci\u00f3n del producto Vaca \nFeliz haga que la compre m\u00e1s gente. Pero como resultado, parte de la demanda de \nLa Ternera Verde se ir\u00e1 a Vaca Feliz. Si el pron\u00f3stico de demanda de los productos \nTernera Verde no se reduce con precisi\u00f3n, el retailer tiene un gran riesgo de sobre \nstock que finalmente acabar\u00e1 en mermas.\nImagen 5: Para este producto, cuando se coloca en un expositor g\u00f3ndola sin \ncambiarle el precio, da como resultado un notable aumento de las ventas, pero la \nsubida es modesta comparada con el efecto de un 50% de descuento.",
        "549abd68-7012-45be-80c1-6a9b7fcf9888": "12\nAjustar manualmente los pron\u00f3sticos para todos los productos potencialmente \ncanibalizados es algo imposible en la mayor\u00eda de los contextos de retail porque \nel n\u00famero de productos es sencillamente demasiado alto. Los patrones son \nnormalmente bastante espec\u00edficos para los surtidos de las tiendas individuales y \nlos patrones de compra. Aqu\u00ed es donde entra la capacidad de los algoritmos del \naprendizaje autom\u00e1tico para identificar autom\u00e1ticamente los patrones y ajustar los \npron\u00f3sticos en consecuencia, lo que a\u00f1ade un gran valor.\nPor otro lado, una promoci\u00f3n de los productos de Vaca Feliz har\u00e1 que \nprobablemente aumenten las ventas de algunos de los productos relacionados \ndiferentes a la carne picada, y a esto se le llama efecto halo. Panes para \nhamburguesas, por ejemplo, tienen una relaci\u00f3n obvia y predecible con la carne \npicada. Lamentablemente, el impacto puede ser tan difuso en el surtido que \nidentificar cada producto afectado se hace casi imposible, incluso con machine \nlearning: pensemos en cebollas, patatas chips, cervezas, kit para tacos, pasta de \nlasa\u00f1a, mostaza\u2026o cualquier otro art\u00edculo que los compradores puedan asociar \ncon un plato de carne picada. Pero si ni siquiera los sistemas de pron\u00f3stico pueden \nidentificar todas las relaciones de halo posibles, lo que s\u00ed deber\u00edan hacer es \npermitir que los planificadores puedan ajustar f\u00e1cilmente los pron\u00f3sticos para las \nrelaciones que s\u00ed saben que existen.\nImagen 6: Los algoritmos de machine learning pueden modelar con precisi\u00f3n los \nefectos de canibalizaci\u00f3n en relaci\u00f3n con las promociones o los cambios de precio \nen los datos hist\u00f3ricos.",
        "a582ce48-0a1d-4730-8ad6-84f9e66e87b2": "13\n2.3 Meteorolog\u00eda, eventos locales, y otros factores \nexternos que afectan las ventas\nLos factores externos como la meteorolog\u00eda, conciertos y actividades deportivas \nlocales, y cambios de precio de la competencia pueden tener un impacto \nsignificativo en la demanda, pero son dif\u00edciles de tener en cuenta en los \npron\u00f3sticos sin un sistema que automatice una gran cantidad del trabajo. A nivel \ngeneral, el impacto puede ser bastante intuitivo. Durante un d\u00eda c\u00e1lido, es muy \nprobable que aumenten las ventas de helados, mientras que en la temporada de \nlluvias, se ver\u00e1 un aumento de la demanda de paraguas, etc.\nPero al observar el surtido completo de retail, el reto se complica a\u00fan m\u00e1s, \u00bfc\u00f3mo \nse pueden identificar de manera efectiva todos los productos que reaccionan a la \nmeteorolog\u00eda? \u00bfpodemos tener en cuenta todas las variables que comportan las \nprevisiones meteorol\u00f3gicas \u2013 temperatura, sol, lluvias, etc.? \u00bfEl impacto de un d\u00eda \nsoleado ser\u00e1 mayor en verano que en invierno? \u00bfO es mayor en fines de semana que \nentre semana?\nEl uso de los datos meteorol\u00f3gicos en las previsiones de la demanda es un \nexcelente ejemplo del poder del aprendizaje autom\u00e1tico. Los algoritmos de \nmachine learning pueden detectar autom\u00e1ticamente las relaciones entre las \nvariables meteorol\u00f3gicas locales y las ventas locales. Pueden mapear estas \nrelaciones en un nivel m\u00e1s granular y localizado que cualquier esfuerzo humano, \ny tambi\u00e9n pueden identificar y actuar en relaciones menos obvias, por las que la \nintuici\u00f3n o el sentido com\u00fan de las personas podr\u00edan pasar por alto.\nCuando a los planificadores de demanda o a los empleados de tienda se les pide \nque comprueben manualmente las previsiones meteorol\u00f3gicas que pueden influir \nen las decisiones de compra, se centran en asegurar el suministro de los productos \nm\u00e1s afectados por el incremento esperado de la demanda, como por ejemplo, \nlos helados en una ola de calor, por ejemplo. Pero \u00bfalguien tiene tiempo para \ndisminuir ligeramente los pron\u00f3sticos de helados durante las semanas lluviosas o \nde aperitivos fr\u00edos en verano? Un equipo de planificaci\u00f3n que usa machine learning \nno tiene por qu\u00e9 preocuparse en hacer ajustes de este tipo, ya que su sistema \npuede sugerirlos de forma autom\u00e1tica.\nEn nuestra experiencia, considerar autom\u00e1ticamente los efectos de la \nmeteorolog\u00eda en los pron\u00f3sticos de la demanda reduce los errores entre un 5% y \nun 15% a nivel producto para los productos sensibles al clima y hasta un 40% a \nnivel grupo de producto y tienda.",
        "62385e41-2deb-4f07-8a1d-cfbbb42c7bf7": "14\nPero los datos meteorol\u00f3gicos no son los \u00fanicos datos externos que podr\u00edan o \ndeber\u00edan incorporarse en los pron\u00f3sticos de demanda del retail. Cualquier fuente \nde datos externos, como eventos locales pasados y futuros (partidos de f\u00fatbol o \nconciertos), datos sobre los precios de la competencia, y datos sobre movilidad \npueden usarse para mejorar los resultados.\nComo ejemplo, RELEX utiliza el aprendizaje autom\u00e1tico para ayudar a WHSmith \na comprender mejor c\u00f3mo los horarios de los vuelos afectaban los patrones de \ndemanda en sus aeropuertos. Al introducir en el sistema los datos externos de las \naerol\u00edneas, WHSmith mejor\u00f3 sus pron\u00f3sticos y pudo reducir las mermas de sus \nproductos frescos a la vez que mejoraba la disponibilidad\n2.4 Factores desconocidos que tienen un impacto en \nla demanda\nHasta ahora, hemos visto contextos en los que los factores que impactan la \ndemanda \u2013 patrones semanales y estacionales, decisiones de negocio, y factores \nexternos \u2013 son f\u00e1cilmente identificables. Pero el machine learning puede ayudar \na ajustar los pron\u00f3sticos incluso en situaciones donde los factores afectantes, ya \nsean internos o externos, sean desconocidos.\nEn las tiendas de retail f\u00edsicas, las circunstancias locales \u2013 como la competencia \ndirecta abriendo o cerrando una tienda cercana \u2013 pueden causar un cambio en \nla demanda. Lamentablemente, los datos sobre el factor que causa este cambio \nImagen 7: Machine learning posibilita que los retailers introduzcan en las ventas \nel impacto de la meteorolog\u00eda y otros factores externos para el c\u00e1lculo de los \npron\u00f3sticos.",
        "367d6db0-8e41-42ee-a9f6-8bd0a874f253": "15\npuede que no queden registrados en ning\u00fan sistema. A veces, las decisiones \ninternas de los retailers tampoco se registran, como cuando se ampl\u00eda la \nexposici\u00f3n de un producto poni\u00e9ndolo en un expositor especial adem\u00e1s de su \nubicaci\u00f3n habitual.\nPor suerte, el aprendizaje autom\u00e1tico puede ayudar en estas situaciones. Los \nalgoritmos de machine learning pueden proponer tentativamente un \u201cpunto de \ncambio\u201d en el modelo de pron\u00f3stico, luego rastrear los datos subsecuentes para \nrechazar o validar la hip\u00f3tesis. Esto permite que los pron\u00f3sticos se adapten r\u00e1pida \ny autom\u00e1ticamente a los nuevos niveles de demanda.\nTomemos como ejemplo la tabla de abajo, en el que se ha creado un expositor para \nun producto adem\u00e1s del espacio normal de estanter\u00eda. Aunque este cambio no se \nregistr\u00f3 en los datos maestros, el sistema fue capaz de relacionar el cambio en la \ndemanda con un cambio en c\u00f3mo se expon\u00eda el producto en tienda.\nImagen 8: Los cambios escalonados en la demanda para este producto \u2013 primero \nsube y luego baja \u2013 son el resultado de tener un expositor adicional del producto \nen tienda. Este display adicional no se registr\u00f3 en los datos maestros de la tienda, \npero el algoritmo de machine learning pudo detectar el cambio con muy poca \ndemora y ajustar el pron\u00f3stico.",
        "7eea41de-6009-4342-8128-3f3da5ef4905": "16\n3  \nHaz que el machine \nlearning trabaje para ti en \nla gesti\u00f3n de la demanda",
        "1765e6b9-6ac3-4347-8e19-2b9b063cbb4f": "17\nAunque el aprendizaje autom\u00e1tico es cada vez m\u00e1s com\u00fan, los retailers deber\u00edan \ntener en cuenta algunas consideraciones sobre como usarlo en sus negocios. \nAlgunas consideraciones son espec\u00edficas para el contexto de retail, mientras \nque otras, como el nivel de transparencia, por ejemplo, son lo suficientemente \ngen\u00e9ricas como para aplicarlas a cualquier situaci\u00f3n que requiera un trabajo en \nequipo persona-computadora\n3.1 Trabajar con productos de baja rotaci\u00f3n\nUno de los retos espec\u00edficos de retail es que, pese a la gran cantidad de datos de \nlos que se dispone, la cantidad de datos disponibles por producto, tienda/canal, \ny factores que afectan en la demanda, a veces es muy poca. Incluso si sus ventas \nanuales son de miles de millones, el volumen total se distribuye entre decenas de \nmillones en flujos de inventarios y cientos de d\u00edas.\nLas ventas de los productos de baja rotaci\u00f3n \u2013 aquellos productos de los que \nsolamente se venden unas pocas unidades por d\u00eda o semana \u2013 a menudo tienen \nmuchas variaciones aleatorias, y puede ser dif\u00edcil identificar de forma fiable los \npatrones de relaciones entre tanto ruido. Con pocos puntos de datos disponibles \n(decenas o cientos, en vez de miles), diferenciar el impacto de los factores que \ninfluyen en la demanda (la meteorolog\u00eda, los cambios de precios, cambios en los \ndisplays, o actividades de la competencia) de la variaci\u00f3n aleatoria se convierte en \nun gran desaf\u00edo.\nCuando los productos con un volumen de ventas bajo introducen una cantidad \nsignificativa de variaciones aleatorias, se corre el riesgo de sobreajustar y el \nalgoritmo se vuelve demasiado complejo o contiene demasiadas variables. \nEn situaciones de sobreajuste el algoritmo puede terminar memorizando el \nruido, en vez de encontrar la verdadera se\u00f1al de demanda subyacente. Este \nmodelo sobreajustado acabar\u00eda haciendo predicciones basadas en ruido. Podr\u00eda \nfuncionar excepcionalmente bien si se usan los datos de entrenamiento del \nmodelo, pero extremadamente mal cuando se le pide que incorpore datos nuevos \ncon un comportamiento desconocido. Por lo general, un sobreajuste da como \nresultado unos pron\u00f3sticos con valores at\u00edpicos discordantes con las muestras, \no pron\u00f3sticos nerviosos, donde el pron\u00f3stico reacciona demasiado a cambios \nmenores en los datos.",
        "b9d63325-412a-46ff-afdd-b1bb65add8eb": "18\nAdem\u00e1s, puede que no sea posible detectar un patr\u00f3n estacional a nivel producto-\ntienda para los productos de baja rotaci\u00f3n, pero un an\u00e1lisis de las ventas totales \na nivel cadena para ese producto puede identificar f\u00e1cilmente un patr\u00f3n claro. \nEste patr\u00f3n ha de tenerse en cuenta en el reabastecimiento de los centros de \nsuministro y distribuci\u00f3n.\nDebido a los bajos vol\u00famenes y la dispersi\u00f3n de los datos a nivel producto-tienda/\ncanal en retail, es muy importante que:\n1. Los algoritmos de machine learning usados sean lo suficientemente robustos \ncomo para no entregar resultados at\u00edpicos basados en puntos de datos \nescasos.\n2. Los algoritmos de machine learning eviten sobreajustes minimizando o \neliminando los factores que tienen un impacto en la demanda bajo o nulo.\n3. Los algoritmos de machine learning se pueden aplicar no solo a nivel producto-\ntienda/canal sino tambi\u00e9n a diferentes niveles de agregaci\u00f3n (por ejemplo: \nproducto-regi\u00f3n o producto-tienda) y con agrupaciones flexibles.\nImagen 9: Para evitar sobreajustes al aplicar el aprendizaje autom\u00e1tico a \ndatos dispersos de ventas de retail, el sistema ha de poder: 1) tender hacia la \nsimplicidad al eliminar los factores que tienen poco impacto en la demanda, y \n2) suministrar datos significativos que permitan al sistema encontrar relaciones \nrelevantes de forma efectiva.",
        "054b7b9c-7651-40a0-b2d8-a370b825794b": "19\n3.2 Aprovechar la experiencia humana\nLa crisis de la COVID-19 ha demostrado que los procesos autom\u00e1ticos para las \nprevisiones de la demanda y el reabastecimiento son extremadamente \u00fatiles \ncuando los retailers se enfrentan a perturbaciones a gran escala, ya que la \nautomatizaci\u00f3n hace que se libere mucho tiempo de planificaci\u00f3n. Pero en \ncualquier caso, los planificadores siguen siendo necesarios para guiar al sistema \ncuando surgen eventos ins\u00f3litos que tienen un gran impacto. En estas situaciones, \nlas decisiones deber\u00edan ser algo m\u00e1s que tratar de hacer buenas predicciones, los \nretailers deben evaluar el riesgo comercial de los escenarios positivos y negativos. \nPara crear una interacci\u00f3n humano-computadora efectiva, ya sea en escenarios \ntan excepcionales como el provocado por el coronavirus o durante periodos de \ndemanda m\u00e1s normales, los retailers necesitan hacer an\u00e1lisis que permitan llegar \na conclusiones y a un plan de acci\u00f3n que los respalde.\nComo las previsiones nunca son perfectas, siempre habr\u00e1 situaciones en las que \nlos planificadores tengan que diseccionar un pron\u00f3stico. Cuando los planificadores \npueden acceder f\u00e1cilmente a los factores que se han usado para producir los \npron\u00f3sticos, y entienden c\u00f3mo se han usado, es m\u00e1s probable que conf\u00eden en el \nsistema para gestionar situaciones de negocio \u201cnormales\u201d, y as\u00ed se pueden centrar \nen las situaciones excepcionales que son las que necesitan realmente su atenci\u00f3n. \nLos sistemas de caja negra con poca transparencia imposibilitan que se sepa por \nqu\u00e9 se hacen las recomendaciones automatizadas. Erosionan la confianza del \nusuario, y a menudo generan tasas de adopci\u00f3n del sistema bajas",
        "3a8a5528-6e4e-4f54-b52f-c2dfd37c8a92": "20\nUna soluci\u00f3n transparente tambi\u00e9n ofrece a los planificadores informaci\u00f3n valiosa \npara futuras mejoras \u2013 ya sean mejores datos, la necesidad de una clasificaci\u00f3n \nadicional del producto, o probar nuevas combinaciones de factores, como una \nvariable de \u201cprecio m\u00e1s bajo\u201d.\n3.3 La previsi\u00f3n de la demanda es solo una parte de la \nplanificaci\u00f3n y optimizaci\u00f3n del retail\nPara finalizar, deber\u00edamos tener en mente que, aunque la previsi\u00f3n de la \ndemanda del retail es esencial, incluso grandes cantidades de pron\u00f3sticos no \nsirven para nada si no se usan de forma inteligente para dirigir las decisiones de \nnegocio. Al gestionar productos de baja rotaci\u00f3n, por ejemplo, la precisi\u00f3n de \npron\u00f3stico es mucho menos importante para la rentabilidad que la optimizaci\u00f3n \ndel reabastecimiento y los espacios, que impulsar\u00e1n flujos de productos bien \nImagen 10: La inteligencia artificial no puede ser una caja negra. Los \nplanificadores tienen que entender c\u00f3mo se ha producido el pron\u00f3stico y han de \npoder ajustar los c\u00e1lculos usando su experiencia.",
        "5ba90cdd-e9cc-415a-a73b-c475d2dd4e95": "21\nequilibrados de baja rotaci\u00f3n en toda la cadena de suministro. Los retailers m\u00e1s \navanzados est\u00e1n aplicando la inteligencia artificial en sus principales procesos \nde planificaci\u00f3n \u2013 demanda, operaciones y merchandising \u2013 para mejorar la \nrentabilidad y sostenibilidad.\nSi bien la implantaci\u00f3n de los pron\u00f3sticos de demanda basados en machine \nlearning ofrece una base s\u00f3lida para empezar con la IA aplicada, el retailer no \ndeber\u00eda conformarse solo con esto. La IA ha demostrado ya su valor para abordar \nuna amplia variedad de los retos de planificaci\u00f3n t\u00edpicos del retail: desde la \noptimizaci\u00f3n de las jornadas laborales a una reposici\u00f3n de los productos m\u00e1s \nefectiva en las tiendas y una optimizaci\u00f3n de los descuentos automatizada y \nde m\u00e1s impacto. Con la aplicaci\u00f3n de la IA pragm\u00e1tica en todos los procesos \nprincipales del retail se obtendr\u00e1n resultados de forma sorprendentemente f\u00e1cil y \ncon grandes beneficios r\u00e1pidamente.\nImagen 11: La previsi\u00f3n de la demanda es s\u00f3lo un \u00e1rea de aplicaci\u00f3n de la IA en \nretail. Los retailers m\u00e1s avanzados aplican tambi\u00e9n la IA al merchandising y a \noperaciones para mejorar la rentabilidad y la sostenibilidad.",
        "c54d1e29-e4cc-4382-a12b-1944cf131c34": "22\nSobre RELEX Solutions\nRELEX Solutions ayuda a las empresas retail a mejorar su competitividad \na trav\u00e9s de surtidos personalizados, uso eficiente del espacio, previsi\u00f3n y \nreabastecimiento precisos y una planificaci\u00f3n optimizada de los equipos \nde trabajo. Nuestra soluci\u00f3n \u2018SaaS\u2019 brinda un r\u00e1pido retorno de la inversi\u00f3n \ny se puede usar de forma independiente o conjunta para la planificaci\u00f3n \nunificada, lo que permite la optimizaci\u00f3n multifuncional de los procesos \nprincipales del comercio de retail: comercializaci\u00f3n, cadena de suministro y \noperaciones de tienda. RELEX Solutions cuenta con la confianza de marcas \nl\u00edderes como MediaMarkt, WHSmith, Morrisons, AO.com, Coop Denmark y \nRossmann y tiene oficinas en Norteam\u00e9rica y Europa.\nwww.relexsolutions.es",
        "dbd958f8-8977-44e1-a1b4-d75b83746d7e": "Data Mining and Knowledge Discovery (2023) 37:788\u2013832\nhttps://doi.org/10.1007/s10618-022-00894-5\nForecast evaluation for data scientists: common pitfalls and\nbest practices\nHansika Hewamalage 1 \u00b7 Klaus Ackermann 2 \u00b7 Christoph Bergmeir 3\nReceived: 4 April 2022 / Accepted: 7 November 2022 / Published online: 2 December 2022\n\u00a9 The Author(s) 2022\nAbstract\nRecent trends in the Machine Learning (ML) and in particular Deep Learning (DL)\ndomains have demonstrated that with the availability of massive amounts of time\nseries, ML and DL techniques are competitive in time series forecasting. Neverthe-\nless, the different forms of non-stationarities associated with time series challenge\nthe capabilities of data-driven ML models. Furthermore, due to the domain of fore-\ncasting being fostered mainly by statisticians and econometricians over the years,\nthe concepts related to forecast evaluation are not the mainstream knowledge among\nML researchers. We demonstrate in our work that as a consequence, ML researchers\noftentimes adopt \ufb02awed evaluation practices which results in spurious conclusions\nsuggesting methods that are not competitive in reality to be seemingly competitive.\nTherefore, in this work we provide a tutorial-like compilation of the details associated\nwith forecast evaluation. This way, we intend to impart the information associated\nwith forecast evaluation to \ufb01t the context of ML, as means of bridging the knowledge\ngap between traditional methods of forecasting and adopting current state-of-the-art\nML techniques.We elaborate the details of the different problematic characteristics of\ntime series such as non-normality and non-stationarities and how they are associated\nwith common pitfalls in forecast evaluation. Best practices in forecast evaluation are\noutlined with respect to the different steps such as data partitioning, error calculation,\nResponsible editor: Eamonn Keogh\nB Christoph Bergmeir\nChristoph.Bergmeir@monash.edu\nHansika Hewamalage\nh.hewamalage@unsw.edu.au\nKlaus Ackermann\nKlaus.Ackermann@monash.edu\n1 School of Computer Science & Engineering, University of New South Wales, Sydney, Australia\n2 SoDa Labs and Department of Econometrics & Business Statistics, Monash Business School,\nMonash University, Melbourne, Australia\n3 Department of Data Science and AI, Faculty of IT, Monash University, Melbourne, Australia\n123",
        "c2550989-8606-4dd7-a8fc-53622502749f": "Forecast evaluation for... 789\nstatistical testing, and others. Further guidelines are also provided along selecting valid\nand suitable error measures depending on the speci\ufb01c characteristics of the dataset at\nhand.\nKeywords Time series forecasting \u00b7 Forecast evaluation\n1 Introduction\nIn the present era of Big Data, Machine Learning (ML) and Deep Learning (DL)\nbased techniques are driving the automatic decision making in many domains such\nas Natural Language Processing (NLP) or Time Series Classi\ufb01cation (TSC, Bagnall\net al. 2016; Fawaz et al. 2019). Although \ufb01elds such as NLP and Computer Vision\nhave heavily been dominated by ML and DL based techniques for decades by now, this\nhas hardly been the case for the \ufb01eld of forecasting, until very recently. Forecasting\nwas traditionally the \ufb01eld of statisticians and econometricians. However, with massive\nscales of data being collected nowadays, ML and DL has now emerged as the state\nof the art for many forecasting tasks. Furthermore, with many companies hiring data\nscientists, often these data scientists are tasked with forecasting. Therefore, now in\nmany situations practitioners tasked with forecasting have a good background in ML\nand data science, but are less aware of the decades of research in the forecasting space.\nThis involves many aspects of the process of forecasting, from the point of data pre-\nprocessing, building models to \ufb01nal forecast evaluation. Due to the self-supervised\nand sequential nature of forecasting tasks, it is often associated with many pitfalls that\nusual ML practitioners are not aware of. The usage of bad evaluation practices worsens\nthis problem since they are not clearly distinguishing the truly competitive methods\nfrom the inferior ones by avoiding spurious results. Evaluating the performance of\nmodels is key to the development of concepts and practices in any domain. Hence, in\nthis particular work, we focus on the evaluation of point forecasts as a key step in the\noverall process of forecasting.\nThe general process of forecast evaluation involves employing a number of models\nhaving different characteristics, training them on a training dataset and then applying\nthem on a validation set afterwards. Then, model selection may be performed by eval-\nuating on the validation set to select the best models. Otherwise, ensemble models\nmay be developed instead, by combining the forecasts from all the different models,\nand usually a \ufb01nal evaluation is then performed on a test set (Godahewa et al. 2021).\nIn research areas such as classi\ufb01cation and regression, there are well-established stan-\ndard practices for evaluation. Data partitioning is performed by using a standard k-fold\nCross-V alidation (CV) to tune the model hyperparameters based on the error on a vali-\ndation set, the model with the best hyperparameter combination is tested on the testing\nset, standard error measures such as squared errors, absolute errors or precision, recall,\nor area under the curve are computed and \ufb01nally the best models are selected. These\nbest methods may continue to deliver reasonable predictions for a certain problem\ntask, i.e., they generalize well, under the assumption that there are no changes of the\ndistribution of the underlying data, which otherwise would need to be addressed as\n123",
        "c7d434aa-c831-4c83-8025-4e282a1835f7": "790 H. Hewamalage et al.\nconcept drift (Webb et al. 2016; Ghomeshi et al. 2019; Ikonomovska et al. 2010)o r\nnon-stationarity.\nIn contrast, evaluating forecasting models can be a surprisingly complicated task,\nalready for point forecasting. Data partitioning has many different options in the con-\ntext of forecasting, including \ufb01xed origin, rolling origin evaluation and other CV\nsetups as well as controversial arguments associated with them. Due to the inherent\ndependency, non-stationarity and non-normality of time series, these choices are com-\nplex. Also, most error measures are susceptible to break down under certain of these\nconditions. Other considerations are whether to summarize errors across all available\ntime series or consider different steps of the forecast horizon separately etc. As a\nconsequence, we regularly come across papers in top Arti\ufb01cial Intelligence (AI)/ML\nconferences and journals (even winning best paper awards) that use inadequate and\nmisleading benchmark methods for comparison (e.g., non-seasonal models for long-\nterm forecasting on seasonal series), others that use mean absolute percentage error\n(MAPE) for evaluation with series, e.g., with values in the [\u22121, 1] interval because\nthe authors think the MAPE is a somewhat generic \u201ctime series error measure\u201d, even\nthough MAPE is clearly inadequate in such settings. Other works make statements\nalong the lines of Auto-Regressive Integrated Moving Average (ARIMA) being able\nto tackle non-stationarity whereas ML models can\u2019t, neglecting that the only thing\nARIMA does is a differencing of the series as a pre-processing step to address non-\nstationarity. A step that can easily be done as preprocessing for any ML method as\nwell. In other works, we see methods compared using Mean Absolute Error (MAE)\nas the error measure, and only the proposed method by those authors is trained with\nL1 loss, all other competitors with L2 loss, which leads to unfair comparisons as the\nL1 loss optimizes towards MAE, whereas the L2 loss optimizes towards Root Mean\nSquared Error (RMSE). Many other works evaluate on a handful of somewhat ran-\ndomly picked time series and then show plots of forecasts versus actuals as \u201cproof\u201d of\nhow well their method works, without considering simple benchmarks or meaningful\nerror measures, and other similar problems. Also, frequently forecasting competitions\nand research works introduce new evaluation measures and methodologies, some-\ntimes neglecting the prior research, e.g., by seemingly not understanding that dividing\na series by its mean will not solve scaling issues for many types of non-stationarities\n(e.g., strong trends). Thus, there is no generally accepted standard for forecast eval-\nuation in every possible scenario. This gap has harmed the evaluation practices used\nalong with ML methods for forecasting signi\ufb01cantly in the past. It is damaging the\narea currently, with spurious results in many papers, with researchers new to the \ufb01eld\nnot being able to distinguish between methods that work and methods that don\u2019t, and\nthe associated waste of resources.\nOverall, this article makes an effort in the direction of raising awareness among ML\npractitioners regarding the best practices and pitfalls associated with the different steps\nof the point forecast evaluation process. Similar exhaustive efforts have been taken\nin the literature to review, formally de\ufb01ne and categorize other important concepts\nin the ML domain such as concept drift (Webb et al. 2016), concept drift adapta-\ntion (Gama et al. 2014) and mining statistically sound patterns from data (H\u00e4m\u00e4l\u00e4inen\nand Webb 2019). In the time series space, less comprehensive and/or systematic works\nin the direction of certain aspects of our work exist. Cerqueira et al. ( 2020) have per-\n123",
        "842b61b5-62f1-4b5f-bf90-997949a256ee": "Forecast evaluation for... 791\nformed empirical studies using different data partitioning and performance estimation\nmethods on some real-world and synthetic datasets and presented guidelines around\nwhich methods work under different characteristics of time series. In the work by\nPetropoulos (2022) as well, those authors have a section dedicated to explaining fore-\ncast evaluation measures, best practices for both point and probabilistic forecasting as\nwell as benchmarking. Ditzler et al. ( 2015) have conducted a survey on existing meth-\nods for learning in non-stationary environments and the associated dif\ufb01culties and\nchallenges. In the work by Shcherbakov et al. ( 2013), those authors have presented a\nreview on several error measures for forecast evaluation along with their drawbacks\nand also proposed another new measure to speci\ufb01cally become robust to outliers on\ntime series. Recommendations have also been given around selecting error measures\nunder a speci\ufb01c context. Gujarati ( 2021) has provided a comprehensive overview on\nrecent developments in econometric techniques in general using many examples.\nThe rest of this paper is structured as follows. Section 2 \ufb01rst introduces ter-\nminology associated with forecast evaluation, including different forms of non-\nstationarities/non-normality seen in time series data. Next, Sect. 3 details the\nmotivation for this article, along with common pitfalls seen in the literature related\nto using suf\ufb01cient datasets, selecting appropriate measures for evaluation, using com-\npetitive benchmarks, visualisation of results using forecast plots and data leakage\nin forecast evaluation. Then, in Sect. 4, we provide best practices and guidelines\naround different aspects of forecast evaluation including how to best partition the\ndata for a given forecasting problem with non-stationarities involved with the series,\nhow to select evaluation measures depending on the characteristics of the time series\nunder consideration and details of popular techniques used for statistical testing for\nsigni\ufb01cance of differences between models. Finally, Sect. 5 concludes the paper by\nsummarising the overall \ufb01ndings of the paper and highlighting the best practices for\nforecast evaluation. The code used for this work is publicly available for reproducibil-\nity of the results.\n1\n2 Terminology of forecast evaluation\nThis article focuses on point forecast evaluation, where the interest is to evaluate one\nparticular statistic (mean/median) of the overall forecast distribution. However, we\nnote that there are many works in the literature around predicting distributions and\nevaluating accordingly. Figure 1 indicates a common forecasting scenario with the\ntraining region of the data, the forecast origin which is the last known data point from\nwhich the forecasting begins and the forecast horizon. In this section we provide a\ngeneral overview of the terminology used in the context of forecast evaluation.\nIn forecast evaluation, similar to other ML tasks, validation and test sets are used\nfor hyperparameter tuning of the models and for testing. Evaluations on validation and\ntest sets are often called out-of-sample (OOS) evaluations in forecasting. The two main\nsetups for OOS evaluation in forecasting are \ufb01xed origin evaluation and rolling origin\nevaluation (Tashman 2000). Figure 2 shows the difference between the two setups.\n1 Available at https://github.com/HansikaPH/Forecast_Evaluation_Pitfalls.\n123",
        "b53580d7-7c1b-427c-9c46-ce92ed103771": "792 H. Hewamalage et al.\nFig. 1 A forecasting scenario with training region of the data, forecast origin and the forecast horizon\nIn the \ufb01xed origin setup, the forecast origin is \ufb01xed as well as the training region,\nand the forecasts are computed as one-step ahead or multi-step ahead depending on\nthe requirements. In the rolling origin setup, the size of the forecast horizon is \ufb01xed,\nbut the forecast origin changes over the time series (rolling origin), thus effectively\ncreating multiple test periods for evaluation. With every new forecast origin, new data\nbecomes available for the model which can be used for re-\ufb01tting of the model. The\nrolling origin setup is also called time series cross-validation (tsCV) and prequential\nevaluation in the literature (Hyndman and Athanasopoulos 2018;G a m ae ta l . 2013).\nTime series can have different forms of non-stationarities and non-normality and\nthey make time series forecasting and evaluation a more dif\ufb01cult problem in com-\nparison to other ML tasks. Listed below are some of such possibly problematic\ncharacteristics of time series.\n1. Non-stationarities.\n\u2022 Seasonality\n\u2022 Trends (Deterministic, e.g., Linear/Exponential)\n\u2022 Stochastic Trends / Unit Roots\n\u2022 Heteroscedasticity\n\u2022 Structural Breaks (sudden changes, often with level shifts)\n2. Non-normality\n\u2022 Non-symmetric distributions\n\u2022 Fat tails\n123",
        "9758fdc6-6b8c-4e96-a1c3-75640613046e": "Forecast evaluation for... 793\nFig. 2 Comparison of \ufb01xed origin versus rolling origin setups. The blue and orange data points represent\nthe training and testing sets respectively at each evaluation. The \ufb01gure on the left side shows the \ufb01xed origin\nsetup where the forecast origin remains constant. The \ufb01gure on the right shows the rolling origin setup\nwhere the forecast origin rolls forward and the forecast horizon is constant. The red dotted lined triangle\nencloses all the time steps used for testing across all the evaluations. Compared to the \ufb01xed origin setup, it\nis seen that in the rolling origin setup, testing data instances in each evaluation pass on to the training set in\nthe next evaluation step\n\u2022 Intermittency\n\u2022 Outliers\n3. Series with very short history\nNon-stationarity in general means that the distribution of the data in the time series is\nnot constant, but it changes depending on the time (see, e.g., Salles et al. 2019). What\nwe refer to as non-stationarity in this work is the violation of strong stationarity de\ufb01ned\nas in Eq. ( 1) (Cox and Miller 1965). Strong stationarity is de\ufb01ned as the distribution\nof a \ufb01nite window (sub-sequence) of a time series (discrete-time stochastic process)\nremaining the same as we shift the window across time. In Eq. ( 1), y\nt refers to the time\nseries value at time step t; \u03c4 \u2208 Z is the size of the shift of the window and n \u2208 N is the\nsize of the window. FY (yt+\u03c4 , yt+1+\u03c4 , ...,yt+n+\u03c4 ) refers to the cumulative distribution\nfunction of the joint distribution of (yt+\u03c4 , yt+1+\u03c4 , ...,yt+n+\u03c4 ). Hence, according to\nEq. ( 1), FY is not a function of time, it does not depend on the shift of the window.\nIn the rest of this paper, we refer to the violation of strong stationarity simply as\nnon-stationarity.\nF\nY (yt+\u03c4 , yt+1+\u03c4 , ...,yt+n+\u03c4 ) = FY (yt , yt+1, ...,yt+n), for all \u03c4 \u2208 Z and n \u2208 N\n(1)\nFigure 3 gives an example of possible problems when building ML models on such\ndata, where the models fail to produce reasonable forecasts as the range of values is\ndifferent in the training and test sets. Different types of non-stationarities are illustrated\nin Fig. 4. Seasonality usually means that the mean of the series changes periodically\nover time, with a \ufb01xed length periodicity. Trends can be twofold; 1) deterministic trends\n- change the mean of the series 2) stochastic trends (resulting from unit roots) - change\nboth the mean and variance of the series (Salles et al. 2019). Note that neither trend\n123",
        "a863ec4e-b18a-4e82-b78f-1cde864ab19e": "794 H. Hewamalage et al.\nFig. 3 Forecasts from different models on a series with unit root based non-stationarity, with stochastic\ntrends. In this example, we have a continuously increasing series (increasing mean) due to the unit root.\nThe ML models are built as autoregressive models without any pre- or post-processing, and as such have\nvery limited capacity to predict values beyond the domain of the training set, seen in the second part of the\ntest set where predictions are considerably worse than in the \ufb01rst part\nnor seasonality are concepts that have precise formal de\ufb01nitions. They are usually\nmerely de\ufb01ned as smoothed versions of the time series, where for the seasonality\nthe smoothing occurs over particular seasons (e.g., in a daily series, the series of all\nMondays needs to be smooth, etc.). Heteroscedasticity changes the variance of the\nseries and structural breaks can change the mean or other properties of the series.\nStructural break is a term used in Econometrics and Statistics in a time series context\nto describe a sudden change at a certain point in the series. It therewith has considerable\noverlap with the notion of sudden concept drift in an ML environment, where a sudden\nchange of the data distribution is observed (Webb et al. 2016).\nOn the other hand, data can be far from normality, for example having fat tails,\nor when conditions such as outliers or intermittency are observed in the series. Non-\nstationarities and non-normality are both seen quite commonly in many real-world\ntime series and the decisions taken during forecast evaluation depend on which of\nthese characteristics the series have. There is no single universal rule that applies to\nevery scenario.\n3 Motivation and common pitfalls\nAs brie\ufb02y explained in Sect. 1, there exist many ML based papers for forecasting in\nthe recent literature that are \ufb02awed or at least weak with regards to forecast evaluation.\nThis section is devoted to provide the motivation of our work by discussing the most\ncommon problems and pitfalls associated with forecast evaluation in many recent\nliterature.\n3.1 Benchmarks for forecast evaluation\nBenchmarks are an important part of forecast evaluation. Comparison against the\nright benchmarks and especially the simpler ones is essential. However, often in the\n123",
        "f946905a-75d5-445d-9c5a-749347fb8f48": "Forecast evaluation for... 795\nFig. 4 Different non-stationarities of series\nforecasting literature, newly proposed algorithms are not rigorously compared against\nthe relevant benchmarks.\n3.1.1 Na\u00efve benchmark on \ufb01nance series\nArguably the simplest benchmark that is commonly employed in forecasting is the\nna\u00efve forecast, also called persistence model or no-change model, that simply uses the\nlast known observation as the forecast. It has demonstrated competitive performance\nin many scenarios (Armstrong Jan 2001), especially on series that demonstrate random\nwalk properties. Equation ( 2) shows the de\ufb01nition of a random walk, where \u03f5\nt is white\nnoise; i.e. sampled from a normal distribution. Accordingly, the na\u00efve forecast at any\ntime step in the horizon can be de\ufb01ned as in Eq. ( 3). As the na\u00efve forecast is the\nlast known observation, the forecast is a shifted version of the time series where the\nforecast simply follows the actuals (see Fig. 5b).\ny\nt+1 = yt + \u03f5t (2)\n\u02c6yt+h = yt (3)\nFigure 5 illustrates the behaviour of different models that have been trained with\ndifferencing as appropriate preprocessing on a series that has a unit root based non-\nstationarity. If the series has no further predictable properties above the unit root (as\nin this example), i.e., it is a random walk where the innovation added to the last\nobservation follows a normal distribution with a mean of zero, the na\u00efve forecast is the\ntheoretically best forecast, as also suggested by the RMSE values reported in Table 1.\nOther, more complex forecasting methods in this scenario will have no true predictive\npower beyond the na\u00efve method, and any potential superiority, e.g., in error evaluations\n123",
        "29de542d-42d9-4fdd-9e3b-18dd08395d3e": "796 H. Hewamalage et al.\nTable 1 RMSE values of several\nML methods and the na\u00efve\nforecast on a random walk\nsimulated time series using\nrolling origin data partitioning\nModel RMSE\nRandom forest (RF) 1.01\nSupport vector machine 1.00\nNeural network 0.98\nNa\u00efve 0.96\nThe na\u00efve forecast is the theoretically best forecasting method here\nFig. 5 Forecasts from different models on a series with unit root based non-stationarity, with stochastic\ntrends. The ML models are built as autoregressive integrated models, i.e., differencing has been done as\npre-processing. The methods show very similar behaviour to the na\u00efve forecast, and do not add any value\nover it by de\ufb01nition of the Data Generating Process (DGP) used\nis by pure chance, and should be able to be identi\ufb01ed as a spurious result on suf\ufb01ciently\nlarge datasets.\nIn many practical applications, we \ufb01nd series that show strongly integrated\nbehaviour and therewith are close to random walks, as their innovations have high\ndegrees of noise (such as stock market data, exchange rate data, and to a lesser extent\nwind power, wind speed). Here, a na\u00efve forecast is a trivial yet competitive bench-\nmark and without comparing against it, quality of more complex models cannot be\n123",
        "1a6c2715-3c52-46a9-bd0d-f50d99cc767f": "Forecast evaluation for... 797\nmeaningfully assessed. More complex methods will in such series usually show a\nbehaviour where they mostly follow the series in the same way as the na\u00efve fore-\ncast, and improvements are often small percentages over the performance of the na\u00efve\nbenchmark.\nFinancial time series such as exchange rates and stock prices are particularly prob-\nlematic to forecast. For example, exchange rates are a function of current economic\nconditions and expectations about future valuations. Simultaneously, currencies are\ntraded on the futures market (e.g., a market participant says they will buy X amount\nof US dollars in 1 year price for Y amount of Australian dollars), providing a mar-\nket expectation of future price movements. The survey by Rossi ( 2013) has analysed\nthe literature on exchange rate forecasting based on additional economic information\nand concluded that the most challenging benchmark is the random walk without drift\nmodel. Yet, ML based researchers have continued to introduce sophisticated Neural\nNetwork (NN) models for exchange rate forecasting without proper benchmarking. In\nt h ew o r kb yW ue ta l .(2021), those authors have introduced a transformer based model\nwith an embedded decomposition block and an autocorrelation mechanism to address\nlong-term time series properties, called Autoformer. Their evaluation setup includes\nan exchange rate dataset used in many recent papers of this type (Lai et al. 2018), to be\nforecasted 720 days into the future. Predicting daily exchange rates based on only past\nexchange rates nearly 2 years into the future may sound like an outrageous claim to\nEconomists already, and those authors themselves state in that paper, that the dataset\ncontains no obvious periodicities and thus is hard to be predicted compared to other\ndatasets. It is thus unclear how the decomposition mechanism used in their proposed\nmodel should in any way make a valid contribution to predicting these series. As those\nauthors have not compared their model against the na\u00efve benchmark, we experiment\nusing a na\u00efve forecast on this exchange rate dataset, under the same evaluation setup\nas those authors. The results are as reported in Table 3. Table 3 reports the results\nfor Autoformer both from our experiments as well as the experiments reported in the\npaper. As seen here, the error values that we get for Autoformer are slightly different\nfrom the error values reported in the paper, due to the randomness of the seed values\nused in the experiments. Regardless, the na\u00efve forecast beats both the results from\nAutoformer across all the horizon sizes tested by a considerable margin, indicating\nthat the proposed method (and all comparison methods used in the original paper)\nis essentially useless on this particular dataset. Also keep in mind that in this exam-\nple Autoformer takes hours to run on CPU or alternatively needs a GPU with 24GB\nof memory, to \ufb01nally arrive at results that are worse than trivial results that require\nessentially no computation at all.\nMore recently, in the work by Zhou et al. ( 2022a), those authors have proposed\na Frequency improved Legendre Memory (FiLM) model which helps with removing\nnoisiness in signals and also preserves historical information for long-term forecasting.\nIn that paper too, those authors have experimented on the same exchange rate dataset.\nAccording to the results reported in that paper, that model outperforms the na\u00efve\nforecast on the longest horizon size of 720 on the multivariate forecasting study of the\nexchange rate dataset (the FiLM model has reported an MSE and MAE of 0 .727 and\n0.669, respectively, whereas the na\u00efve forecast has an MSE of 0 .817 and an MAE of\n0.694 as reported in Table 3). We have attempted to reproduce the same results of the\n123",
        "64f34f35-6d60-421f-8b41-579efa2d8d5d": "798 H. Hewamalage et al.\nTable 2 Results from several\nreruns of the FiLM model in the\nwork by Zhou et al. ( 2022a)\nTrial No. MSE MAE\n1 1.100 0.798\n2 1.302 0.869\n3 1.491 0.940\n4 1.218 0.841\n5 1.261 0.855\nTable 3 Results from the na\u00efve forecast and the Autoformer model on the exchange rate dataset\nHorizon Na\u00efve Autoformer (Rerun) Autoformer (Original Paper)\nMAE MSE MAE MSE MAE MSE\n96 0.192 0.078 0.279 0.149 0.323 0.197\n192 0.282 0.158 0.399 0.299 0.369 0.300\n336 0.388 0.287 0.504 0.460 0.524 0.509\n720 0.694 0.817 0.963 1.552 0.941 1.447\nBest models shown in boldface font\nFiLM model, to investigate the statistical signi\ufb01cance of the difference compared to\nthe na\u00efve forecast. However, using \ufb01ve trials we have been unable to reproduce the\nexact same results on the horizon size of 720, and all the results we have been able to\nobtain using the code published with the original paper show a performance inferior\nto the na\u00efve forecast. These results from the \ufb01ve trials are reported in Table 2.A l s o\nnote that within each trial, the experiment is repeated 5 times using random seeds\nand the mean of the metrics is reported to comply with what the authors have done\nin their work. As reported in Table 2, the inability of the FiLM model to consistently\noutperform the na\u00efve forecast indicates that the results reported in the paper are most\nlikely spurious and obtained randomly by chance.\nThere have been many other recent works both published in ML outlets or pub-\nlished on preprint servers alone which have followed similar approaches to introduce\nNN based algorithms for long-term forecasting and then tested them using the same\nexchange rate dataset but without comparisons against the na\u00efve benchmark and there-\nwith leading to the same problematic conclusions of superiority of the respective\nmethods, namely Zhou et al. ( 2022b); Challu et al. ( 2022); Du et al. ( 2022); Sun and\nBoning ( 2022); Woo et al. ( 2022); Zhou et al. ( 2022c); Li et al. ( 2022a); Shabani\net al. ( 2022). While it is good practice to follow a common setup for further research\nwhich allows for comparison against the previous state-of-the-art methods, if the orig-\ninal setup is \ufb02awed this means that all successors are \ufb02awed as well. As such, the\nbenchmarks and the error measure used play an important role in such a setting. For\ninstance, by using a relative error measure (detailed further in Sect. 4.2) that lets us\ndirectly compare against a simple benchmark such as the na\u00efve, we can be certain of\nthe competitiveness of the model against simple methods.\nApart from the failure to use the correct benchmarks as explained above, there are\nfurther issues associated with these exchange rate time series, that makes producing\n123",
        "caef3238-448b-4ff4-8d37-453505896e09": "Forecast evaluation for... 799\nforecasts for them a fundamentally \ufb02awed task. One issue is that exchange rate data\n(and in particular this dataset) is based on trading days, meaning that the time series\nthat all the aforementioned works have dealt with do not contain weekends and are not\nequally spaced, so that any comments on seasonality and cycle length in these papers\nare likely wrong. However, the most important point is that data is more than input into\nan algorithm. The large body of literature in economics and \ufb01nance over 50 years states\nthat it is not sensible to forecast exchange rate time series, as it violates the ef\ufb01cient\nmarket hypothesis (Fama 1970). The nature of a market is that the price re\ufb02ects all the\ninformation publicly available, and even if it does not do it for a short period (such as\nminutes or days; or milliseconds in high-frequency trading), and some investors enjoy\nextra information, they will act on it, and the market price will adapt. There is a known\npersistence in the return volatility of foreign exchange rate markets (Berger et al.\n2009). Still, there is no evidence that it is reasonable to assume to forecast exchange\nrates 720 days into the future. The \ufb01nal open question of forecasting these exchange\nrates completely left out by the aforementioned literature is, why we are forecasting\nexchange rate in the \ufb01rst place. Is the intention to trade on that information, or is it\nfor risk management? How does an error measure that translates to being more than\n50% of the time wrong lead to anything else than the bankruptcy of the user? Would\nthe authors themselves be satis\ufb01ed that their pension fund is using their own model\nfor investing their money? We guess it is fair to answer this with no.\nSimilar considerations hold for stock price forecasting. Some examples from the\nrecent ML literature in this area that benchmark on stock market related data without\ncomparisons against the na\u00efve benchmark are Shen et al. ( 2020); Du et al. ( 2021); Lin\net al. ( 2021). Stock market data is another classical example where data is abundant,\nbut stock returns are deemed to be \u201calmost unpredictable\u201d (Engle 2003), especially\nusing only past stock prices as inputs alone, in the classic Economics literature, as\nstock prices are again assumed to not be a function of their own past but of current\nmarket conditions and expectations about future valuations, and in an ef\ufb01cient mar-\nket, forecasting using only past stock price data will not yield results more accurate\nthan a na\u00efve forecast. It is important to note in this context that this holds for stock\nprices and returns, but not volatility, which is predictable, e.g., using autoregressive\nconditional heteroskedasticity (ARCH), a \ufb01nding which led to the award of the 2003\nNobel Memorial Prize in Economic Sciences to Robert F. Engle (Engle 2003).\nAs such, papers that claim that they can predict stock prices or returns, or exchange\nrates based on historic readings of these same signals alone need to be aware that\ntheir claims contradict some central notions in Economics and that they need to be\nevaluated very rigorously, as their results are likely to be spurious.\n3.1.2 Other simple forecasting benchmarks\nOn series that have clear seasonal patterns, models should accordingly be bench-\nmarked against the seasonal na\u00efve model as the most simplistic benchmark, and also\nother simple benchmarks are commonly used in forecasting. In the work by Zhou et al.\n(2021) those authors have proposed a novel memory and time ef\ufb01cient transformer\nbased architecture, namely Informer for long sequence forecasting. That paper has\nalso won the outstanding paper award at the Association for the Advancement of Arti-\n123",
        "52444999-e17f-42da-a188-08a10837125a": "800 H. Hewamalage et al.\nTable 4 Results of the\nDHR-ARIMA model along with\nInformer and the other\nbenchmarks on the univariate\nforecasting task in the work by\nZhou et al. ( 2021)\nModel ETTh 1 (720) ECL (960)\nMSE MAE MSE MAE\nInformer 0.269 0.435 0.582 0.608\nInformer\u2020 0.257 0.421 0.594 0.638\nLongTrans 0.273 0.463 0.624 0.645\nReformer 2.112 1.436 7.019 5.105\nLSTMa 0.683 0.768 1.545 1.006\nDeepAR 0.658 0.707 0.657 0.683\nARIMA 0.659 0.766 1.370 0.982\nProphet 2.735 3.253 6.901 4.264\nDHR-ARIMA 0.140 0.297 0.433 0.499\n\u2020 which uses a canonical self-attention mechanism\nBest models shown in boldface font\n\ufb01cial Intelligence (AAAI) conference 2021. In that work several experiments have\nbeen conducted using Electricity Transformer Temperature data (ETT), Electricity\nConsumption Load (ECL) 2 data and Weather data. The ETT and ECL hourly datasets\nclearly show strong multiple seasonal patterns (being hourly series, daily, weekly,\nand yearly patterns are to be expected). However, the Informer model has only been\nbenchmarked against non-seasonal ARIMA which is not capable of handling multiple\nseasonalities, and is a grotesquely misspeci\ufb01ed model that would not be used in prac-\ntice. To claim its superior performance in the long horizon forecasting problems, the\nproposed Informer model in this case needs to be compared against statistical standard\nbenchmarks that inherently handle multiple seasonalities well, such as the Dynamic\nHarmonic Regression ARIMA (DHR-ARIMA) model and the TBA TS model (Hynd-\nman and Athanasopoulos 2018). To demonstrate this, we conduct an experiment with\na DHR-ARIMA model on the ETTh\n1 and the ECL datasets on their respective longest\nhorizon sizes (720 for the ETTh 1 dataset and 960 for the ECL dataset) for the uni-\nvariate forecasting task. For the ETTh 1 dataset, daily and yearly seasonal patterns are\nincorporated where as for the ECL dataset, all daily, weekly and yearly seasonalities\nare included using Fourier terms in the DHR-ARIMA model. The results are reported\nin Table 4, along with the results for the benchmark models shown in the original\npaper. The horizon size is shown within parentheses next to the dataset name in Table\n4. As seen from these results, when the Fourier terms are incorporated to capture the\nmultiple seasonalities, the standard DHR-ARIMA can outperform ARIMA as well as\nthe two variants of the proposed algorithm, Informer and Informer\n\u2020.\nApart from that, the recent work by Zeng et al. ( 2022) has challenged the long-term\ntime series forecasting capability of transformer based models in general by comparing\nagainst a relatively simple linear layer based NN, i.e., a set of linear models trained\nfor the forecasting horizon in question directly. As those authors have stated in their\nwork, most of the performance gains of the aforementioned transformer based models\nfor long-term forecasting are due to comparing their direct multi-step ahead forecasts\nagainst iterative forecasts that are produced from more traditional methods, which\n2 https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014\n123",
        "22bb6e38-dee0-4121-9243-10ae0eac8f33": "Forecast evaluation for... 801\ninherently have error accumulation issues due to the recursive nature of forecasting.\nThis claim once again emphasises the need to perform comparisons with the right\nand the most competitive established forecasting benchmarks for the relevant study,\nas directly trained linear models have been shown to outperform all the considered\ntransformer architectures in that work.\n3.2 Datasets for empirical evaluations\nAnother common problem in the ML based forecasting literature is that many works\ndo not use suf\ufb01cient amounts of datasets/time series for the experiments for reason-\nably claiming the superior performance of the proposed algorithms. While it may be\nsomewhat subjective what amount of series is suf\ufb01cient, oftentimes papers use only\na handful of series when the authors clearly don\u2019t seem to care about their particu-\nlar application and/or when hundreds of series could be readily available for the same\napplication case, e.g., in notorious stock return prediction tasks. Some examples along\nthese lines (there are many more in the literature) are the works of Liu et al. ( 2021,\n2020); Godfrey and Gashler ( 2018); Shen et al. ( 2020), and Zhang et al. ( 2021). In\nparticular, Zhang et al. ( 2021) use 3 time series in total, a simulated AR(1) process, a\nbitcoin price series and an in\ufb02uenza-like illness series, to evaluate their non-parametric\nneural network method. While the in\ufb02uenza-like illness series may be a good fore-\ncasting case study, basically the same considerations as for exchange rates and stock\nprices hold for bitcoin prices, though bitcoin was presumably a less ef\ufb01cient market,\nespecially in its infancy. The best model to forecast an AR(1) process is trivially an\nAR(1) model (which is not used as a benchmark in that paper), so \ufb01tting complex neu-\nral networks to this series makes very limited sense.\n3 The authors are here effectively\n\ufb01tting a neural network to model a 2-dimensional linear relationship plus noise.\n3.3 Evaluation measures for forecasting\nA variety of evaluation measures have been proposed for forecast evaluation over the\nyears, and thus ML based forecasting researchers seem to be in a situation unable\nto clearly pick the evaluation measures that best suit their requirements and the data\nat hand. For example, in the work by Lai et al. ( 2018), those authors have used two\nmeasures Root Relative Squared Error (RSE) and Empirical Correlation Coef\ufb01cient\n(CORR) for the evaluation which both use scaling based on the mean of the time\nseries. While this may work as a scaling technique for time series that have minimal\nor no trends, for series that contain trend based non-stationarities this does not scale\nthe series meaningfully. Yet, this information is only implicit and not conveyed to the\nreader in their work. Consequently, there have been many other works which followed\nthe same evaluation setup and the measures without any attention to whether the used\n3 One could argue that not always the true data generating process (DGP) is the best forecasting model,\nbut this usually happens for complex DGPs where not enough data is available to estimate their parameters\ncorrectly and so simpler models perform better for forecasting. However, an AR(1) is already very simple\nso that compared to a considerably more complex neural network this consideration seems not relevant\nhere.\n123",
        "edcd5348-264d-4e12-a57e-76941a1e6529": "802 H. Hewamalage et al.\nseries contain trends or not (examples are Guo et al. 2022; Shih et al. 2019;W ue ta l .\n2020;Y ee ta l . 2022). Although this allows for direct comparisons against previous\nwork, it also has caused all successive works to overlook the same issues with trended\ntime series with the used error measures.\nSome works also use scale-dependent measures such as Mean Squared Error (MSE),\nRMSE and MAE on multivariate datasets having many time series (examples are Cui\net al. 2021;D ue ta l . 2021;Y ee ta l . 2022). While this is reasonable if all the series in\nthe dataset have similar scales, if the scales are different, this means that the overall\nerror value would be driven by particular series. Some have used the coef\ufb01cient of\ndetermination (R\n2) between the forecasts and the actual values as a forecast evaluation\nmeasure as well (for example Shen et al. 2020; Zhou et al. 2022). This can be a quite\nmisleading evaluation measure especially in the case of random walk time series,\nwhich may give almost perfect R\n2 values (close to 1) due to the following nature\nof the series indicating a competitive performance of the model, whereas in reality\nthe series does not have any predictable patterns at all. MAPE is another evaluation\nmeasure commonly applied incorrectly on series having very small values in the range\n[\u22121, 1] (examples are Moon et al. 2022;W ue ta l . 2020). Due to the denominator of\nthe MAPE which is the actual value of the time series, on series having values close\nto 0, MAPE gives excessively large values irrespective of the actual prediction.\n3.4 Forecast plots\nPlots with time series forecasting results can be quite misleading and should be used\nwith caution. Analysing plots of forecasts from different models along with the actuals\nand concluding that they seem to \ufb01t well can lead to wrong conclusions. It is important\nto use benchmarks and evaluation metrics that are right for the context. In a scenario\nlike a random walk series as in Fig. 5 as stated before, visually our models may look\nlike achieving similar or better accuracy than the na\u00efve method, but it will be a spurious\nresult. The visual appeal of a generated forecast or the possibility of such a forecast\nto happen in general are not good criteria to judge forecasts. However, many recent\nforecasting literature seem to use forecast plots that do not convey much information\nregarding the performance of the methods (for example Liu et al. 2021, 2020;D ue ta l .\n2021).\nFigure 6a shows a monthly time series with yearly seasonal patterns along with\nforecasts from the ETS model. The \ufb01gure furthermore shows the forecasts under\n\ufb01xed origin and rolling origin data partitioning schemes for the na\u00efve forecast. When\nperiodic re-\ufb01tting is done with new data coming in as in a rolling origin setup, the\nna\u00efve forecast gets continuously updated with the last observed value. For the \ufb01xed\norigin context on the other hand, the na\u00efve forecast remains constant as a straight line\ncorresponding to the last seen observation in the training series. We see that with a\nrolling-origin na\u00efve forecast, the predictions tend to look visually very appealing, as\nthe forecasts follow the actuals and our eyes are deceived by the smaller horizontal\ndistances instead of the vertical distances that are relevant for evaluation. Figure 6b\nillustrates this behaviour. It is clear how the horizontal distance between the actuals\nand the na\u00efve forecast at both points A and B are much less compared to the vertical\n123",
        "bcccdf0d-362f-4ca4-b561-6603c50784c3": "Forecast evaluation for... 803\nFig. 6 Properties of the na\u00efve forecast\nTable 5 RMSE values of rolling\norigin versus \ufb01xed origin na\u00efve\nforecasts and ETS forecasts on\nthe time series in Fig. 6a\nModel RMSE\nNa\u00efve (Rolling Origin) 31.23\nNa\u00efve (Fixed Origin) 37.10\nETS 29.93\ndistances which are the relevant ones for evaluation. In these situations we need to\nrely on the error measures, as the plots do not give us much information. As reported\nin Table 5, for this scenario the ETS forecasts have a smaller RMSE error compared\nto both rolling origin and \ufb01xed origin na\u00efve forecasts.\nFigure 7 shows another series having unit root based non-stationarity and \ufb01xed\norigin forecasts from several models and the na\u00efve forecast for a forecast horizon of\n60 time steps ahead. This shows another issue with using plots to determine forecast\naccuracy. As explained previously, on a random walk time series, a na\u00efve forecast is\nthe theoretically best forecast that can be obtained. This is also clari\ufb01ed by the RMSE\nvalues for these forecasts from the different models as reported in Table 6. However,\n123",
        "3ac305ee-5644-4643-b441-febcd5943a12": "804 H. Hewamalage et al.\nTable 6 RMSE values of several\nmethods and the na\u00efve forecast\non a unit root based time series\nu s i n g\ufb01 x e do r i g i nd a t a\npartitioning\nModel RMSE\nRF 3.18\nSupport V ector Machine 17.88\nNeural Network 35.22\nARIMA 6.44\nNa\u00efve 3.04\nFig. 7 Fixed origin forecasts from several models and the na\u00efve forecast on a random walk time series\nthe na\u00efve forecast for \ufb01xed origin is a constant. Although this does not look realistic,\nand in most application domains we can be certain that the actuals will not be constant,\npractitioners may mistakenly identify such behaviour as a potential problem with the\nmodels, where this forecast is indeed the best possible forecast in the sense that it\nminimizes the error based on the information available at present.\nIn summary, plots of the forecasts can be deceiving and should be used mostly for\nsanity checking. Decisions should mostly be made based on evaluations with error\nmeasures and not based on plots.\n3.5 Data leakage in forecast evaluation\nData leakage refers to the inadvertent use of data from the test set, or more generally\ndata not available during inference, while training a model. It is always a potential\nproblem in any ML task. For example, Kaufman et al. ( 2012) present an extensive\nreview on the concept of data leakage for data mining and potential ways to avoid it.\nA r n o t te ta l .(2019) discuss this in relation to the domain of \ufb01nance. Hannun et al.\n(2021) propose a technique based on Fisher information that can be used to detect data\nleakage of a model with respect to various subsets of the dataset. Brownlee ( 2020)\nalso provide a tutorial overview on data preparation for common ML applications\nwhile avoiding data leakage in the process. However, in forecasting data leakage can\nhappen easier and can be harder to avoid than in other ML tasks such as classi\ufb01ca-\ntion/regression.\n123",
        "efa8ef36-2ef9-448c-a362-238168acbd3c": "Forecast evaluation for... 805\nForecasting is usually performed in a self-supervised manner with rolling origin\nevaluations where periodic re-training of models is performed, and within this re-\ntraining, it is normal that data travels from the test to the training set as seen on Fig.\n2. As such, it is often dif\ufb01cult and not practical to separate training and evaluation\ncode bases. As such, we often have to trust the software provider that everything is\nimplemented correctly, and an external evaluation is dif\ufb01cult.\nAlso, more indirect forms of data leakage can happen in forecasting. In analogy\nto classi\ufb01cation/regression, where data leakage sometimes happens by normalising\ndata before partitioning for cross-validation, in forecasting, data leakage can happen\nby performing smoothing, decomposition (mode decomposition), normalisation etc.\nover the whole series before partitioning for training and testing. This can sometimes\nbe seen in ML based forecasting literature. In the work by Ran et al. ( 2023) those\nauthors perform Empirical Mode Decomposition (EMD) presumably over the whole\nseries. Zhou et al. ( 2022) perform normalisation of the time series presumably before\nthe train, test set split. Kuranga and Pillay ( 2022) also perform 0-1 normalisation of\nthe time series presumably before modelling. They have used two forecast horizons\non the same series (in a rolling origin fashion), yet there is no mention to performing\nthe normalisation twice to re\ufb02ect the training data in each case. Hence that could lead\nto data leakage too. To demonstrate the effect of leakage, we perform an experiment\non a random walk time series using a rolling origin setup. EMD is performed on\nthe series and the intrinsic mode functions are each modelled using Random Forest\n(RF) models and the residue modelled using an ARIMA model. In the data leakage\nscenario, EMD is performed on the whole series whereas in the no leakage scenario,\nEMD is performed iteratively for each new training set in the rolling origin setup.\nAs EMD has some low-frequent components, applying it over the full series, these\nlow-frequent components contain considerable information about the series\u2019 future.\nThe forecast horizon is set to 20 steps with 50 rolling origins constituting an overall\ntest set of 1000 steps. The results are reported in Table 7. As seen by the results, with\ndata leakage the model becomes the best model even outperforming the na\u00efve forecast\nwhich is the theoretically best forecast on a random walk series. The p-value indicates\nthe p-value from the Wilcoxon signed-rank test applied to measure the statistical\nsigni\ufb01cance of the differences of the two techniques (leakage vs. no leakage) against\nthe na\u00efve forecast. A p-value < 0.05 indicates that the method is signi\ufb01cantly different\nfrom the na\u00efve forecast. Therefore, from these results it is clear that although when\nno leakage is present in the method, the forecast is signi\ufb01cantly worse than the na\u00efve\nforecast, when leakage is incorporated, the method has a nearly signi\ufb01cant p-value\nrendering it better than the na\u00efve forecast. These forecasts are further visualized in\nthe plots in Fig. 8. Data leakage can happen even when extracting features such as\ntsfeatures (Hyndman et al. 2019), catch22 (Lubba et al. 2019) that are not\nconstant over time, to feed as inputs to the model. Thus, features can be extracted only\nfrom the training set data, and may need to be re-calculated either periodically or over\nthe speci\ufb01c input windows. However, this can be computationally expensive.\nAnother type of leakage especially when training global models that learn across\nseries, which is common practice nowadays for ML models, is when one series in\nthe dataset contains information about the future of another series. For example with\nan external shock like COVID-19 or a global economy collapse, all the series in the\n123",
        "5b5a558c-9dba-4932-be92-68645f47d82b": "806 H. Hewamalage et al.\nTable 7 RMSE values for the\nleakage and no leakage\nexperiments on a unit root based\ntime series\nModel RMSE p-value\nNa\u00efve 3.46 \u2013\nLeakage Model 3.12 0.067\nNo Leakage Model 5.65 1.85 x 10 \u22126\nThe p-values from the statistical tests of differences against the na\u00efve\nforecast are also reported\nFig. 8 Forecasts from a model with leakage and no leakage on a time series having unit root based non-\nstationarity\ndataset can be equally affected. Therefore, if the series in the dataset are not aligned\nand one series contains the future values with respect to another, when splitting the\ntraining region, future information can be already included within the training set.\nHowever, in real world application series are usually aligned so that this is not a big\nproblem. On the other hand, in a competition setup such as the M3 and M4 forecasting\ncompetitions (Makridakis and Hibon 2000; Makridakis et al. 2020), where the series\nare not aligned, this can easily happen (Talagala 2020).\nData leakage can also happen simply due to using the wrong forecast horizon. This\ncan happen by using data that in practice will become available later. For example,\nwe could build a one-day-ahead model, but use summary statistics over the whole\nday. This means that we cannot run the model until midnight, when we have all data\nfrom that day available. If the relevant people who use the forecasts work only from\n9am-5pm, it becomes effectively a same-day model. The other option is to set the day\nto start and end at 5pm everyday, but that may lead to other problems.\nIn conclusion, data leakage dangers are common in self-supervised forecasting\ntasks. It is important to avoid leakage problems 1) in rolling origin schemes by being\nable to verify and trust the implementation, as external evaluation can be dif\ufb01cult 2)\nduring preprocessing of the data (normalising, smoothing etc.) and extracting features\nsuch as tsfeatures by splitting the data into training and test sets beforehand 3)\nby making sure that within a set of series, one series does not contain in its training\nperiod potential information about the future of another series.\n123",
        "8db3973e-3e7f-4108-9ec0-75f0a500f07a": "Forecast evaluation for... 807\n4 Guidelines and best practices for forecast evaluation\nForecast model building and evaluation typically encompasses the following steps.\n\u2022 Data partitioning\n\u2022 Forecasting\n\u2022 Error Calculation\n\u2022 Error Measure Calculation\n\u2022 Statistical Tests for Signi\ufb01cance (optional)\nThe process of evaluation in a usual regression problem is quite straightforward.\nThe best model out of a pool of \ufb01tted models is selected based on the value of a \ufb01nal\nerror measure on the validation set. The relevant error measures used etc. are standard\nand established as best practices in these domains. However, when it comes to forecast\nevaluation, many different options are available for each of the aforementioned steps\nand no standards have been established thus far, and hence all the pitfalls in the\nliterature as explained in Sect. 3. Therefore, in this section we are presenting a set of best\npractices and guidelines for each of the aforementioned steps in forecast evaluation.\n4.1 Data partitioning\nIn the following we present the guidelines around data partitioning for forecast eval-\nuation.\n4.1.1 Fixed origin setup\nFixed origin setup is a faster and easier to implement evaluation setup. However, with a\nsingle series, the \ufb01xed origin setup only provides one forecast per each forecast step in\nthe horizon. According to Tashman ( 2000), a preferred characteristic of OOS forecast\nevaluation is to have suf\ufb01cient forecasts at each forecast step. Also, having multiple\nforecasts for the same forecast step allows to produce a forecast distribution per each\nstep for further analysis. Another requirement of OOS forecast evaluation is to make\nthe forecast error measures insensitive to speci\ufb01c phases of business (Tashman 2000).\nHowever, with a \ufb01xed origin setup, the errors may be the result of particular patterns\nonly observable in that particular region of the horizon (Tashman 2000). Therefore,\nthe following multi period evaluation setups are introduced as opposed to the \ufb01xed\norigin setup.\n4.1.2 Rolling origin, time series cross-validation and prequential evaluation setups\nArmstrong and Grohman ( 1972) are among the \ufb01rst researchers to give a descriptive\nexplanation of the rolling origin evaluation setup. Although the terms rolling origin\nsetup and tsCV are used interchangeably in the literature, in addition to the forecast\norigin rolling forward, tsCV also allows to skip origins, effectively rolling forward by\nmore than one step at a time (analogously to the difference between a leave-one-out\nCV and a k-fold CV).\n123",
        "fd82ca9e-5ccb-4ad1-823a-adfb3a87878a": "808 H. Hewamalage et al.\nFig. 9 Comparison of Expanding Window versus Rolling Window setups. The blue and orange points\nrepresent the training and test sets, respectively. The \ufb01gure on the left side shows the Expanding Window\nsetup where the training set keeps expanding. The \ufb01gure on the right shows the Rolling Window setup\nwhere the size of the training set keeps constant and the \ufb01rst point of the training set keeps rolling forward\nWith such multi period evaluations, each time the forecast origin updates, the model\nencounters new actual data. With new data becoming available, we have the options\nto \u2013 in the terminology of Tashman ( 2000) \u2013 either update the model (feed in new\ndata as inputs) or recalibrate it (re\ufb01t with new data). Although for some of the tradi-\ntional models such as ETS and ARIMA, the usual practice (and the implementation\nin the forecast package) in a rolling origin setup is to recalibrate the models, for\ngeneral ML models it is more common to mostly just accept new data as inputs and\nonly periodically retrain the model (updating). As ML methods tend to work better\nwith higher granularities, re-\ufb01tting is not an option (for example, a monthly series\npredicted with ETS vs. a 5-minutely series predicted with Light Gradient Boosting\nModels). Therefore, retraining as the most recent data becomes available happens in\nML methods mostly only when some sort of concept drift (change of the underlying\ndata generating process) is encountered (Webb et al. 2016).\nRolling origin evaluation can be conducted in two ways; 1) Expanding window\nsetup 2) Rolling window setup. Figure 9 illustrates the difference between the two\napproaches. The expanding window method is a good setup for small datasets/short\nseries (Bell and Smyl 2018). On the other hand, the rolling window setup removes\nthe oldest data from training as new data becomes available (Cerqueira et al. 2020).\nThis will not make a difference with forecasting techniques that only minimally attend\nto the distant past, such as ETS, but may be bene\ufb01cial with pure autoregressive ML\nmodels, that have no notion of time beyond the windows. A potential problem of the\nrolling origin setup is that the \ufb01rst folds may not have much data available. However,\nthe size of the \ufb01rst folds is not an issue when dealing with long series, thus making\nrolling origin setup a good choice with suf\ufb01cient amounts of data. On the other hand,\nwith short series it is also possible to perform a combination of the aforementioned\ntwo rolling origin setups where we start with an expanding window setup and then\nmove to a rolling window setup.\n123",
        "3247ab11-164b-4b40-91ea-1ace20cb9ce0": "Forecast evaluation for... 809\nFig. 10 Comparison of\nrandomized CV versus OOS\nevaluation. The blue and orange\ndots represent the training and\ntest sets, respectively. In the\nusual k-fold-CV setup the testing\ninstances are chosen randomly\nover the series. In OOS, the test\nset is always reserved from the\nend of the series\n4.1.3 (Randomized) Cross-validation\nThe aforementioned two techniques of data partitioning preserve the temporal order of\nthe time series when splitting and using the data. A common misconception is that this\nis always a necessity when dealing with time series. Another form of data partitioning\nis to use a common randomized CV scheme as \ufb01rst proposed by Stone ( 1974). This\nscheme is visualized in Fig. 10. Compared to the aforementioned validation schemes\nwhich preserve the temporal order of the data, this form of randomized CV strategy\ncan make ef\ufb01cient use of the data, since all the data is used for both model training\nas well as evaluation in iterations (Hastie et al. 2009). This helps to make a more\ninformed estimation about the generalisation error of the model.\nHowever, this form of random splitting of a time series does not preserve the\ntemporal order of the data, and is therefore oftentimes not used and seen as problematic.\nThe common points of criticism for this strategy are that, 1) it can make it dif\ufb01cult for\na model to capture serial correlation between data points (autocorrelation) properly, 2)\npotential non-stationarities in time series can cause problems (for example, depending\non the way that the data is partitioned, if all data from Sundays happen to be in the\ntest set but not the training set in a series with weekly seasonality, then the model\nwill not be able to produce accurate forecasts for Sundays since it has never seen data\nof Sundays before), 3) the training data contains future observations and the test set\ncontains past data due to the random splitting and 4) since evaluation data is reserved\nrandomly across the series, the forecasting problem shifts to a missing value imputation\nproblem which certain time series models are not capable of handling (Petropoulos\n2022).\nDespite these problems, randomized CV can be applied to pure AR models without\nserial correlation issues. Bergmeir et al. ( 2018) theoretically and empirically show that\nCV performs well in a pure AR setup, as long as the models nest or approximate the\ntrue model, as then the errors are uncorrelated, leaving no dependency between the\nindividual windows. To check this, it is important to estimate the serial correlation of\nresiduals. For this, the Ljung-Box test (Ljung and Box 1978) can be used on the OOS\nresiduals of the models. While for over\ufb01tting models there will be no autocorrelation\nleft in the residuals, if the models are under\ufb01tted, some autocorrelation will be left\nin the OOS residuals. If there is autocorrelation left, then the model still does not\nuse all the information available in the data, which means there will be dependencies\nbetween the separate windows. In such a scenario, CV of the time series dataset will\nnot hold valid, and underestimate the true generalisation error. The existence of signif-\nicant autocorrelations anyway means that the model should be improved to do better\n123",
        "0365e678-2851-43a1-b33e-a28600de4705": "810 H. Hewamalage et al.\non the respective series (increase the AR order to capture autocorrelation etc.), since\nthe model has not captured all the available information. Once the models are suf\ufb01-\nciently competent in capturing the patterns of the series, for pure AR setups (without\nexogenous variables), standard k-fold CV is a valid strategy. Therefore, in situations\nwith short series and small amounts of training data, where it is not practically feasible\nto apply the aforementioned tsCV techniques due to the initial folds involving very\nsmall lengths of the series, the standard CV method with some control of under\ufb01tting\nof the models is a better choice with ef\ufb01cient use of data.\nThe aforementioned problem that the testing windows can contain future obser-\nvations, is also addressed by Bergmeir et al. ( 2018). With the CV strategy, the past\nobservations not in the training data but existing in the test set can be considered\nmissing observations, and the task is seen more as a missing value imputation prob-\nlem rather than a forecasting problem. Many forecasting models such as ETS (in its\nimplementation in the forecast package (Hyndman and Athanasopoulos 2018)),\nwhich iterate throughout the whole series, cannot properly deal with missing data. For\nRecurrent Neural Networks (RNN) as well, due to their internal states that are prop-\nagated forward along the series, standard k-fold CV which partitions data randomly\nacross the series is usually not applicable. Therefore, for such models, the only feasible\nvalidation strategy is tsCV . Models such as ETS can anyway train competitively with\nminimal amounts of data (as is the case with the initial folds of the tsCV technique)\nand thus, are not quite problematic with tsCV . However, for reasonably trained pure\nAR models, where the forecasts for one window do not in any way depend on the\ninformation from other windows (due to not under\ufb01tting and having no internal state),\nit does not make a difference between \ufb01lling the missing values in the middle of the\nseries and predicting future values, where both are performed OOS. Nevertheless, the\n\ufb01ndings by Bergmeir et al. ( 2018) are restricted to only stationary series.\n4.1.4 Data partitioning for non-stationary data\nCerqueira et al. ( 2020) experimented using non-stationary series, where they have\nconcluded that OOS validation procedures preserving the temporal order (such as\ntsCV), are the right choice when non-stationarities exist in the series. However, a pos-\nsible criticism of that work is the choice of models. We have seen in Sect. 3 that ML\nmodels are oftentimes not able to address certain types of non-stationarities out of the\nbox. More generally speaking, ML models are non-parametric, data-driven models.\nAs such, the models are typically very \ufb02exible and the function \ufb01tted depends heavily\non the characteristics of the observed data. Though recently challenged (Balestriero\net al. 2021), a common notion is that ML models are typically good at interpolation\nand lack extrapolation capabilities. The models used by Cerqueira et al. ( 2020) include\nseveral ML models such as a Rule-based Regression (RBR) model, a RF model and\na Generalized Linear Model (GLM), without in any way explicitly tackling the non-\nstationarity in the data (similar to our example in Sect. 3). Thus, if a model is poor\nand not producing good forecasts, performing a validation to select hyperparameters,\nusing any of the aforementioned CV strategies, will be of limited value. Furthermore,\nand more importantly, non-stationarity is a broad concept and both for the modelling\nand the evaluation it will depend on the type of non-stationarity which procedures will\n123",
        "99eb5b42-3cce-4750-bc6c-aa72a7017d2e": "Forecast evaluation for... 811\nperform well. For example, with abrupt structural breaks and level shifts occurring in\nthe unknown future, but not in the training and test set, it will be impossible for the\nmodels to address this change and none of the aforementioned evaluation strategies\nwould do so either. In this situation, even tsCV would grossly underestimate the gen-\neralisation error. For a more gradual underlying change of the DGP , a validation set\nat the end of the series would be more appropriate since in that case, the data points\ncloser to the end of the series may be already undergoing the change of the distribu-\ntion. On the other hand, if the series has deterministic trend or seasonality, which are\nstraightforward to forecast, they can be simply extracted from the series and predicted\nseparately whereas the stationary remainder can be handled using the model. In such\na setup, the k-fold CV scheme will work well for the model, since the remainder\ncomplies with the stationarity condition. For other non-deterministic trends, there are\nseveral data pre-processing steps mentioned in the literature such as lag-1 differencing,\nlogarithmic transformation (for exponential trends), Seasonal and Trend Decompo-\nsition using Loess (STL Decomposition), local window normalisation (Hewamalage\net al. 2021), moving average smoothing, percentage change transform, wavelet trans-\nform etc. (Salles et al. 2019). The \ufb01ndings of Salles et al. ( 2019) have concluded\nthat there is no single universally best transformation technique across all datasets;\nrather it depends on the characteristics of the individual datasets. If appropriate data\npre-processing steps are applied to enable models to handle non-stationarities, with a\npure AR setup, the CV strategy still holds valid after the data transformation, if the\ntransformation achieves stationarity. As such, to conclude, for non-stationarities, tsCV\nseems the most adequate as it preserves the temporal order in the data. However, there\nare situations where also tsCV will be misleading, and the forecasting practitioner will\nalready for the modelling need to attempt to understand the type of non-stationarity\nthey are dealing with. This information can subsequently be used for evaluation, which\nmay render CV methods for stationary data applicable after transformations of the data\nto make them stationary.\n4.1.5 Summary and guidelines for data partitioning\nIt is important to identify which out of the above data partitioning strategies most\nclosely estimates (without under/overestimation) the \ufb01nal error of a model for the\ntest set under the given scenario (subject to different non-stationarities/serial corre-\nlations/amount of data of the given time series). The gist of the guidelines for data\npartitioning is visualized by the \ufb02ow chart in Fig. 11. If the series are not short, tsCV is\nusually preferrable over k-fold CV , if there are no practical considerations such as that\nan implementation of an algorithm is used that is not primarily intended for time series\nforecasting, and that internally performs a certain type of cross-validation. If series\nare short, then k-fold CV should be used, accounting adequately for non-stationarities\nand autocorrelation in the residuals.\n123",
        "79839053-88ca-4238-b9db-2bb50f66f370": "812 H. Hewamalage et al.\nFig. 11 Flowchart for guidelines on data partitioning\n4.2 Error measures for forecast evaluation\nOnce the predictions are obtained from models, the next requirement is to compute\nerrors of the predictions to assess the model performance. Bias in predictions is a\ncommon issue and because of this, a model can be very accurate (forecasts being very\nclose to actuals), but consistently produce more overestimations than underestima-\ntions, which may be concerning from a business perspective. Therefore, forecast bias\nis calculated with a sign, as opposed to absolute errors, so that it indicates the direc-\ntion of the forecast errors, either positive or negative. For example, scale-dependent\nforecast bias can be assessed with the Mean Error (ME) as de\ufb01ned in Equation 4.\nHere, y\nt indicates the true value of the series, \u02c6yt the forecast and n, the number of\nall available errors. Other scale-free versions of bias can be de\ufb01ned by scaling with\nrespect to appropriate scaling factors, such as actual values of the series.\nME = 1\nn\nn\u2211\nt=1\n(\nyt \u2212\u02c6yt\n)\n(4)\nTwo other popular and simple error measures used in a usual regression context\nare MSE and MAE, which are both scale-dependent measures. Depending on the\nbusiness context, it can be a valid objective to forecast more accurately the series\nthat have higher scales, since they may be really the objects of interest. However, the\nproblem with scale-dependent measures is that, as soon as the scale of the series is\nchanged (for example converting from one currency to another), the value of the error\nmeasures change (Tashman 2000). On the other hand, for certain businesses, it is a\nrequirement to compare errors across series. For example, if we say that MAE is 10\nfor a particular series, we have no idea whether it is a good or a bad accuracy. For\na series with an average value of 1000, this amount of accuracy is presumably quite\ngood, whereas for another series with an average value of 1, it is a very bad accuracy.\nFor this reason, the measures need to be scaled to achieve scale-independent measures,\nand it has turned out to be next to impossible to develop a scaling procedure that works\nfor any type of possible non-stationarity and non-normality in a time series. Hence,\na wide variety of error measures have been proposed by researchers for this purpose\nover the years. Nevertheless, eventually we encounter a particular condition of the time\nseries in the real world, that makes the proposed error measure fail (Svetunkov 2021).\n123",
        "f87cb1d9-837a-4ae4-8974-919ad2d096d4": "Forecast evaluation for... 813\nThere are many options available for scaling such as per-step, per-series or per-dataset\nscaling. Scaling can also be done by dividing either by in-sample or OOS values of\nthe time series. Apart from dividing by certain quantities, scaling can also be achieved\nthrough log transformation of errors and ranking based on errors as well. The key to\nselecting a particular error measure for forecast evaluation is that it is mathematically\nand practically robust under the given data.\nDifferent point forecast evaluation measures are targeted towards optimizing for a\nspeci\ufb01c statistic of the distribution. For example, measures with squared base errors\nsuch as MSE and RMSE optimize for the mean whereas others with absolute value\nbase errors such as MAE and Mean Absolute Scaled Error (MASE) optimize for the\nmedian. Although the mean and median are the same for a symmetric distribution, that\ndoes not hold for skewed distributions as with intermittent series. There exist numerous\ncontroversies in the literature regarding this. Petropoulos ( 2022) suggest that it is not\nappropriate to evaluate the same forecasts using many different error measures, since\neach one optimizes for a different statistic of the distribution. Also according to Kolassa\n(2020), if different point forecast evaluation measures are considered, multiple point\nforecasts for each series and time point also need to be created. Kolassa ( 2020) further\nargues that, if the ultimate evaluation measure is, e.g., MAE which focuses on the\nmedian of the distribution, it does not make sense to optimize the models using an\nerror measure like MSE (which accounts for the mean). It is more meaningful to\nconsider MAE also during model training as well. However, these arguments hold\nonly if it is not an application requirement for the same forecasts to perform generally\nwell under all these measures. Koutsandreas et al. ( 2021) have empirically shown\nthat, when the sample size is large, a wide variety of error measures agree on the most\nconsistently dominating methods as the best methods for that scenario. They have\nalso demonstrated that using two different error measures for optimizing and \ufb01nal\nevaluation has an insigni\ufb01cant impact on the \ufb01nal accuracy of the models. Berm\u00fadez\net al. ( 2006) have developed a fuzzy ETS model optimized via a multi-objective\nfunction combining three error measures MAPE, RMSE and MAE. Empirical results\nhave demonstrated that using such a mix of error measures instead of just one for the\nloss function leads to overall better, robust and generalisable results even when the\n\ufb01nal evaluation is performed with just one of those measures. Fry and Lichtendahl\n(2020) also assess their same forecasts across numerous error measures in a business\ncontext. Evaluating the same forecasts with respect to many evaluation measures is a\nform of sanity checking to ensure that even under other measures (though not directly\noptimizing for them), the forecasts still perform well.\nThere are many different point forecast error measures available in the forecasting\nliterature categorized based on 1) whether squared or absolute errors are used 2)\ntechniques used to make them scale-free and 3) the operator such as mean, median\nused to summarize the errors (Koutsandreas et al. 2021). Also, there are different forms\nof base errors involved with each of the error measures. In the following base error\nde\ufb01nitions, y\nt indicates the true value of the series, \u02c6yt the forecast and T , the number\nof time steps in the training region of the time series.\n123",
        "d721139a-015e-4525-96ba-0f83735bf82f": "814 H. Hewamalage et al.\n\u2022 Scale-dependent base error\net = yt \u2212\u02c6yt (5)\n\u2022 Percentage error\npt = 100et\nyt\n(6)\n\u2022 Percentage error (In-sample scaling) - Named as scaled Error (sE) in the work of\nPetropoulos and Kourentzes ( 2015).\np\u2020\nt = et\n1\nT\n\u2211 T\nt=1 yt\n(7)\n\u2022 Percentage absolute error (In-sample scaling) - Named as scaled Absolute Error\n(sAE) in the work of Petropoulos and Kourentzes ( 2015).\np\u2021\nt = |et |\n1\nT\n\u2211 T\nt=1 yt\n(8)\n\u2022 Relative error - eb\nt in Eq. ( 9) is the scale-dependent base error of the benchmark\nmethod.\nrt = et\neb\nt\n(9)\n\u2022 Scaled error (using MAE for the benchmark)\nqt = et\n1\nT \u22121\n\u2211 T\nt=2 |yt \u2212 yt\u22121|\n(10)\n\u2022 Scaled error (using MSE for the benchmark)\nq\u2020\nt = e2\nt\n1\nT \u22121\n\u2211 T\nt=2(yt \u2212 yt\u22121)2 (11)\n\u2022 Logarithmic error - ln in Eq. ( 12) de\ufb01nes the natural logarithm.\nlt = ln (yt + 1) \u2212 ln (\u02c6yt + 1) (12)\nThis is mathematically equivalent to the following.\nlt = ln\n( yt + 1\n\u02c6yt + 1\n)\n(13)\n123",
        "9a2fd43a-b480-4239-bd7c-48c295e74f19": "Forecast evaluation for... 815\n\u2022 Rate-based error (Kourentzes 2014)\nct =\u02c6yt \u2212 1\nt\nt\u2211\ni=1\nyi (14)\nTable 8 contains the de\ufb01nitions of error measures proposed in the literature using\nthe aforementioned base errors. In the de\ufb01nitions of Table 8, n indicates the number of\nall available base errors, m denotes the number of time series, h indicates the number\nof time steps in the forecast horizon and hi , the horizon size for the ith series.\nDepending on each of the characteristics of time series as also stated in Sect. 2,\ndifferent error measures de\ufb01ned in Table 8 are preferable or should be avoided in each\ncase. Table 9 summarises this information and can be used to choose error measures\nunder given characteristics of the data. In Table 9, the scaling column indicates the type\nof scaling associated with each error measure mentioned in the previous column. This\nincludes no scaling, scaling based on actual values, scaling based on benchmark errors\nas well as the categorisation such as per-step, per-series and all-series (per-dataset)\nscaling. The \u2020 sign in Table 9 indicates that the respective error measures need to be\nused with caution under the given circumstances.\nIn almost any scenario, when applying error measures that scale based on errors\nfrom a benchmark method, the relative competence of the benchmark method in the\nintended forecast horizon needs to be taken into account, since otherwise benchmark\nerrors can unnecessarily drive the overall error measure values higher or lower. With\nseries having seasonality, percentage based measures may underestimate the errors\nat peaks heavily, due to dividing by large actual values (Wong 2019; Kunst 2016)o r\noverstate the errors at troughs. This can be overcome by scaling based on aggregated\nvalues (per series, all-series). On series having trends or structural breaks with level\nshifts, scale-free measures which compute their scale by aggregating the values (actual\nvalues or benchmark errors) at several time steps, tend to face problems. This is as\nexplained by Chen et al. ( 2017), that the error values at each time step need to comply\nwith the scale of the series at each point. A scale computed by aggregating over\nseveral time steps which include such level shifts may not always be a good estimator\nto represent the scaling factors for all the time steps of such a series. Also on series\nwith exponential trends, log transformation based error measures greatly reduce the\nimpact of errors from models. Unit roots are very similar to trends except that measures\nwhich compute a per-step scaling may not capture peak points on such series similar\nto seasonal series.\nSimilarly, on series having heteroscedasticity too, due to potential peaks and troughs\nin the series which may have very high and low variances, measures such as MAPE\nand RMSPE may have problems with capturing those points correctly. Apart from\nthat, log transformation based errors can reduce the impact from heteroscedasticity as\nwell. Especially on series having structural breaks, with measures which scale based\non benchmark errors, when those errors are computed in-sample, they may not be\nrepresentative of the errors that happen OOS when the structural breaks are either\nin the forecast horizon or the forecast origin. On intermittent series, measures that\noptimize for the median are problematic since they consider constant zeros as the\n123",
        "5eb7d61e-e0a8-439a-aa78-c1dcc8d4c41a": "816 H. Hewamalage et al.\nTable 8 Error measure de\ufb01nitions in the forecasting literature\nCategory Error measure De\ufb01nition\nScale-Dependent\nMeasures\nRoot Mean Squared Error (RMSE) RMSE =\n\ued6a\ued6b\ued6b\u221a 1\nn\nn\u2211\nt=1\n(e2t )\nRoot Median Squared Error (RMdSE) RMdSE =\n\u221a\nmedian(e2t )\nMedian Absolute Error (MdAE) MdAE = median(|et |)\nGeometric Root Mean Squared Error (GRMSE, Syntetos\nand Boylan 2005)\nGRMSE = 2n\n\ued6a\ued6b\ued6b\u221a\nn\u220f\nt=1\ne2t\nGeometric Mean Absolute Error (GMAE) GMAE = n\n\ued6a\ued6b\ued6b\n\u221a\nn\u220f\nt=1\n|et |\nMeasures based\non Percentage\nErrors\nMean Absolute Percentage Error (MAPE) MAPE = 1\nn\nn\u2211\nt=1\n(|pt |)\nMedian Absolute Percentage Error (MdAPE) MdAPE = median(|pt |)\nRoot Mean Square Percentage Error (RMSPE, Bojer and\nMeldgaard 2020)\nRMSPE =\n\ued6a\ued6b\ued6b\u221a 1\nn\nn\u2211\nt=1\n(p2t )\nRoot Median Square Percentage Error (RMdSPE) RMdSPE =\n\u221a\nmedian(p2t )\nSymmetric Mean Absolute Percentage Error (sMAPE \ufb01rst\nproposed by Makridakis ( 1993))\nsMAPE = 1\nn\nn\u2211\nt=1\n( 200|et |\n|yt |+|\u02c6 yt |)\nSymmetric Median Absolute Percentage Error (sMdAPE) sMdAPE = median( 200|et |\n|yt |+|\u02c6 yt |)\nModi\ufb01ed Symmetric Mean Absolute Percentage\nError (msMAPE, Suilin 2017)\nmsMAPE = 1\nn\nn\u2211\nt=1\n200|et |\nmax(|yt |+|\u02c6 yt |+ \u03f5, 0.5 + \u03f5) ,w h e r e\n\u03f5 = 0.1 by default\n123",
        "7225ed0e-e76b-4da3-8d29-35958b732063": "Forecast evaluation for... 817Table 8 continued\nCategory Error measure De\ufb01nition\nMean Arctangent Absolute Percentage Error (MAAPE,\nKim and Kim 2016)\nMAAPE = 1\nn\nn\u2211\nt=1\narctan(| et\nyt\n|)\nWeighted Absolute Percentage Error (W APE) WAPE =\n\u2211 T +h\nt=T +1 |et |\n\u2211 T +h\nt=T +1 |yt |\nSymmetric Weighted Absolute Percentage Error (sW APE) sWAPE =\n\u2211 T +h\nt=T +1 |et |\n\u2211 T +h\nt=T +1 |yt |+|\u02c6 yt |\nWeighted Root Mean Squared Percentage Error (WRMSPE) WRMSPE =\n\ued6a\ued6b\ued6b\u221a\n1\nh\n\u2211 T +h\nt=T +1 e2t\n1\nh\n\u2211 T +h\nt=T +1 |yt |\nRelative Total Absolute Error (RTAE) RTAE =\n1\nh\n\u2211 T +h\nt=T +1 |et |\nmax (C, 1\nh\n\u2211 T +h\nt=T +1 |yt |)\n,w h e r eC refers to a\nregularisation threshold\nScaled Mean Error (sME, Petropoulos and Kourentzes\n2015)\nsME = 1\nn\nn\u2211\nt=1\n(p\u2020\nt )\nScaled Mean Squared Error (sMSE, Petropoulos and\nKourentzes 2015)\nsMSE = 1\nn\nn\u2211\nt=1\n(p\u2020\nt\n2\n)\nScaled Mean Absolute Error (sMAE, Petropoulos and\nKourentzes 2015)\nsMAE = 1\nn\nn\u2211\nt=1\n(p\u2021\nt )\nNormalized Deviation (ND, Salinas et al. 2020) - The scale\nin the denominator is computed globally using many\nseries.\nND =\n\u2211\nn\nt=1 |et |\u2211 n\nt=1 |yt |\n123",
        "f4df1c68-81c9-404a-bdcb-44c4fadcd6a3": "818 H. Hewamalage et al.\nTable 8 continued\nCategory Error measure De\ufb01nition\nNormalized Root Mean Squared Error (NRMSE, Salinas\net al. 2020) - The scale in the denominator is computed\nglobally using many series.\nNRMSE =\n\u221a\n1\nn\n\u2211 n\nt=1(e2t )\n1\nn\n\u2211 n\nt=1(|yt |)\nMeasures based\non Relative\nErrors\nMean Relative Absolute Error (MRAE) MRAE = 1\nn\nn\u2211\nt=1\n(|rt |)\nMedian Relative Absolute Error (MdRAE) MdRAE = median(|rt |)\nRoot Mean Relative Squared Errors (RMRSE) RMRSE =\n\ued6a\ued6b\ued6b\u221a 1\nn\nn\u2211\nt=1\n(r2t )\nGeometric Mean Relative Absoluate Error (GMRAE) GMRAE = n\n\ued6a\ued6b\ued6b\n\u221a\nn\u220f\nt=1\n|rt |\nRelative Geometric Root Mean Squared Error (RGRMSE) RGRMSE = 2n\n\ued6a\ued6b\ued6b\u221a\nn\u220f\nt=1\nr2t\nRelative Measures Relative Mean Absolute Error (RelMAE) RelMAE = MAE\nMAEb\n,w h e r eMAEb is the MAE of the\nbenchmark method\nRelative Mean Squared Error (RelMSE) RelMSE = MSE\nMSEb\n,w h e r eMSEb is the MSE of the\nbenchmark method\n123",
        "c75478e5-0f6c-4c8c-b5e9-7bd1b56329af": "Forecast evaluation for... 819Table 8 continued\nCategory Error measure De\ufb01nition\nRelative Root Mean Squared Error (RelRMSE) RelRMSE =\n\u221a\nMSE\nMSEb\n,w h e r eMSEb is the MSE of the\nbenchmark method\nRoot Relative Squared Error (RSE, Lai et al. 2018) RSE =\n\u221a \u2211 n\nt=1 e2t\u221a \u2211 n\nt=1(yt \u2212\u00afy)2\nAverage Relative Mean Absolute Error (AvgRelMAE,\nDavydenko and Fildes 2013)\nAvgRelMAE =\n\u239b\n\u239d\nm\u220f\ni=1\n(\nMAEi\nMAEi\nb\n) hi\n\u239e\n\u23a0\n1\u2211 m\ni=1 hi\n,w h e r eMAEi\nb\nis the MAE of the benchmark method for the ith series\nMeasures based\non Scaled\nErrors (Hynd-\nman and\nKoehler 2006)\nMean Absolute Scaled Error (MASE) MASE = 1\nn\nn\u2211\nt=1\nqt\nMedian Absolute Scaled Error (MdASE) MdASE = median(qt )\nRoot Mean Squared Scaled Error (RMSSE, Makridakis\net al. 2022)\nRMSSE =\n\ued6a\ued6b\ued6b\n\u221a\n1\nn\nn\u2211\nt=1\nq\u2020\nt\n123",
        "a3a39d85-0be5-4dc1-b690-0bb25d32fb69": "820 H. Hewamalage et al.\nTable 8 continued\nCategory Error measure De\ufb01nition\nMeasures based\non\nRanks/Counting\nPercentage Better (PB Score, Hyndman and Koehler 2006)\n- Counts how many times (across series and time steps) a\ngiven method is better than the benchmark and reports it\nas a percentage.\nPB(MAE) = 100 mean(I {MAE < MAEb}),w h e r eMAEb is\nthe MAE of the benchmark method.\nPercentage of Critical Event for Margin X - Wong ( 2019)\nproposed this to measure the percentage of forecasts\nwhere the value of error is higher than a margin.\n100 mean(I {E > X}),w h e r eE is the error and X is the\nmargin\nMeasures based\non\nTransformation\nRoot Mean Squared Logarithmic Error (RMSLE, Bojer and\nMeldgaard 2020)\nRMSLE =\n\ued6a\ued6b\ued6b\u221a\n1\nn\nn\u2211\nt=1\nlt\nNormalized Weighted Root Mean Squared Logarithmic\nError (NWRMSLE, Bojer and Meldgaard 2020)\nNWRMSLE =\n\u221a \u2211 n\nt=1 wt l2t\u2211 n\nt=1 wt\n,w h e r ewt is a weight\nassigned to the error at time step t\nRate-based Mea-\nsures (Kourentzes\n2014)\nMean Squared Rate (MSR) MSR =\nn\u2211\nt=1\nc2\nt\nMean Absolute Rate (MAR) MAR =\nn\u2211\nt=1\n|ct |\nOther Error\nMeasures\nWeighted Mean Absolute Error (WMAE, Bojer and\nMeldgaard 2020)\nWMAE =\n\u2211 n\nt=1 wt |et |\u2211 n\nt=1 wt\n,w h e r ewt is a weight assigned to\nthe error at time step t\nEmpirical Correlation Coef\ufb01cient (CORR, Lai et al. 2018) CORR = 1\nm\nm\u2211\ni=1\n(\n\u2211 T +h\nt=T +1(yit \u2212\u00afyi )(\u02c6yit \u2212 \u00af\u02c6yi )\n\u221a \u2211 T +h\nt=T +1(yit \u2212\u00afyi )2(\u02c6yit \u2212 \u00af\u02c6yi )2\n),\nwhere \u00afyi is the mean of series i and \u00af\u02c6yi is the mean of the\npredictions for series i\n123",
        "33014d2f-841f-45f7-b449-dfeb053306b9": "Forecast evaluation for... 821\nbest prediction. Measures with per-step scaling based on actual values can also be\nproblematic on intermittent series due to dividing by zero. This can be addressed by\nusing per-series scaling, but can again have issues if all time steps have zero values.\nWith measures that scale based on benchmark errors on intermittent series, it can be\nproblematic when benchmark errors have prefect predictions (zero errors), for example\nwith the na\u00efve method giving exact zeros on zero actual values. With respect to outliers,\nsome applications may be interested in capturing them whereas others may want to\nbe robust against them. To be robust against outliers, geometric mean or median can\nbe used as the summary operator instead of the mean. Absolute base errors need to\nbe used instead of squared base errors to be robust against outliers. Measures which\nscale based on per-step or per-series quantities may be heavily affected by outliers.\nSimilarly, with measures that scale based on benchmark errors, if the forecast of the\nbenchmark in the horizon is heavily affected by the outliers in the training region of\nthe series, it can be problematic.\nThe \ufb02ow chart in Fig. 12 provides further support for forecast evaluation measure\nselection based on user requirements and other characteristics in the data. In Fig. 12,\nthe error measures selected to be used with outlier time series are in the context of\nbeing robust against outliers, not capturing them.\n4.3 Statistical tests for significance\nWhile forecast evaluation measures are critical to see the relative performance of the\nmethods and select the best ones from their rankings, they do not give information\nregarding the statistical signi\ufb01cance of the differences between these methods; i.e.\nwhether better performance of the best method is just by chance on this sample of\nthe series or whether it is likely to dominate all the methods signi\ufb01cantly in other\nsamples of the data. The selected best method could be the only one to use, or there\ncould be other methods that are not signi\ufb01cantly different from the best that can\nbe used interchangeably due to their other preferable properties such as simplicity,\ncomputational ef\ufb01ciency etc.\nThere are many ways of performing statistical signi\ufb01cance tests reported in the\nliterature. The Diebold-Mariano test (Diebold and Mariano 2002) and the Wilcoxon\nrank-sum test (Mann and Whitney 1947) are both designed for comparing only between\ntwo competing forecasts, not necessarily methods or models. However, the Diebold-\nMariano test is designed speci\ufb01cally for time series and parametric, meaning that it\nhas the assumption of normality of the data whereas the Wilcoxon test is a generic\nnon-parametric test based on the ranks of the methods. Due to considering ranks of\nmethods for each series separately, the error measures used do not necessarily have to\nbe scale-free. The Giacomini-White test (Giacomini and White 2006) again is based on\nthe comparison of two forecasts, with the potential to assess the conditional predictive\nability (CPA), a concept that refers to conditioning the choice of a potential future\nstate of the economy, an important concept for macro economic forecasting of a small\nnumber of series. A continuation in this line of research is work by Li et al. ( 2022b) that\nfocuses on conditional superior predictive ability, in regards to a benchmark method\nand time series with general serial dependence. It should be noted that many of the\n123",
        "f5344be8-dfa6-4c72-8979-87d2e7a6f4c9": "822 H. Hewamalage et al.\nTable 9 Checklist for selecting error measures for \ufb01nal forecast evaluation based on different time series characteristics\nStationary\nCount )\nData(>> 0\nSeasonality Trend\n(Linear/\nExp.)\nUnit Roots Heteroscedasticity Structural Breaks (With Scale Differences) Intermittence Outliers Error\nMeasures\nScaling\nForecast\nHorizon\nTraining\nRegion\nForecast\nOrigin\n\u2713 \u2713 \u2713\u2713\u2713 \u2713\u2713\u2713 \u2713 \u2717 RMSE None\n\u2713 \u2713 \u2713\u2713\u2713 \u2713\u2713\u2713 \u2717 \u2713 MAE\n\u2713\u2717 \u2713 \u2713 \u2020 \u2713\u2020 \u2713\u2713\u2713 \u2717 \u2717 MAPE OOS Per\nStep\nActual\nVa l u e s\n\u2713\u2717 \u2713 \u2713 \u2020 \u2713\u2020 \u2713\u2713\u2713 \u2717 \u2717 RMSPE\n\u2713 \u2713 \u2713\u2713\u2713 \u2713\u2713\u2713 \u2717 \u2713 sMAPE\n\u2713 \u2713 \u2713\u2713\u2713 \u2713\u2713\u2713 \u2713 \u2713 msMAPE\n\u2713\u2713 \u2717 \u2717 \u2713 \u2717 \u2713 \u2713 \u2717 \u2717 W APE OOS Per\nSeries\n\u2713\u2713 \u2717 \u2717 \u2713 \u2717 \u2713 \u2713 \u2713 \u2020 \u2717 WRMSPE\n\u2713 \u2713 \u2717\u2717\u2713 \u2717\u2717\u2717 \u2717 \u2717 sMAE In-Sample\nPer Series\n\u2713 \u2713 \u2717\u2717\u2713 \u2717\u2717\u2717 \u2713 \u2020 \u2717 sMSE\n\u2713 \u2713 \u2713\u2713\u2713 \u2713\u2713\u2713 \u2717 \u2713 ND OOS All\nSeries\n\u2713 \u2713 \u2713\u2713\u2713 \u2713\u2713\u2713 \u2713 \u2717 NRMSE\n123",
        "35e0085f-11b4-42ad-9052-a44b52753fcb": "Forecast evaluation for... 823Table 9 continued\nStationary\nCount )\nData(>> 0\nSeasonalityTrend\n(Linear/\nExp.)\nUnit Roots HeteroscedasticityStructural Breaks (With Scale Differences)IntermittenceOutliersError\nMeasures\nScaling\nForecast\nHorizon\nTraining\nRegion\nForecast\nOrigin\n\u2713\u2020 \u2713\u2020 \u2717\u2717\u2713 \u2713 \u2713 \u2020 \u2713\u2717 \u2713 \u2020 MRAE OOS Per\nStep\nBenchmark\nErrors\n\u2713\u2020 \u2713\u2020 \u2717\u2717\u2713 \u2713 \u2713 \u2020 \u2713\u2717 \u2713 \u2020 MdRAE\n\u2713\u2020 \u2713\u2020 \u2717\u2717\u2713 \u2713 \u2713 \u2020 \u2713\u2717 \u2713 \u2020 GMRAE\n\u2713\u2020 \u2713\u2020 \u2717\u2717\u2713 \u2713 \u2713 \u2020 \u2713\u2713 \u2020 \u2717 RMRSE\n\u2713\u2020 \u2713\u2020 \u2717\u2717\u2713 \u2713 \u2713 \u2020 \u2713\u2713 \u2020 \u2713\u2020 Relative Measures OOS Per\nSeries\n\u2713\u2020 \u2713\u2020 \u2713\u2713\u2713 \u2717 \u2713 \u2020 \u2717\u2717 \u2713 MASE In-Sample\nPerSeries\n\u2713\u2020 \u2713\u2020 \u2713\u2713\u2713 \u2717 \u2713 \u2020 \u2717\u2713 \u2020 \u2717 RMSSE\n\u2713\u2020 \u2713\u2020 \u2713\u2713\u2713 \u2713\u2713\u2713 \u2713 \u2713 In-Sample\nAll Series\n\u2713\u2713 \u2713 \u2020 \u2713\u2713 \u2020 \u2713\u2713\u2713 \u2713 \u2713 Measures with TransformationsNone\n123",
        "b29100a8-9968-42fc-b982-122cb627c1d5": "824 H. Hewamalage et al.\nFig. 12 Flow chart for forecast error measure selection\nmentioned comparison tests are per-se designed for comparing two forecasts, and a\nmultiple testing of more than two requires a correction for multiple hypothesis testing,\nsuch as, e.g., a Bonferroni correction.\nThere are other techniques developed to perform comparison within a group of\nmethods (more than 2) as well. Means of error distributions from different methods\ncan be used to compare the mean performance of the methods. The F-test and the\nt-test are statistical tests in this respect. They both have parametric assumptions for\n123",
        "b2512ac5-6f04-4f06-b094-ba8fa8664fe0": "Forecast evaluation for... 825\nFig. 13 An example of a CD diagram to visualize the signi\ufb01cance of the differences between a number of\ncompeting methods. The best three methods A, B and C are not signi\ufb01cantly different from each other. On\nthe other hand, methods D, E and F are signi\ufb01cantly worse than those three methods. The amount of data\nhas not been enough to check whether method E is signi\ufb01cantly better than method D or worse than method\nF\nthe means of the error distributions, that they need to follow a normal distribution.\nAlthough, according to the Central Limit Theorem, this could hold for measures such\nas MSE, MAE etc., for a suf\ufb01ciently large random sample (of size n \u2265 30), it does\nnot hold for e.g., RMSE, since the root of a normally distributed variable is following\na chi-square distribution, which is close to normality but not equivalent. On the other\nhand, the Friedman test (Friedman 1937, 1939, 1940) is a non-parametric statistical\ntest that can be used to detect signi\ufb01cance between multiple competing methods, using\nthe ranks of the methods according to mean errors.\nThe Friedman test is usually followed by a post-hoc test, when the null hypoth-\nesis which states that \u201cthere are no signi\ufb01cant differences between the methods\u201d,\nis rejected. There are different types of post-hoc tests, for example, the Hochberg\nprocedure (Hochberg 1988), the Holm process (Holm 1979), the Bonferroni-Dunn\nprocedure (Dunn 1961), the Nemenyi method (Nemenyi 1963), the Multiple Compar-\nisons with the Best (MCB) method (practically equivalent to the Nemenyi method)\nor the Multiple Comparisons with the Mean (ANOM) method (Halperin et al. 1955),\nand others. In general, the ANOM test holds less value in practice since it is more\nuseful to \ufb01nd which methods are not signi\ufb01cantly different from the best, than from\nsome averagely performing method overall. The Nemenyi method works by de\ufb01n-\ning con\ufb01dence bounds, in terms of a Critical Distance (CD) around the mean ranks\nof the methods to identify which methods have overlapping con\ufb01dence bounds and\nwhich do not. As Dem\u0161ar ( 2006) suggests, if all the comparisons are to be performed\nagainst one control method as opposed to each method against each other, procedures\nsuch as Bonferroni-Dunn and Hochberg\u2019s are better over the Nemenyi test. Once, the\nquantitative results for the signi\ufb01cance of the differences are obtained using any of the\naforementioned methods, they can be visualized using CD diagrams (Dem\u0161ar 2006).\nIn general, in these diagrams, a horizontal axis reports the average ranks of all the\nmethods. Groups of methods that are not signi\ufb01cantly different from each other are\nconnected using black bars. This is illustrated in Fig. 13, an example CD diagram.\n123",
        "809592e0-d4ec-49cf-b3c5-ed5e200b4fc1": "826 H. Hewamalage et al.\nFig. 14 Flow chart for statistical tests selection to measure signi\ufb01cance of model differences\nWhen performing signi\ufb01cance testing, the amount of data included heavily impacts\nthe results of the signi\ufb01cance tests. For example, with a very high number of series,\nthe CD is usually very low, producing signi\ufb01cant results for even small differences\nbetween models. This means that the results are more reliable, that even the slightest\ndifferences between models encountered for such a large amount of data are statis-\ntically highly signi\ufb01cant. On the other hand, it also depends on the number and the\nrelative performance of the set of models included in the comparison. For example,\nhaving more and more poorly performing methods in the group may tend towards\nmaking the CD larger, thus making other intermediate methods have no signi\ufb01cant\ndifference from the best. The \ufb02ow chart in Fig. 14 summarises the decision making\nprocess in selecting a statistical test to measure signi\ufb01cance of differences between\nmodels.\n5 Conclusions\nModel evaluation, just as in any other domain, is a crucial step in forecasting. In\nother major \ufb01elds such as regression, classi\ufb01cation, there exist established techniques\nthat are the standard best practices. On the contrary, in the domain of forecasting,\n123",
        "c2f12592-7aae-48b9-b8c6-3a496837d8d5": "Forecast evaluation for... 827\nevaluation remains a much more complex task. The general trend in the literature has\nbeen to propose new methodologies to address pitfalls associated with the previously\nintroduced. Nevertheless, for example with the forecast evaluation measures, to the\nbest of our knowledge, all the introduced measures thus far can break under given\ncertain characteristics/non-stationarities of the time series. General ML practitioners\nand Data Scientists new to the \ufb01eld of forecasting are often not aware of these issues.\nConsequently, as we demonstrate through our work, forecast evaluation practices used\nby many works even published at top-tier venues in the ML domain can be \ufb02awed.\nAll of this is a consequence of the lack of established best practices and guidelines\nfor the different steps of the forecast evaluation process. Therefore, to support the\nML community in this aspect, we provide a compilation of common pitfalls and best\npractice guidelines related to forecast evaluation. The key set of guidelines that we\ndevelop are as follows.\n\u2022 To claim the competitiveness of the proposed methods, they need to be bench-\nmarked on suf\ufb01ciently large amounts of datasets.\n\u2022 It is always important to compare models against the right and the simplest bench-\nmarks such as the na\u00efve and the seasonal na\u00efve.\n\u2022 Using forecast plots can be misleading; making decisions purely based on the\nvisual appeal on forecast plots is not advisable.\n\u2022 Data leakage needs to be avoided explicitly in rolling origin evaluation and other\ndata pre-processing tasks such as smoothing, decomposition and normalisation of\nthe series.\n\u2022 If enough data are available, tsCV is the procedure of choice. Also, for models with\na continuous state such as RNNs and ETS where the temporal order of the data\nis important, tsCV may be the only applicable validation strategy. k-fold CV is a\nvalid and a data ef\ufb01cient strategy of data partitioning for forecast model validation\nwith pure AR based setups, when the models do not under\ufb01t the data (which can be\ndetected with a test for serial correlation in the residuals, such as the Ljung-Box\ntest). As such, we advise this procedure especially for short series where tsCV\nleads to test sets that are too small. However, if the models under\ufb01t, it is advisable\nto improve the models \ufb01rst before using any CV technique.\n\u2022 There is no single globally accepted evaluation measure for all scenarios. It depends\non the characteristics of the data as summarized in Table 9.\n\u2022 When using statistical testing for signi\ufb01cance of the differences between models,\nbalancing the diversity of the compared models against the number of data points\nis important to avoid spurious statistical similarity/difference between models.\nWhile the literature on evaluation measures is quite extensive, the exact errors\n(squared/ absolute), summarisation operators (mean/median/geometric mean), type of\nscaling to use (global/per-series/per-step/, in-sample/OOS, relative/percentage) differ\nbased on the user expectations, business utility and the characteristics of the underlying\ntime series. Due to the lack of proper knowledge in forecast evaluation, ML research\nin the literature thus far has often either struggled to demonstrate the competitiveness\nof its models or arrived at spurious conclusions. It is our objective that this effort\nencourages better and correct forecast evaluation practices within the ML community.\nAs a potential avenue for further work especially with respect to evaluation measures,\n123",
        "e5e4305d-a822-4f5c-a70c-66856aa46083": "828 H. Hewamalage et al.\nit would be useful to design combination based evaluation measures for forecasting,\nsimilar to the Huber loss for model training, which is a combination of the MAE\nand the RMSE. These types of measures can be quite robust, combining the strengths\nof both measures while minimising the potential disadvantages associated with the\nindividual measures.\nAcknowledgements This work was done as part of the PhD degree of Hansika Hewamalage at the Faculty\nof IT, Monash University. This research was supported by the Australian Research Council under grant\nDE190100045, a Facebook Statistics for Improving Insights and Decisions research award and Monash\nUniversity Graduate Research funding. We would like to thank two anonymous reviewers and the Associate\nEditor whose comments have lead to considerable improvements of the paper.\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International License, which\npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give\nappropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence,\nand indicate if changes were made. The images or other third party material in this article are included\nin the article\u2019s Creative Commons licence, unless indicated otherwise in a credit line to the material. If\nmaterial is not included in the article\u2019s Creative Commons licence and your intended use is not permitted\nby statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the\ncopyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/ .\nReferences\nArmstrong J (2001) Evaluating forecasting methods. In: Armstrong JS (ed) Principles of forecasting: a\nhandbook for researchers and practitioners. Kluwer Academic Publishers, Norwell, MA\nArmstrong JS, Grohman MC (1972) A comparative study of methods for long-range market forecasting.\nManag Sci 19(2):211\u2013221\nArnott R, Harvey C R, Markowitz H (2019) A backtesting protocol in the era of machine learning. J Financ\nData Sci\nBagnall A, Lines J, Bostrom A, Large J, Keogh E (2016) The great time series classi\ufb01cation bake off: a review\nand experimental evaluation of recent algorithmic advances. Data Min Knowl Disc 31(3):606\u2013660\nBalestriero R, Pesenti J, LeCun Y (2021) Learning in high dimension always amounts to extrapolation.\narXiv preprint arXiv:2110.09485\nBell F, Smyl S, (2018) Forecasting at uber: an introduction. https://eng.uber.com/forecasting-introduction/\nBerger D, Chaboud A, Hjalmarsson E (2009) What drives volatility persistence in the foreign exchange\nmarket? J Financ Econ 94(2):192\u2013213\nBergmeir C, Hyndman RJ, Koo B (2018) A note on the validity of cross-validation for evaluating autore-\ngressive time series prediction. Comput Stat Data Anal 120:70\u201383\nBerm\u00fadez JD, Segura JV , V ercher E (2006) A decision support system methodology for forecasting of time\nseries based on soft computing. Comput Stat Data Anal 51(1):177\u2013191\nBojer C S, Meldgaard J P (2020) Kaggle forecasting competitions: an overlooked learning opportunity. Int\nJ Forecast\nBrownlee J (2020) Data preparation for machine learning: data cleaning, feature selection, and data trans-\nforms in Python. Mach Learn Mastery\nCerqueira V , Torgo L, Mozeti\u02c7c I (2020) Evaluating time series forecasting models: an empirical study on\nperformance estimation methods. Mach Learn 109(11):1997\u20132028\nChallu C, Olivares K. G, Oreshkin B N, Garza, F, Mergenthaler-Canseco M, Dubrawski A (2022) N-hits:\nneural hierarchical interpolation for time series forecasting. arXiv:2201.12886\nChen C, Twycross J, Garibaldi JM (2017) A new accuracy measure based on bounded relative error for\ntime series forecasting.",
        "e065658b-5cf5-4ba2-86ba-7fb6b731029f": "Int\nJ Forecast\nBrownlee J (2020) Data preparation for machine learning: data cleaning, feature selection, and data trans-\nforms in Python. Mach Learn Mastery\nCerqueira V , Torgo L, Mozeti\u02c7c I (2020) Evaluating time series forecasting models: an empirical study on\nperformance estimation methods. Mach Learn 109(11):1997\u20132028\nChallu C, Olivares K. G, Oreshkin B N, Garza, F, Mergenthaler-Canseco M, Dubrawski A (2022) N-hits:\nneural hierarchical interpolation for time series forecasting. arXiv:2201.12886\nChen C, Twycross J, Garibaldi JM (2017) A new accuracy measure based on bounded relative error for\ntime series forecasting. PLoS ONE 12(3):e0174202\nCox D, Miller H (1965) The Theory of Stochastic Processes\nCui Y , Xie J, Zheng K (2021) Historical inertia: a neglected but powerful baseline for long sequence\ntime-series forecasting. In: Proceedings of the 30th ACM International Conference on Information &\n123",
        "5f8e8fab-c686-4b9d-afa3-e9e037c0ea68": "Forecast evaluation for... 829\nKnowledge Management. CIKM \u201921. Association for Computing Machinery, New Y ork, NY , USA,\npp 2965-2969\nDavydenko A, Fildes R (2013) Measuring forecasting accuracy: The case of judgmental adjustments to\nSKU-level demand forecasts. Int J Forecast 29(3):510\u2013522\nDem\u0161ar J (2006) Statistical comparisons of classi\ufb01ers over multiple data sets. J Mach Learn Res 7(1):1\u201330\nDiebold FX, Mariano RS (2002) Comparing predictive accuracy. J Bus Econ Stat 20(1):134\u2013144\nDitzler G, Roveri M, Alippi C, Polikar R (2015) Learning in nonstationary environments: a survey. IEEE\nComput Intell Mag 10(4):12\u201325\nDunn OJ (1961) Multiple comparisons among means. J Am Stat Assoc 56(293):52\u201364\nDu D, Su B, Wei Z (2022) Preformer: predictive transformer with multi-scale segment-wise correlations\nfor long-term time series forecasting. arXiv:2202.11356\nDu Y , Wang J, Feng W, Pan S, Qin T, Xu R, Wang C (2021) Adarnn: adaptive learning and forecasting of\ntime series. In: Proceedings of the 30th ACM International Conference on Information & Knowledge\nManagement. CIKM \u201921. Association for Computing Machinery, New Y ork, NY , USA, pp 402-411\nEngle R F (2003) Risk and volatility: econometric models and \ufb01nancial practice. Nobel Lect. https://www.\nnobelprize.org/uploads/2018/06/engle-lecture.pdf\nFama EF (1970) Ef\ufb01cient capital markets: a review of theory and empirical work. J Financ 25(2):383\u2013417\nFawaz HI, Forestier G, Weber J, Idoumghar L, Muller P-A (2019) Deep learning for time series classi\ufb01cation:\na review. Data Min Knowl Discov 33(4):917\u2013963\nFriedman M (1937) The use of ranks to avoid the assumption of normality implicit in the analysis of\nvariance. J Am Stat Assoc 32(200):675\u2013701\nFriedman M (1939) A correction: the use of ranks to avoid the assumption of normality implicit in the\nanalysis of variance. J Am Stat Assoc 34(205):109\u2013109\nFriedman M (1940) A comparison of alternative tests of signi\ufb01cance for the problem of m rankings. Ann\nMath Stat 11(1):86\u201392\nFry C, Lichtendahl C (2020) Google practitioner session. In: 40th International Symposium on Forecasting.\nhttps://www.youtube.com/watch?v=FoUX-muLlB4&t=3007s\nGama J, Sebastiao R, Rodrigues PP (2013) On evaluating stream learning algorithms. Mach Learn\n90(3):317\u2013346\nGama J. a, \u017dliobaitunde\ufb01ned I, Bifet A, Pechenizkiy M, Bouchachia A (2014) A survey on concept drift\nadaptation. ACM Comput Surv 46 (4)\nGhomeshi H, Gaber MM, Kovalchuk Y (2019) EACD: evolutionary adaptation to concept drifts in data\nstreams. Data Min Knowl Disc 33(3):663\u2013694\nGiacomini R, White H (2006) Tests of conditional predictive ability. Econometrica 74(6):1545\u20131578\nGodahewa R, Bandara K, Webb GI, Smyl S, Bergmeir C (2021) Ensembles of localised models for time\nseries forecasting. Knowl Based Syst 233:107518\nGodfrey LB, Gashler MS (2018) Neural decomposition of time-series data for effective generalization.\nIEEE Trans Neural Netw Learn Syst 29(7):2973\u20132985\nGujarati DN (2021) Essentials of econometrics.",
        "7469f7ed-d234-4ea2-9a98-584f4e98e653": "ACM Comput Surv 46 (4)\nGhomeshi H, Gaber MM, Kovalchuk Y (2019) EACD: evolutionary adaptation to concept drifts in data\nstreams. Data Min Knowl Disc 33(3):663\u2013694\nGiacomini R, White H (2006) Tests of conditional predictive ability. Econometrica 74(6):1545\u20131578\nGodahewa R, Bandara K, Webb GI, Smyl S, Bergmeir C (2021) Ensembles of localised models for time\nseries forecasting. Knowl Based Syst 233:107518\nGodfrey LB, Gashler MS (2018) Neural decomposition of time-series data for effective generalization.\nIEEE Trans Neural Netw Learn Syst 29(7):2973\u20132985\nGujarati DN (2021) Essentials of econometrics. Sage Publications, Christchurch, New Zealand\nGuo Y , Zhang S, Yang J, Y u G, Wang Y (2022) Dual memory scale network for multi-step time series\nforecasting in thermal environment of aquaculture facility: a case study of recirculating aquaculture\nwater temperature. Expert Syst Appl 208:118218\nHalperin M, Greenhouse SW, Corn\ufb01eld J, Zalokar J (1955) Tables of percentage points for the studentized\nmaximum absolute deviate in normal samples. J Am Stat Assoc 50(269):185\u2013195\nH\u00e4m\u00e4l\u00e4inen W, Webb G I, (2019) A tutorial on statistically sound pattern discovery. Data Min Knowl\nDiscov 33 (2): 325\u2013377\nHannun A, Guo C, van der Maaten L (2021) Measuring data leakage in machine-learning models with \ufb01sher\ninformation. In: de Campos, C, Maathuis, M H (eds) Proceedings of the Thirty-Seventh Conference\non Uncertainty in Arti\ufb01cial Intelligence. vol 161, pp 760\u2013770\nHastie T, Tibshirani R, Friedman J (2009) The elements of statistical learning: data mining, inference, and\nprediction. Springer, New Y ork, NY\nHewamalage H, Bergmeir C, Bandara K (2021) Recurrent neural networks for time series forecasting:\ncurrent status and future directions. Int J Forecast 37(1):388\u2013427\nHochberg Y (1988) A sharper bonferroni procedure for multiple tests of signi\ufb01cance. Biometrika 75(4):800\u2013\n802\nHolm S (1979) A simple sequentially rejective multiple test procedure. Scand J Stat 6(2):65\u201370\n123",
        "a506794d-24c1-460f-8b6a-403d4d48d0fb": "830 H. Hewamalage et al.\nHyndman R J, Athanasopoulos G (2018) Forecasting: principles and Practice, 2nd edn. OTexts. https://\notexts.com/fpp2/\nHyndman RJ, Koehler AB (2006) Another look at measures of forecast accuracy. Int J Forecast 22(4):679\u2013\n688\nHyndman R, Kang Y , Talagala T, Wang E, Yang Y (2019) tsfeatures: time series feature extraction. R\npackage version 1.0.0. https://pkg.robjhyndman.com/tsfeatures/\nIkonomovska E, Gama J, D\u017eeroski S (2010) Learning model trees from evolving data streams. Data Min\nKnowl Discov 23(1):128\u2013168\nKaufman S, Rosset S, Perlich C, Stitelman O (2012) Leakage in data mining: Formulation, detection, and\navoidance. ACM Trans Knowl Discov Data 6(4):1\u201321\nKim S, Kim H (2016) A new metric of absolute percentage error for intermittent demand forecasts. Int J\nForecast 32(3):669\u2013679\nKolassa S (2020) Why the best point forecast depends on the error or accuracy measure. Int J Forecast\n36(1):208\u2013211\nKourentzes N (2014) On intermittent demand model optimisation and selection. Int J Prod Econ 156:180\u2013\n190\nKoutsandreas D, Spiliotis E, Petropoulos F, Assimakopoulos V (2021) Aasures. J Oper Res Soc, 1\u201318\nKunst R (2016) Visualization of distance measures implied by forecast evaluation criteria. In: Interna-\ntional Symposium on Forecasting 2016. https://forecasters.org/wp-content/uploads/gravity_forms/7-\n621289a708af3e7af65a7cd487aee6eb/2016/07/Kunst_Robert_ISF2016.pdf\nKuranga C, Pillay N (2022) A comparative study of nonlinear regression and autoregressive techniques in\nhybrid with particle swarm optimization for time-series forecasting. Expert Syst Appl 190:116163\nLai G, Chang W.-C, Yang Y , Liu H (2018) Modeling long- and short-term temporal patterns with deep\nneural networks. In: The 41st International ACM SIGIR Conference on Research & Development in\nInformation Retrieval. SIGIR \u201918. Association for Computing Machinery, New Y ork, NY , USA, pp\n95-104\nLi J, Liao Z, Quaedvlieg R (2022) Conditional superior predictive ability. Rev Econ Stud 89(2):843\u2013875\nLi B, Du S, Li T, Hu J, Jia Z (2022a) Draformer: differentially reconstructed attention transformer for\ntime-series forecasting. arXiv:2206.05495\nLin G, Lin A, Cao J (2021) Multidimensional knn algorithm based on eemd and complexity measures in\n\ufb01nancial time series forecasting. Expert Syst Appl 168:114443\nLiu S, Ji H, Wang MC (2020) Nonpooling convolutional neural network forecasting for seasonal time series\nwith trends. IEEE Trans Neural Netw Learn Syst 31(8):2879\u20132888\nLiu Q, Long L, Peng H, Wang J, Yang Q, Song X, Riscos-N\u00fa\u00f1ez A, P\u00e9rez-Jim\u00e9nez M J (2021) Gated\nspiking neural p systems for time series forecasting. IEEE Trans Neural Netw Learn Syst, 1\u201310\nLjung GM, Box GEP (1978) On a measure of lack of \ufb01t in time series models. Biometrika 65(2):297\u2013303\nLubba CH, Sethi SS, Knaute P , Schultz SR, Fulcher BD, Jones NS (2019) catch22: CAnonical time-series\nCHaracteristics.",
        "aae37e37-7e42-480a-bcc1-cb47457e3ee8": "IEEE Trans Neural Netw Learn Syst 31(8):2879\u20132888\nLiu Q, Long L, Peng H, Wang J, Yang Q, Song X, Riscos-N\u00fa\u00f1ez A, P\u00e9rez-Jim\u00e9nez M J (2021) Gated\nspiking neural p systems for time series forecasting. IEEE Trans Neural Netw Learn Syst, 1\u201310\nLjung GM, Box GEP (1978) On a measure of lack of \ufb01t in time series models. Biometrika 65(2):297\u2013303\nLubba CH, Sethi SS, Knaute P , Schultz SR, Fulcher BD, Jones NS (2019) catch22: CAnonical time-series\nCHaracteristics. Data Min Knowl Disc 33(6):1821\u20131852\nMakridakis S (1993) Accuracy measures: theoretical and practical concerns. Int J Forecast 9(4):527\u2013529\nMakridakis S, Hibon M (2000) The m3-competition: results, conclusions and implications. Int J Forecast\n16(4):451\u2013476\nMakridakis S, Spiliotis E, Assimakopoulos V (2020) The M4 Competition: 100,000 time series and 61\nforecasting methods. Int J Forecast 36(1):54\u201374\nMakridakis S, Spiliotis E, Assimakopoulos V (2022) M5 accuracy competition: results, \ufb01ndings, and con-\nclusions. Int J Forecast 38(4):1346\u20131364\nMann HB, Whitney DR (1947) On a test of whether one of two random variables is stochastically larger\nthan the other. Ann Math Stat 18(1):50\u201360\nMoon H, Lee H, Song B (2022) Mixed pooling of seasonality for time series forecasting: an application to\npallet transport data. Expert Syst Appl 201:117195\nNemenyi P (1963) Distribution-free multiple comparisons. In: Ph.D. thesis, Princeton University\nPetropoulos F et al (2022) Forecasting: theory and practice. Int J Forecast 38(3):705\u2013871\nPetropoulos F, Kourentzes N (2015) Forecast combinations for intermittent demand. J Oper Res Soc\n66(6):914\u2013924\nRan P , Dong K, Liu X, Wang J (2023) Short-term load forecasting based on ceemdan and transformer.\nElectric Power Sys Res 214:108885\nRossi B (2013) Exchange rate predictability. J Econ Lit 51(4):1063\u20131119\n123",
        "781de399-d0f5-4d86-82d0-44cb7c59f30b": "Forecast evaluation for... 831\nSalinas D, Flunkert V , Gasthaus J, Januschowski T (2020) Deepar: probabilistic forecasting with autore-\ngressive recurrent networks. Int J Forecast 36(3):1181\u20131191\nSalles R, Belloze K, Porto F, Gonzalez PH, Ogasawara E (2019) Nonstationary time series transformation\nmethods: an experimental review. Knowl Based Syst 164:274\u2013291\nShabani A, Abdi A, Meng L, Sylvain T (2022) Scaleformer: iterative multi-scale re\ufb01ning transformers for\ntime series forecasting. arXiv:2206.04038\nShcherbakov M, Brebels A, Shcherbakova N, Tyukov A, Janovsky T, Kamaev V (2013) A survey of forecast\nerror measures. World Appl Sci J 24(24):171\u2013176\nShen Z, Zhang Y , Lu J, Xu J, Xiao G (2020) A novel time series forecasting model with deep learning.\nNeurocomputing 396:302\u2013313\nShih S-Y , Sun F-K, Lee H-Y (2019) Temporal pattern attention for multivariate time series forecasting.\nMach Learn 108(8):1421\u20131441\nStone M (1974) Cross-validatory choice and assessment of statistical predictions. J R Stat Soc Ser B\nMethodol 36(2):111\u2013147\nSuilin A (2017) kaggle-web-traf\ufb01c. Accessed: 2018-11-19. https://github.com/Arturus/kaggle-web-traf\ufb01c/\nSun F-K, Boning D S (2022) Fredo: frequency domain-based long-term time series forecasting.\narXiv:2205.12301\nSvetunkov I (2021) Forecasting and analytics with adam. OpenForecast, (version: [current date]). https://\nopenforecast.org/adam/\nSyntetos AA, Boylan JE (2005) The accuracy of intermittent demand estimates. Int J Forecast 21(2):303\u2013314\nTalagala T S (2020) A tool to detect potential data leaks in forecasting competitions. In: International\nSymposium on Forecasting 2020. https://thiyanga.netlify.app/talk/isf20-talk/\nTashman LJ (2000) Out-of-sample tests of forecasting accuracy: an analysis and review. Int J Forecast\n16(4):437\u2013450\nWebb GI, Hyde R, Cao H, Nguyen HL, Petitjean F (2016) Characterizing concept drift. Data Min Knowl\nDiscov 30(4):964\u2013994\nWong L (2019) Error metrics in time series forecasting. In: International Symposium\non Forecasting 2019. https://isf.forecasters.org/wp-content/uploads/gravity_forms/2-\ndd30f7ae09136fa695c552259bdb3f99/2019/07/ISF_2019_slides.pdf\nWoo G, Liu C, Sahoo D, Kumar A, Hoi S (2022) Etsformer: exponential smoothing transformers for\ntime-series forecasting. arXiv:2202.01381\nWu Z, Pan S, Long G, Jiang J, Chang X, Zhang C (2020) Connecting the dots: Multivariate time series\nforecasting with graph neural networks. In: Proceedings of the 26th ACM SIGKDD International Con-\nference on Knowledge Discovery & Data Mining. KDD \u201920. Association for Computing Machinery,\nNew Y ork, NY , USA, pp 753-763\nWu H, Xu J, Wang J, Long M (2021) Autoformer: Decomposition transformers with Auto-Correlation for\nlong-term series forecasting. In: Advances in Neural Information Processing Systems\nYe J, Liu Z, Du B, Sun L, Li W, Fu Y , Xiong H (2022) Learning the evolutionary and multi-scale graph\nstructure for multivariate time series forecasting. In: Proceedings of the 28th ACM SIGKDD Confer-\nence on Knowledge Discovery and Data Mining. KDD \u201922.",
        "86beef95-fdc3-4111-beec-07856012eb03": "In: Proceedings of the 26th ACM SIGKDD International Con-\nference on Knowledge Discovery & Data Mining. KDD \u201920. Association for Computing Machinery,\nNew Y ork, NY , USA, pp 753-763\nWu H, Xu J, Wang J, Long M (2021) Autoformer: Decomposition transformers with Auto-Correlation for\nlong-term series forecasting. In: Advances in Neural Information Processing Systems\nYe J, Liu Z, Du B, Sun L, Li W, Fu Y , Xiong H (2022) Learning the evolutionary and multi-scale graph\nstructure for multivariate time series forecasting. In: Proceedings of the 28th ACM SIGKDD Confer-\nence on Knowledge Discovery and Data Mining. KDD \u201922. Association for Computing Machinery,\nNew Y ork, NY , USA, pp 2296-2306\nZeng A, Chen M, Zhang L, Xu Q (2022) Are transformers effective for time series forecasting?\nZhang X, He K, Bao Y (2021) Error-feedback stochastic modeling strategy for time series forecasting with\nconvolutional neural networks. Neurocomputing 459:234\u2013248\nZhou Y , Zhang M, Lin K-P (2022) Time series forecasting by the novel gaussian process wavelet self-join\nadjacent-feedback loop reservoir model. Expert Syst Appl 198:116772\nZhou T, Ma Z, wang X, Wen Q, Sun L, Yao T, Yin W, Jin R (2022a) Film: frequency improved legendre\nmemory model for long-term time series forecasting. In: Advances in Neural Information Processing\nSystems. arXiv:2205.08897\nZhou T, Ma Z, Wen Q, Wang X, Sun L, Jin R (2022b) FEDformer: Frequency enhanced decomposed\ntransformer for long-term series forecasting. In: Chaudhuri K, Jegelka S, Song L, Szepesvari C, Niu\nG, Sabato S (eds), Proceedings of the 39th International Conference on Machine Learning. V ol. 162\nof Proceedings of Machine Learning Research. PMLR, pp 27268\u201327286\nZhou H, Zhang S, Peng J, Zhang S, Li J, Xiong H, Zhang W (2021) Informer: Beyond ef\ufb01cient transformer for\nlong sequence time-series forecasting. In: The Thirty-Fifth AAAI Conference on Arti\ufb01cial Intelligence,\nAAAI 2021, Virtual Conference. vol 35. AAAI Press, pp 11106\u201311115\n123",
        "c08b5023-ef37-42da-ba7b-048fd75b090a": "832 H. Hewamalage et al.\nZhou T, Zhu J, Wang X, Ma Z, Wen Q, Sun L, Jin R (2022c) Treedrnet:a robust deep model for long term\ntime series forecasting. arXiv:2206.12106\nPublisher\u2019s Note Springer Nature remains neutral with regard to jurisdictional claims in published maps\nand institutional af\ufb01liations.\n123",
        "9f7856fe-9196-4eec-bac2-622e3f60f0be": "See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/361515440\nMachine Learning for Probabilistic Prediction (PhD thesis, VALERY\nMANOKHIN)\nThesis \u00b7 June 2022\nDOI: 10.5281/zenodo.6727505\nCITATIONS\n0\nREADS\n14,135\n1 author:\nValery Manokhin\nRoyal Holloway University of London\n26 PUBLICATIONS\u00a0\u00a0\u00a0243 CITATIONS\u00a0\u00a0\u00a0\nSEE PROFILE\nAll content following this page was uploaded by Valery Manokhin on 24 June 2022.\nThe user has requested enhancement of the downloaded file.",
        "a41528cd-829e-46e3-9b9e-17cfb8e41d3b": "1\nROYAL HOLLOWAY UNIVERSITY OF LONDON\nCENTRE FOR RELIABLE MACHINE LEARNING\nDEPARTMENT OF COMPUTER SCIENCE\nMachine Learning for Probabilistic Prediction\nVALERY MANOKHIN\nSupervisors:\nProf. Vladimir Vovk\nProf. Alessio Sancetta\nJune 24, 2022\nA THESIS SUBMITTED FOR THE DEGREE OF DOCTOR OF PHILOSOPHY IN MACHINE LEARNING",
        "4a6be992-c09f-4b45-a124-a76a5e3334e3": "2",
        "4a92987a-a879-4820-a2b2-f511360a2498": "3\nDeclaration of Authorship\nI hereby declare that this thesis and the work presented in it is entirely my own. Where I\nhave consulted the work of others, this is always clearly stated.\nValery Manokhin June 24, 2022",
        "80c134fc-bd73-47ef-919e-8612edddf3b8": "4\n\u201cProphesy is a good line of business, but it is full of risks.\u201d\nMark Twain in \u201cFollowing the Equator.\u201d",
        "8be7962c-3382-4981-ad8e-2ae083f17031": "5\nAbstract\nPrediction is the key objective of many machine learning applications. Accurate, reli-\nable and robust predictions are essential for optimal and fair decisions by downstream\ncomponents of artificial intelligence systems, especially in high-stakes applications, such\nas personalised health, self-driving cars, finance, new drug development, forecasting of\nelection outcomes and pandemics.\nMany modern machine learning algorithms output overconfident predictions, result-\ninginincorrectdecisionsandtechnologyacceptanceissues. Classicalcalibrationmethods\nrely on artificial assumptions and often result in overfitting, whilst modern calibration\nmethods attempt to solve calibration issues by modifying components of black-box deep-\nlearning systems. While this provides a partial solution, such modifications do not pro-\nvide mathematical guarantees of predictions validity, are intrusive, complex, and costly\nto implement.\nThis thesis introduces novel methods for producing well-calibrated probabilistic pre-\ndictions for machine learning classification and regression problems. A new method\nfor multi-class classification problems is developed and compared to traditional calibra-\ntion approaches. In the regression setting, the thesis develops novel methods for proba-\nbilistic regression to derive predictive distribution functions that are valid under a non-\nparametricIIDassumptionintermsofguaranteedcoverageandcontainmoreinformation\nwhen compared to classical conformal prediction methods whilst improving computa-\ntional e\ufb00iciency. Experimental studies of the methods introduced in this thesis demon-\nstrate advantages with regard to state-of-the-art. The main advantage of split conformal\npredictive systems is their guaranteed validity, whilst cross-conformal predictive systems\nenjoyhigherpredictivee\ufb00iciencyandempiricalvalidityintheabsenceofexcessrandomi-\nsation.",
        "84285a97-c448-44c9-9db0-c6c9abe20c1f": "6",
        "16e3ed71-76d1-432c-b64f-c32e9d47c7c7": "7\nAcknowledgements\nI would like to thank Prof. Vladimir Vovk and Prof. Alessio Sancetta for their invaluable\nsupport and guidance during the course of my PhD. My research has been supported by\nthe Leverhulme Magna Carta Doctoral Centre.",
        "9cdcfc6a-3886-4e19-8065-2cf3fcfaba58": "8",
        "737f2521-66f2-4df1-924b-4e74b3617e5e": "9\nContents\nDeclaration of Authorship 3\nAcknowledgements 7\n1 Introduction 19\n1.1 Machine learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n1.1.1 Terminology of machine learning . . . . . . . . . . . . . . . . . . . . 20\n1.1.2 History of supervised learning . . . . . . . . . . . . . . . . . . . . . 20\n1.2 Probabilistic prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n1.2.1 Terminology of probabilistic prediction . . . . . . . . . . . . . . . . 23\n1.2.2 Conformal prediction . . . . . . . . . . . . . . . . . . . . . . . . . . 24\n1.3 Main contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\n2 Literature review 29\n2.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\n2.2 Calibration methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\n2.2.1 Histogram binning . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\n2.2.2 Platt\u2019s method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\n2.2.3 Isotonic regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\n2.2.4 Smooth isotonic regression . . . . . . . . . . . . . . . . . . . . . . . 36\n2.2.5 Nested Dichotomies . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\n2.2.6 Beta calibration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\n2.2.7 Scaling-binning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38\n2.2.8 Probability Calibration Trees . . . . . . . . . . . . . . . . . . . . . . 39\n2.2.9 Bayesian Binning into Quantiles . . . . . . . . . . . . . . . . . . . . 39",
        "8a82bdfc-4de1-49e9-866b-12dc1e5c03b6": "10\n2.2.10 Ensemble of Near Isotonic Regression (ENIR). . . . . . . . . . . . 40\n2.2.11 Temperature Scaling . . . . . . . . . . . . . . . . . . . . . . . . . . . 41\n2.2.12 Entropy penalty . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41\n2.2.13 Maximum Mean Calibration Error (MMCE). . . . . . . . . . . . . 42\n2.2.14 Label smoothing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43\n2.2.15 Focal loss . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\n2.2.16 Venn predictors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\n2.2.17 Venn-Abers predictors . . . . . . . . . . . . . . . . . . . . . . . . . . 46\n3 Probabilistic classification 49\n3.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49\n3.2 Obtaining multi-class probabilities from pairwise classification. . . . . . . 54\n3.3 Inductive and cross Venn-Abers predictors . . . . . . . . . . . . . . . . . . 55\n3.3.1 Computational details of IVAP and CVAP. . . . . . . . . . . . . . . 56\n3.4 Experiments on multi-class data sets . . . . . . . . . . . . . . . . . . . . . . 59\n3.4.1 Waveform data set. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60\n3.4.2 Satimage data set. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62\n3.4.3 Vehicle silhouettes data set . . . . . . . . . . . . . . . . . .",
        "51ead69e-05b8-4c3d-abf8-cdb0a357529a": ". . . . . . 59\n3.4.1 Waveform data set. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60\n3.4.2 Satimage data set. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62\n3.4.3 Vehicle silhouettes data set . . . . . . . . . . . . . . . . . . . . . 63\n3.4.4 Balance scale data set . . . . . . . . . . . . . . . . . . . . . . . . . 65\n3.4.5 Eye movement data set . . . . . . . . . . . . . . . . . . . . . . . . . . 66\n3.4.6 Pasture data set . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67\n3.4.7 Abalone data set . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68\n3.5 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69\n4 Probabilistic regression 71\n4.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71\n4.2 Predictive distributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72\n4.3 Predictive distribution functions . . . . . . . . . . . . . . . . . . . . . . . . 73\n4.4 Randomized and conformal predictive distributions . . . . . . . . . . . . . 73\n4.5 Monotonic conformity measures . . . . . . . . . . . . . . . . . . . . . . . . 78\n4.6 Least Squares Prediction Machine . . . . . . . . . . . . . . . . . . . . . . . . 80",
        "2e8c564b-036b-4fbd-80e3-b1ef818a0a12": "11\n4.6.1 High leverage points . . . . . . . . . . . . . . . . . . . . . . . . . . . 81\n4.7 Validity of the LSPM in the online mode. . . . . . . . . . . . . . . . . . . . 85\n4.8 Asymptotic e\ufb00iciency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86\n4.9 Experimental results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93\n4.10 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95\n5 Kernel probabilistic regression 99\n5.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99\n5.2 The problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101\n5.3 Bayesian solution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102\n5.4 Fiducial predictive distributions . . . . . . . . . . . . . . . . . . . . . . . . . 103\n5.5 Conformal predictive distributions . . . . . . . . . . . . . . . . . . . . . . . 104\n5.6 Kernel Ridge Regression Prediction Machine . . . . . . . . . . . . . . . . . 105\n5.7 Limitation of the KRRPM . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111\n5.8 Experimental results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113\n5.9 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118\n6 Computationally e\ufb00icient probabilistic regression 121\n6.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121\n6.2 Split conformal predictive systems . . . . . . . . . . . . . . . . . . . . . . . 122\n6.3 Cross-conformal predictive distributions . . . . . . . . . . . . . . . . . . . . 127\n6.4 Continuous ranked probability score . . . . . . . . . . . . . . . . . . . . . . 130\n6.5 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132\n6.6 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140\n7 Conclusion 145\nBibliography 149",
        "88c76ae4-1baf-4312-aedd-c34b013995d6": "12",
        "ebd53ff3-029e-4c33-8b33-6aa42b913ab2": "13\nList of Figures\n3.1 Isotonic regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57\n3.2 PAVA algorithm for isotonic regression. . . . . . . . . . . . . . . . . . . . . 57\n4.1 Examples of true predictive distribution functions . . . . . . . . . . . . . . 76\n4.2 The asymptotic variances for the Dempster-Hill (DH) procedure. . . . . 90\n4.3 The cumulative sums Sn of the p-values vsn = 1 , . . . , 1000 . . . . . . . . . 94\n4.4 The cumulative sums S\u03b1\nn vs n = 1 , . . . , 1000 for \u03b1 2 f0.25, 0.5.0.75g . . . . 95\n4.5 The calibration curve: A\u03b1\nN vs \u03b1 2 [0, 1] for N = 1000 . . . . . . . . . . . . . 96\n4.6 Predictions by Oracles I-III . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97\n5.1 Kernel probabilistic regression, synthetic data set of 1000 points. . . . . . 114\n5.2 Kernel probabilistic regression, synthetic data set of 100 points. . . . . . . 114\n5.3 Predictions of the KRRPM on a synthetic data set of 1000 points. . . . . . 115\n5.4 Predictions of the (studentized) KRRPM on a synthetic data set of 10 points116\n5.5 Kernel probabilistic regression for the Boston Housing data set. . . . . . . 118\n6.1 SCPD, the Boston Housing data set. . . . . . . . . . . . . . . . . . . . . . . 131\n6.2 SCPS/CCPS performance, the Boston Housing data set. . . . . . . . . . . 134\n6.3 SCPS/CCPS performance, the Diabetes data set. . . . . . . . . . . . . . . . 135\n6.4 SCPS/CCPS performance, the Yacht Hydrodynamics data set . . . . . . . 136\n6.5 SCPS/CCPS performance, the Wine Quality data set. . . . . . . . . . . . . 137\n6.6 SCPS/CCPS performance, the Naval Propulsion data set . . . . . . . . . . 138\n6.7 Calibration curves, the Boston Housing data set . . . . . . . . . . . . . . . 140\n6.8 Calibration curves, the Diabetes data set. . . . . . . . . . . . . . . . . . . . 141\n6.9 Calibration curves, the Yacht Hydrodynamics data set. . . . . . . . . . . . 142",
        "1f95a53d-cf2e-40e1-93f0-66bcd0e12320": "14\n6.10 Calibration curves, the Wine Quality data set. . . . . . . . . . . . . . . . . 143\n6.11 Calibration curves, the Naval Propulsion data set. . . . . . . . . . . . . . . 144",
        "504c1d2c-c476-4337-b8c8-f61a130a7a36": "15\nList of Tables\n3.1 The Log loss for thewaverfont data set . . . . . . . . . . . . . . . . . . . . 61\n3.2 The Brier loss for thewaveform data set . . . . . . . . . . . . . . . . . . . . 61\n3.3 The Log loss for thesatimage data set . . . . . . . . . . . . . . . . . . . . . 62\n3.4 The Brier loss for thesatimage data set . . . . . . . . . . . . . . . . . . . . 63\n3.5 The Log loss for thevehicle data set. . . . . . . . . . . . . . . . . . . . . . 64\n3.6 The Brier loss for thevehicle data set . . . . . . . . . . . . . . . . . . . . . 64\n3.7 The Log loss for thebalance scale data set . . . . . . . . . . . . . . . . . 65\n3.8 The Brier loss for thebalance scale data set . . . . . . . . . . . . . . . . . 65\n3.9 The Log loss for theeye movement data set . . . . . . . . . . . . . . . . . . 66\n3.10 The Brier loss for theeye movement data set. . . . . . . . . . . . . . . . . . 66\n3.11 The Log loss for thepasture data set. . . . . . . . . . . . . . . . . . . . . . 67\n3.12 The Brier loss for thepasture data set . . . . . . . . . . . . . . . . . . . . . 68\n3.13 The Log loss for theabalone data set. . . . . . . . . . . . . . . . . . . . . . 68\n3.14 The Brier loss for theabalone data set . . . . . . . . . . . . . . . . . . . . . 69\n5.1 The cumulative losses for polynomial kernels and Gaussian kernels. . . . 118\n6.1 Best results for the median CRPS loss for SCPS and CCPS for the five\ndatasets and three underlying algorithms.. . . . . . . . . . . . . . . . . . . 139",
        "cf1905da-a519-445a-8842-a696dc0acd45": "16",
        "27c6a33f-2a77-4ad3-90e7-e4ccacd866fc": "17\nList of Abbreviations\nCP Conformal Prediction\nCPD C onformal PredictiveDistribution\nCPS C onformal PredictiveSystem\nCCPS C ross Conformal PredictiveSystem\nCVAP C ross Venn-Abers Predictors\nDL Deep Learning\nDH Dempster-Hill predictive distribution\nIVAP I nductiveVenn-Abers Predictors\nIR Isotonic Regression\nKRRPM K ernel Ridge Regression Prediction Machine\nLSPM L east Squared Prediction Machine\nLR Logistic Regression\nNB Na\u00efveBayes\nNN Neural Network\nRPD R andomized PredictiveDistribution\nRPS R andomized PredictiveSystem\nRRPM R idge Regression Prediction Machine\nSCPS S plit Conformal PredictiveSystem\nSVM S upport Vector Machines\nVA Venn-Abers\nVP Venn Prediction",
        "cf8e4728-7986-46aa-8762-6a16bb12a108": "18",
        "bc5c3845-985d-4643-a5b9-ee6a5a03cadf": "19\nChapter 1\nIntroduction\n1.1 Machine learning\nMachine learning is concerned with the development of data-driven algorithms that are\nable to make predictions about something in the world. Tom M. Mitchell defined a ma-\nchine learning algorithm as follows: \u201cA computer program is said to learn from experi-\nenceE withrespecttosomeclassoftasks T andperformancemeasure P ifitsperformance\nat tasks inT, as measured byP, improves with experienceE\u201d [92].\nMachine learning applications range from identifying handwritten digits in postal\ncodes for e\ufb00icient sorting of mail [78] to filtering of spam and predicting property prices.\nRecent progress in deep learning (a branch of machine learning based on artifical neu-\nral networks) resulted in human or near-human performance in computer vision, speech\nrecognition and machine translation. This progress is the result of application of new al-\ngorithms for training advanced deep neural network architectures using abundant data\nand high-performance Graphics Processing Units (GPUs) that are able to run parallel\ncomputationsathighspeed. Incomputervision, althoughconvolutionalneuralnetworks\n(CNNs) have been around for decades, faster GPU-based implementations of backprop-\nagation algorithm popularised by Hintonet al.[114] for training of deep neural networks\nallowed to achieve near-human performance in visual pattern recognition contests such\nas ImageNet competition [63].",
        "e718bd53-be56-461e-85d9-f3bdb6ac3132": "20 Chapter 1. Introduction\n1.1.1 Terminology of machine learning\nMachine learning consists of several branches that include supervised learning, unsuper-\nvisedlearningandreinforcementlearning. Insupervisedlearning,anintelligentsystemis\npresented with examples of inputs (objects) and outputs (labels), and the goal is to learn\na function that maps inputs to outputs. In unsupervised learning no labels are available\n\u2014 the goal of an unsupervised learning algorithm is to learn the structure of the data.\nThe output of an unsupervised learning algorithm is either the assignment of objects into\nclusters, the density estimation or the projection of data into a lower dimensional space.\nReinforcement learning is concerned with the behaviour of intelligent agents maximising\ntheir reward function by taking actions in an environment. This thesis is concerned with\nprobabilistic prediction for supervised machine learning problems only.\nIn supervised machine learning problems, an algorithm is presented with a number\nof examplesz from the training set of examplesZ. The examples contain objectsx from\nthe setX and labelsy from the setY. The Cartesian productZ := X \u0002 Y represents the\nset of all possible examples. Supervised machine learning consists of two main classes\nof problems: classification and regression. In a classification problem, the labels consti-\ntute a discrete set that can be as small as the set of numbers from0 to 1 in a standard\nbinary classification problem, several classes in a multi-class classification problem (as in\nMNIST dataset [78] containing examples of objects0 to 9 representing hadwritten digits)\nor as large as tens of thousands classes (as inImageNet [63] competition that contains\nover twenty thousand categories of images). In a machine learning regression problem,\nthe labels are continuous \u2014 the objective of the machine learning regression algorithm is\nto learn the function that maps objects to continuous labels. Examples of machine learn-\ning regression tasks include making predictions about product demand, property prices,\ndrug e\ufb00icacy or toxicity and the lengths of taxi journeys.\n1.1.2 History of supervised learning\nEven though artificial intelligence and machine learning have seen significant develop-\nments since their start in the 1950s and 1960s, these fields use mathematical tools such as\nBayes\u2019 theorem and Least Squares that date back to the 18th and 19th century. In 1950,",
        "a529d40f-5016-4b59-97ef-77b09b3ef892": "1.2. Probabilistic prediction 21\nAlan Turing proposed a learning machine [130] that could become artificially intelligent\nvia the learning process. In 1951, Marvin Minsky and Dean Edmonds built the first neu-\nral network machine, known as SNARC [115]. In 1957, Frank Rosenblatt caused a lot of\nexcitement in the artificial intelligence community by building the perceptron [113]. It\nquickly turned out, however, that the perceptron was not able to recognise many patterns\nincluding a rather simple XOR function as demonstrated in 1969 by Minsky and Papert\n[91]. In 1967, Cover published a paper describing the Nearest neighbour algorithm [23].\nThe Nearest neighbour algorithm provided computers with the ability to perform basic\npatternrecognitiontasks. In1986,DavidRumelhart,GeoffHintonandRonaldJ.Williams\npopularised the method of backpropagation for training of neural networks [114] (con-\ntinuous backpropagation was initially derived by Henry J. Kelley [59] in the context of\ncontrol theory). A number of more powerful supervised machine learning algorithms\nincluding random forest [9] and support vector machines (SVMs) [22] were published\nin the 1990s (the original linear support vector machines were invented and published\nby Vladimir Vapnik and Alexey Chervonenkis in the early 1960s [133]. In 1997, Sepp\nHochreiterandJ\u00fcrgenSchmidhuberpublishedlongshort-termmemory(LSTM, [ 52])\u2014\nanartificialrecurrentneuralnetworkarchitecturethatusedfeedbackconnectionstosolve\ntheproblemofvanishinggradient\u2014theissuethatimpededtheeffectiveuseoftheLSTM\npredecessor \u2014 traditional recurrent neural networks (RNNs).\n1.2 Probabilistic prediction\nPrediction is about forming forecasts about the future, whilst probabilistic prediction is\nabout quantifying the uncertainty in prediction [40]. Forecast uncertainty quantification\ngoes back to year 1906 when Australian meteorologist W.E. Cooke attempted to formu-\nlate weather forecasts in terms of probabilities, however the concept of odds was used in\nweather forecasts for more than 200 years [96]. In 1793, the British forecaster J. Dalton\nissued forecasts that included statements such as \u201cthe probability of a fair day to that of a\nwetoneisas10:1\u201dandinFranceJ.B.Lamarckproducedforecastsusingthetermprobable\nthat implied a probability of an event greater than 0.5 [96].",
        "02e35ff5-605a-4cea-a7d1-65f150bdcc4f": "22 Chapter 1. Introduction\nProbabilistic forecasts are an important component of an optimal decision-making\nprocess [40] and are used to quantify uncertainty in such diverse domains as forecasting\nweather and election results, demographic projections and financial risk management.\nCentral banks worldwide adopted probabilistic forecasting following the Bank of Eng-\nland\u2019s use of probabilistic models for interest rates for almost three decades [40].\nMorerecently,estimatingpredictiveuncertaintyhasbecomevitallyimportantinmany\napplications, such as healthcare, drug development and self-driving cars (see, e.g., [4],\n[105], [129], [60]).\nMore recently Stanford statisticians and Washington Post data scientists built more\nhonestelectionpredictionmodelsusingconformalprediction. AccordingtoStanfordUni-\nversity Prof. Emmanuel Candes: \u201cPredictive models are used to make decisions that can\nhave enormous consequences for people\u2019s lives. It\u2019s extremely important to understand\nthe uncertainty about these predictions, so people don\u2019t make decisions based on false\nbeliefs\u201d [131].\nThe Stanford-Washington Post model is the \u201cfirst real-world application of an exist-\ning statistical technique developed at Stanford by Cand\u00e8s, former postdoctoral scholar\nYaniv Romano and former graduate student Evan Patterson. The technique is applicable\nto a variety of problems and, as in the Post\u2019s predication model, could help elevate the\nimportance of honest uncertainty in forecasting. While the Post continues to fine-tune\ntheir model for future elections, Cand\u00e8s is applying the underlying technique elsewhere,\nincluding to data about COVID-19\u201d (see [131], [112]).\nProbabilistic prediction is rapidly gaining momentum in both academic research and\npractical applications \u2014 the recentM4 [82] andM5 forecasting competitions [83] required\nestimatingpredictionintervals(inadditiontoproducingpointforecasts)foralargenum-\nber of time-series. Large tech companies such as Amazon [39] and Uber [71] are actively\ninvolved in probabilistic forecasting research and have incorporated probabilistic fore-\ncasting into the core of their machine learning systems. Probabilistic forecasting is used\nin production systems to provide large-scale retail demand predictions at Amazon and\ncapacity forecasting at Amazon Web Services (AWS), whilst Uber uses probabilistic fore-\ncasting for marketplace forecasting of demand and resource allocation. Companies like",
        "9751d48b-4cc0-4a8b-a9f0-89c6e43d22b5": "1.2. Probabilistic prediction 23\nWalmart have embraced machine learning and predictive uncertainty estimation and co-\norganised the most recentM5 forecasting competition [83] that included two tracks to\nestimate both accuracy and uncertainty in the forecasts of future Walmart product sales.\nThe estimation of predictive uncertainty becomes particularly important when it is\nnecessary to determine a case for manual intervention, especially in safety-critical appli-\ncations such as healthcare [105] and self-driving cars [34] .\n1.2.1 Terminology of probabilistic prediction\nWe will define the terminology of probabilistic prediction following Gneiting and Katz-\nfuss [40]. The etymology of words \u201cpredict\u201d and \u201cforecast\u201d is different. Whilst the word\n\u201cprediction\u201d comes from the Latin word \u201cpraedicatus\u201d (relating to foretelling or proph-\nesy), the meaning of the word \u201cforecast\u201d comes from the Middle English word for \u201cfore-\nthought\u201d or \u201cprudence\u201d. When compared to prediction, forecast, therefore, implies more\ncontrol over the planning for the future. However, we will use the terms \u201cprobabilistic\nprediction\u201d and \u201cprobabilistic forecasting\u201d interchangeably.\nThe general objective of probabilistic forecasting is to \u201cmaximise the sharpness of a\npredictivedistributionsubjecttocalibration\u201doftheprobabilisticforecast[ 40]. Thesharp-\nness is defined by the concentration of the probabilistic forecast, whilst calibration de-\nscribes the relationship between forecast probabilities and the relative frequency of ob-\nservedevents. Anexampleofaprobabilisticpredictionisastatementthat\u201crainwilloccur\nwithaprobabilityof70%\u201dandiftherelativefrequencyofrainthatactuallyoccursduring\nthe days when rain is predicted to occur turns out to be 70%, we say that the probabilistic\nforecast is well calibrated (see, e.g., [26]). We introduce a number of formal definitions\n(for more details about proper scoring rules and terminology of probabilistic forecasting\nin general see Gneiting and Katzfuss [40]). The definitions below follow [40].\nDefinition 1.2.1(Probability integral transform). Given the observationY , the probabil-\nityintegraltransform(PIT)istherandomvariable ZF = F(Y ),where F isthecumulative\ndistribution function (CDF) forY .\nDefinition 1.2.2 (Forecast calibration). Let F and G be CDF-valued random quantities\nwith PITsZF and ZG:",
        "fdec4acc-384f-41bc-9421-dd237f191c1f": "24 Chapter 1. Introduction\n\u2022 The forecastF is marginally calibrated ifE(F(y)) = P(Y \u0014 y) for ally 2 R.\n\u2022 The forecastF is probabilistically calibrated if its PITZF has a standard uniform\ndistribution.\nFrom the properties of standard uniform distribution, it follows that ifF is probabilis-\ntically calibrated thenvar(ZF ) = 1\n12 and F is well dispersed [40]. The most commonly\nused proper scoring loss is the logarithmic loss:\nDefinition 1.2.3(Logarithmic loss).\nLL(f, y) = \u0000log f(y)\nIn addition to the logarithmic loss, for machine learning classification problem, we\nwill also use Brier loss (see [10], [6]).\nDefinition 1.2.4(Brier loss).\nBL(f, y) =\nX\ny\u2032\u2208Y\n\u00001{y\u2032=y} \u0000 P \u0000\by\u2032\t\u0001\u00012\nBoth the logarithmic loss and the Brier loss are examples of proper loss functions for\nclassification problems. The continuous ranked probability score (CRPS) [40] is an ex-\nample of proper loss function for the regression problems.\nDefinition 1.2.5(Continuous ranked probability score).\nCRPS(F, y) =\nZ \u221e\n\u2212\u221e\n(F(x) \u0000 1fy \u0014 xg)2dx\nThe CRPS generalizes the absolute error and in the case of point forecast is the same\nas the absolute error. The CRPS can be used to compare probabilistic forecasts [40].\n1.2.2 Conformal prediction\nInasupervisedmachinelearningproblem,theobjectiveistopredictthelabelbasedonthe\nfeatures of an object. However, given a point prediction\u02c6y, how can one evaluate predic-\ntion quality (i.e., how likely is it that\u02c6y = y)? In practice, such evaluation can be done on",
        "641075e3-d586-4844-b56a-9c5a76361f91": "1.3. Main contributions 25\nthe basis of how such predictions performed in comparison with the actual results based\nonpastexperience. Thecentralideaofconformalpredictionistousesuchpastexperience\nto determine precise levels of confidence in the individual predictions [120]. Conformal\nprediction is the statistical learning framework that is able to provide measures of confi-\ndence for each individual prediction, thus effectively \u201chedging\u201d the predictions [137].\nThe prediction region\u0393\u03b5 is constructed using the significance level\u03f5 2 (0, 1) with a\nspecified degree of confidence. To construct prediction region, the conformal predictor\nestimates degree of similarity of a new example to the examples in the training set using\nspecifiedconformitymeasure,thisconformitymeasureisthenusedtoproduceprediction\nregion \u0393\u03b5 for each significance level\u03b5 [120].\nTheideasofconformalpredictiongobacktothe1960sasKolmogorov\u2019sthinkingabout\nthemeaningofrandomness[ 2]andlatertothe1970sandthe1980sason-linecompression\nmodels[120]thatwerestudiedusingtheideasfromthestatisticalmechanics[ 72]. Inboth\nfields of study the models use summary statistics that contain all the useful information\nto predict future examples [120]. When a new example is observed, summary statistics\nis updated and \u201cthe probabilistic content of the structure is expressed by Markov kernels\nthat give probabilities for summarized examples conditional on the summaries\u201d [120].\nSpecifically, given a set ofn examples x1, y1, . . . xn\u22121, yn\u22121, xn and a significance level\n\u03f5, the conformal predictor outputs the prediction set\n\u0393\u03f5 (x1, y1, . . . xn\u22121, yn\u22121, xn) .\nThe prediction set \u0393\u03f5 must satisfy the property \u0393\u03f51 (x1, y1, . . . xn\u22121, yn\u22121, xn) \u0012\n\u0393\u03f52 (x1, y1, . . . xn\u22121, yn\u22121, xn) whenever \u03f51 \u2a7e \u03f52, i.e., the larger subset of possible labels\nresults in a more confident prediction.\n1.3 Main contributions\nThegoaloftheresearchdescribedinthisthesisistodevelopnewmethodsforprobabilistic\nprediction for machine learning classification and regression tasks.",
        "5609ce6a-0093-458c-bc0f-6a0d59437b6d": "26 Chapter 1. Introduction\n\u2022 Chapter 2 describes calibration methods, including recently developed compu-\ntationally e\ufb00icient algorithms IVAP (inductive Venn-Abers predictor) and CVAP\n(crossVenn-Aberspredictor). Theremainderofthischapterwilloutlinenewmeth-\nods for probabilistic classification and regression. These new methods further de-\nvelop current state-of-the art in probabilistic prediction.\n\u2022 Chapter3describesanovelmethodofapplicationofIVAPsandCVAPs(probabilis-\ntic algorithms for binary classification) to multi-class classification and studies its\nperformance empirically in comparison with traditional calibration methods such\nasPlatt\u2019sscalingandisotonicregression. Thisworkwaspresentedat\u201cThe6thSym-\nposiumonConformalandProbabilisticPredictionwithApplications\u201d(COPA2017,\nStockholm). The conference paper \u201cMulti-class probabilistic classification using in-\nductive and cross Venn\u2013Abers predictors\u201d by Manokhin [85] introducing a novel\nmethod of application of IVAPs and CVAPs to the problem of multi-class classifica-\ntion and studying the performance of the new algorithm in comparison with tradi-\ntional calibration methods such as Platt\u2019s scaling and isotonic regression has been\npublished in the \u201cProceedings of Machine Learning Research.\u201d The code has been\nreleased as open source [86].\n\u2022 Chapter 4 focuses on the study of probabilistic regression and derives predictive\ndistribution functions that are valid under a non-parametric assumption. A new\nprediction algorithm, called the Least Squared Prediction Machine (LSPM), is in-\ntroduced to generalize the classical Dempster-Hill predictive distribution to regres-\nsion problems. This work waspresented at \u201cThe 6th Symposium on Conformal and\nProbabilistic Prediction with Applications\u201d (COPA 2017, Stockholm). A conference\nversionofthepaper[ 145]hasbeenpublishedinthe\u201cProceedingsofMachineLearn-\ning Research.\u201d. The journal paper \u201cNonparametric predictive distributions based\non conformal prediction\u201d by Vovk, Shen, Manokhin, Xie [146] describing a novel\nmethod of deriving predictive distributions that are valid under a nonparametric\nassumption has been published by \u201cMachine Learning\u201d (Springer).\n\u2022 Chapter 5 studies theoretically and empirically conformal predictive distributions",
        "1ec8bbb9-23b6-4113-bce6-e36a140e32fd": "1.3. Main contributions 27\nwith kernel methods. Kernel Ridge Regression Prediction Machine (KRRPM) is in-\ntroduced and its properties are studied. The main advantage of the KRRPM is its\nflexibility: for a suitable kernel, it gets the location and shape of the predictive dis-\ntribution right in the case of homoscedasticity. This work resulted in publication\nas a chapter \u201cConformal predictive distributions with kernels\u201d in the book \u201cBraver-\nman Readings in Machine Learning. Key Ideas from Inception to Current State\u201d\n(Springer) by Vovk, Nouretdinov, Manokhin and Gammerman [139] with preprint\nversion published on arXiv [139].\n\u2022 Chapter6introducestwonovelcomputationallye\ufb00icientversionsofconformalpre-\ndictive distributions: split and cross conformal predictive distributions, and dis-\ncusses their advantages and limitations. This work has been presented at \u201cThe 7th\nSymposium on Conformal and Probabilistic Prediction with Applications\u201d (COPA\n2018,Maastricht). Theconferencepaper\u201cCross-conformalpredictivedistributions\u201d\n[140] has been published in the \u201cProceedings of Machine Learning Research.\u201d The\njournal version \u201cComputationally e\ufb00icient versions of conformal predictive distri-\nbutions\u201d by Vovk, Petej, Nouretdinov, Manokhin and Gammerman has been pub-\nlished in \u201cNeurocomputing\u201d [143] (pre-print version published on arXiv [142]).\nThis journal version of the conference paper [140] contains more detailed compari-\nsonof SCPS and CCPS,a detailed discussion of Venn-Aberspredictivesystems, and\nthe analysis of universality of various predictive systems. An important finding in\nthepaperisthatSCPSandCCPSareuniversal, whereasVenn\u2013Aberspredictivesys-\ntems are not (Venn-Abers predictive systems introduced in Nouretdinovet al.[101]\nare not classical Venn predictors in a classification setting, but are rather the appli-\ncation of Venn Predictors to any regression method to obtain calibrated predictive\ndistributions).\n\u2022 Created \u201cAwesome Conformal Prediction\u201d [84] \u2013 the most comprehensive profes-\nsionally curated resource on conformal prediction. \u201cAwesome Conformal Predic-\ntion\u201dhasbeencitedinoneofthemostprominentbooksonMachineLearning\u201cProb-\nabilistic Machine Learning: An Introduction\u201d [97] by the leading research scientist\nat Google Kevin Murphy (over 80K Google Scholar citations). The resource has",
        "128aae5c-13ab-40f1-9338-178b5a5c058a": "28 Chapter 1. Introduction\nproven very popular with both the academic and the tech community and accu-\nmulated hundreeds of stars on Github in just a few months raising awareness about\nconformal prediction globally. \u201cAwesome Conformal Prediction\u201d is the o\ufb00icial con-\nformal prediction for the ICML2022 \u201cWorkshop on Distribution-Free Uncertainty\nQuantification\u201d and has been featured in the most popular tutorial on conformal\nprediction \u201cA Gentle Introduction to Conformal Prediction and Distribution-Free\nUncertainty Quantification\u201d [1].",
        "8439118e-24c1-4b12-b944-7a0c86ee1705": "29\nChapter 2\nLiterature review\nThis chapter introduces probabilistic machine learning and describes some of the calibration meth-\nods, both classical and modern, including recently developed computationally e\ufb00icient algorithms\nIV AP (inductive Venn-Abers predictor) and CV AP (cross Venn-Abers predictor). The aim is to\nsummarise the existing approaches, both parametric and non-parametric, which will serve as the\nbasis of comparison for the methods developed in Chapter 3.\n2.1 Introduction\nThe objective of a supervised machine learning classification task is to predict labels for\nnew and unseen objects from the test set. A desirable property of a machine learning\nalgorithm is its ability not only to classify a new object into the correct class, but also to\ngenerate accurate, reliable and robust class probabilities. By assigning class probabili-\nties to the new object, the machine learning algorithm is able to effectively \u201chedge\u201d its\npredictions by complementing class prediction with measures of prediction uncertainty /\nprediction quality. This becomes especially critical in many real-world applications such\nas healthcare [105] and self-driving cars [94], where obtaining accurate class probabili-\ntiessignificantlyaffectscriticalelementsofadecision-makingprocess, suchaswhetherto\nstop a self-driving car when the algorithm is unsure about its prediction of whether there\nis a pedestrian or obstacle on the road or not [94].\nPredictiveclassificationmodelsaremostcommonlyevaluatedbylookingattheirabil-\nity to separate objects into correct classes, this is often evaluated using receiver operating",
        "d7299c50-ce78-4d67-9b23-6776041c7df9": "30 Chapter 2. Literature review\ncharacteristic (ROC-AUC) that are able to provide more comprehensive measure of per-\nformance when compared to accuracy and error rate [36]. Calibration is another impor-\ntant criteria of performance that has received less attention in both academic literature\nand practice, but is becoming increasingly important due to the need to provide predic-\ntions in an accurate, fair and transparent manner. A model is said to be well calibrated\nif its output reflects true probabilities of the events \u2014 for every 100 objects assigned a\nprobability score ofs, close tos will belong to class with label 1 [14].\nIt is often assumed, that traditional neural networks are well-calibrated (see, e.g.,\nCaruana and Mizil [17], however, there is a growing body of research demonstrating that\nnot only modern deep neural networks are not well-calibrated (see, e.g., [44], [89], [94],\n[105]), but also that traditional neural networks are often mis-calibrated as well [57]. As\nshown in [44], whilst very large and deep computer vision models such as ResNet [45]\nare much more accurate than previous classical architectures such as LeNet [77], modern\ndeep neural networks become significantly miscalibrated even as classification accuracy\nimproves. Guo et al. [44] attribute this behaviour to recent advances in deep learning\nsuch as model capacity, batch normalization, weight decay that all have strong negative\neffects on network calibration.\nMessoudi et al. [89] considered Bayesian approach to estimation of confidence of the\npredictions of machine learning models, however concluded that such methods have ma-\njor limitations. In order to accurately estimate posterior distributions, Bayesian methods\nneedcorrectpriordistributions,howeverinpracticesuchpriordistributionsareoftencho-\nsenarbitrarilyorsimplynotavailable. Ifanincorrectpriorischosen, posteriorprobability\nwillnotresultinavalidforecast(see,e.g,[ 88])andpredictiveintervalwillnotcontainac-\ntual labels (\u201cground truth\u201d) with specified probability (e.g., 95% predictive interval will\nnot contain 95% of the actual labels), resulting in predictive intervals being too wide or,\nmost likely, too narrow (overconfident predictions) \u2014 as has been demonstrated in mul-\ntiple forecasting competitions (see, e.g, [43]). In addition, in novel situations such as new\ndrug development it is not possible even for an expert to arrive at correct priors \u2014 a situ-\nation well-known to regulators who discourage using subjective opinions not backed by\nactual data, especially in regulated industries such as pharmaceuticals, financial services",
        "f41498de-6bc7-4c5e-8dcc-a1bdda0f6270": "2.1. Introduction 31\nand healthcare.\nMelluish et al.[88] demonstrated that Bayesian methods give misleading predictions\nand incorrect prediction intervals when the assumptions of the Bayesian model are incor-\nrect. The authors concluded that \u201cwhen correct priors are known, Bayesian algorithms\ngive optimal decisions, and accurate confidence values for predictions can be obtained.\nHowever, ifthepriorisincorrect, theseconfidencevalueshavenotheoreticalbase\u2014even\nthough the algorithm\u2019s predictive performance may be good. Bayesian frameworks are\noften applied to these algorithms in order to obtain such values, however they can rely\non unjustified priors.\u201d\nMukhoti et al. [94] analysed miscalibration of deep neural networks and concluded\nthat many current multi-class classification networks are poorly calibrated, resulting in\nthe mismtach between the model\u2019s confidence and accuracy. The authors concluded that\nthe high capacity of these networks leaves them vulnerable to overfitting on the negative\nlog-likelihood (NLL) loss they conventionally use during the training and replaced the\ncross-entropy with focal-loss to prevent the model from becoming overconfident.\nOutside of the deep learning domain \u2014 in forecasting \u2014 the recent \u201cM4\u201d forecasting\ncompetition [82] is a typical example of the majority of submitted models outputting un-\ncalibrated predictions. According to [43] \u201cprediction intervals became overconfident and\nincreasingly so over the long horizons.\u201d The author concluded [43] that \u201cmany of the\nmodels submitted performed poorly (except, notably, for the top two or three submis-\nsions), in that they often performed worse than the benchmarks provided by the com-\npetition organisers. A possible explanation for this poor performance might be a mix of\noverconfidence and overfitting.\u201d\nMessoudi et al. [89] used conformal prediction to develop more reliabe and cautious\ndeep learning models and concluded that \u201cthe conformal model not only adds reliability\nand robustness to the deep model by detecting ambiguous examples, but also keeps or\neven improves the performance of the basic deep model when it predicts only one class.\nWe also illustrated the ability of conformal prediction to handle noisy and outlier exam-\nples for all three types of data. These experiments show that the conformal method can\ngivemorerobustnessandreliabilitytopredictionsonseveraltypesofdataandbasicdeep",
        "9fbe671a-f52b-4263-88fc-1be6303724da": "32 Chapter 2. Literature review\narchitectures.\u201d\nThe application of conformal prediction to neural networks is not a new idea, Pa-\npadopouloset al.[103]usedVenn-Aberspredictorstoproducewell-calibratedprobabilis-\ntic predictions using neural networks in multiclass setting, by combining Venn predictors\n[137] combined with neural networks applied to binary classifications problems [104].\nPerreiraet al. [105] evaluated various uncertainty quantification methods (described\nin more detail in2.2) to produce individual probabilistic predictions for clinical decisions\nfor patients with Alzheimer decease. Both Platt\u2019s scaling2.2.2 and isotonic regression\n2.2.3 were evaluated and compared to Conformal Prediction and Venn-Abers prediction.\nThe study concluded that Platt\u2019s scaling is only adequate to calibrate the output from the\nSVM, the finding confirmed in several other papers covered in more detail in2.2 . Whilst\nisotonic regression performed better than Platt\u2019s scaling, combining isotonic regression\nwith decision trees produced poor results. Both Conformal Prediction and Venn-Abers\nprediction produced superior results by reducing both false negative and false positives.\nThe study concluded that \u201cConformal Prediciton and Venn-Abers were the preferable\nmethods to complement predictions with measures of uncertainty and conformal pre-\ndiction based methods produced predictions with a low error rate for high credibility\nthresholds.\u201d\nTo summarise, many machine learning algorithms do not produce class membership\nprobabilities and the ones that do often generate classification scores that do not corre-\nspond to class probabilities. In such cases the scores need to be transformed into well-\ncalibrated probabilities that can be combined with utility scores for effective decision-\nmaking.\n2.2 Calibration methods\nTo transform class scores into class probabilities, several methods have been invented.\nSuchmethodsincludebothparametricandnon-parametricmethodsandaresummarised\nbelow.",
        "70d2ac51-62e5-4a08-a91c-5cc543a19de9": "2.2. Calibration methods 33\nIn a binary classification problem, the objective of the calibration process is to convert\nclass scores from an underlying machine learning classifiers = f(x) into calibrated prob-\nabilities using the calibration map\u00b5. A perfectly calibrated classifiers = f(x) is defined\nas follows (see, e.g., [44]):\nDefinition 2.2.1(Perfectlycalibratedclassifier) . Ascoringclassifierisperfectlycalibrated\nif: P( \u02c6Y = Y j \u02c6P = p) = p, 8p 2 [0, 1], where \u02c6Y is class prediction and\u02c6P is confidence\nassociated with class prediction [44].\nIntuitively this means that we would like for model output to represent true probabil-\nity of classes.\n2.2.1 Histogram binning\nHistogram binning is an approach proposed by Zadrozny and Elkan in [152]. In this\napproach, the scores are sorted by value and the set of scores is divided into a number of\nbins. The value of each bin is then selected by solving an optimisation problem in which\nsquarederror(e.g., theBrierloss)isminimizedbyreplacingeachobject\u2019sscorewithwith\nthe value of the bin assigned to that object. The disadvantages of the histogram binning\nmethod include arbitrary choice of the number of bins and the fixed size of the bins [99].\nIn addition, the mapping function is not a continuous function and there is no guarantee\nthat the mapping function is non-decreasing function of the object scores.\n2.2.2 Platt\u2019s method\nPlatt\u2019smethod,alsoknownas\u201cPlatt\u2019sscaling\u201dor\u201cPlattcalibration\u201dwasdesignedtolearn\nthe function mapping classification scores produces by the SVM classifier to probability\ndistribution over classes. The method (described by Platt in [109]) was invented to ad-\ndress specific disadvantage of a very popular classification algorithm \u2014 support vector\nmachines[22]\u2014thatproducedaccurateassignmentoftheobjectstoclasses,butrequired\nfurther calibration of the class scores to align them with true class probabilities. Support\nvector machines (SVM) do not produce class probabilities direct, but instead output un-\ncalibrated classification scores that reflect distance to the maximum-margin hyperplane.",
        "b4e7c314-ed97-485d-9227-9ac54fa0025d": "34 Chapter 2. Literature review\nPlatt observed that the relationship between classification scores produced by the SVM\nand empirical probabilities tended to be of the sigmoid nature that can be desribed by the\nparametrized sigmoid function:\nP(y = 1 jx) = 1\n1 + exp( Ax + B). (2.1)\nThis is equivalent to the assumption that classification scores from the SVM are propor-\ntional to the log-odds based on the probability of an object belonging to the positive class\n[109]:\ng(x) = log P(y = 1 j x)\nP(y = 0 jx) . (2.2)\nThe probability of the positive class can then be expressed as:\nP(y = 1 jx) = 1\n1 + e\u2212g(x) . (2.3)\nPlatt\u2019s suggestion was to estimate parameters A and B by fitting maximum likelihood\nestimator on a new training set(xi, ti) [109] constructed as follows:\nti = yi + 1\n2 .\nThe parameters A and B can then be estimated by minimizing negative log-likelihood\n(LL) on the new training set:\nLL = \u0000\nX\ni\nti log (pi) + (1 \u0000 ti) log (1 \u0000 pi)\nWhere pi is probability of positive class described by (2.3). As a regularized alternative\nmethoddesignedtoaddresspotentialoverfitringbythesigmoidfunction,Platt[ 109]sug-\ngested to modifyti as follows:\nt+ = N+ + 1\nN+ + 2\nt\u2212 = 1\nN\u2212 + 2 .",
        "0146c9f0-7d15-446a-b1e9-9b00136b1471": "2.2. Calibration methods 35\nPlatt [109] demonstrated that that this method produces estimates of probabilities that\nare as accurate as in the earlier method suggested by Vapnik [134] that has mapped the\noutput of the SVM to class probabilities on the basis of feature space decomposition.\nZhang [154] proposed piecewise logistic regression as an alternative to Platt\u2019s [109]\nmethod. The method extends logistic regression by replacing linear function in Platt\u2019s\nsigmoid (2.1) with a piecewise linear function. Such an approach results in the log-odds\n(2.2) also becoming piecewise linear, thus providing additional flexibility in comparison\nwith the original linear functionAx + b used by Platt. The use of piecewise linear func-\ntionallowsforbetter separationof objects intothe threeclasses \u2014 twoclasses where class\nownership is clear and the borderline cluster of objects where there is less certainty as to\nwhich class an object might belong. The authors showed [154] that the piecewise logis-\ntic regression performed significantly better than Platt\u2019s method across a number of text\ncategorization datasets using logistic loss as an error metric.\nWhilst Platt\u2019s method is often used in practice, its mapping function is tailored to the\nveryspecificcaseofSVMasunderlyingmachinelearningclassifierandasshownin[ 105]\nisunsuitabletocalibrateoutputfromclassifiersotherthanSVM.Asshowninmorerecent\nresearch [65] (whilst not explicitely stated in Platt\u2019s original paper [109]), Platt\u2019s scaler\u2019s\nassumptions are very restrictive and are not reasonable for many probabilistic classifiers\nthat output scores in the range [0,1]. In such situations the use of Platt\u2019s scaler results in\nmodelmismatch andcan result inclassifiers becomingless calibratedevenin comparison\nwith class scores produced by an underlying machine learning classifier [65] (this in turn\nis due to the fact that Platt\u2019s mapping does not contain the identity function).\n2.2.3 Isotonic regression\nTo address some of the above mentioned issues with Platt\u2019s scaler, Zadrozny and Elkan\n[153] proposed to use a non-parametric calibration method based on the isotonic regres-\nsion [7] and applied it to a range of datasets using Na\u00efve Bayes and support vector ma-\nchinesasunderlyingclassifiers. ZadroznyandElkan[ 153]demonstratethatfortheNa\u00efve\nBayesthesigmoidassumptionusedbyPlattdoesnotgenerallyfittheshapeofclassscores\nproduced by classifiers other than SVM.",
        "91e7f59a-6fe9-4a2f-b94d-c61198374240": "36 Chapter 2. Literature review\nTo correct this issue, Zadrozny and Elkan proposed a method that is an intermediate\napproach between Platt\u2019s sigmoid and histogram binning. Assuming that the underlying\nclassifier ranks objects correctly (as will be shown in this section, this is a rather limiting\nassumptionwhichresultsinthemajordrawbackoftheisotonicregressionasacalibration\nmethod), the function mapping class scores to probabilities should be non-decreasing.\nIn statistics such an approach is known as isotonic regression [7]. For fitting isotonic\nregression,ZadroznyandElkanusepair-adjacent-violators(PAV)algorithm[ 5],thisnon-\nparametric algorithm solves fitting problem in linear timeO(N) by computing stepwise-\nconstant isotonic function using general mean-squared error as an error metric [153].\nIsotonic regression allocates class scores into bins using the convex hull of the ROC\ncurve [65]. The slope of each segment is related to the empirical likelihood ratio. Cali-\nbrated posterior probabilities can thus be derived by using computed slopes of each seg-\nment. The monotonic mapping of isotonic regression relies on the underlying classifi-\ncation algorithm (whether machine learning or statistical) producing correct ranking of\nclassification scores \u2014 this in turn is equivalent to having ROC AUC score of1 which is\nalmost never possible to obtain on a test set using any underlying classifier unless one\nuses a toy dataset.\n2.2.4 Smooth isotonic regression\nThe mapping function obtained by applying isotonic regression is non-continuous, sim-\nilar to the case of histogram binning. Jianget al. [56] developed a smoother compu-\ntationally effective method by interpolating isotonic regression between representative\nvalues using monotonic splines called \u201cPiecewise Cubic Hermite Interpolating Polyno-\nmial\u201d (PCHIP). In a range of experiments involving both synthetic and real datasets, the\ncalibration results from using smooth isotonic regression were compared with results ob-\ntained using Platt\u2019s scaler and isotonic regression when applied to the output from the\nLogistic Regression. According to the experiments, the smoothing method significantly\nimproved calibration of the LogisticsRegression classification scores compared tothe iso-\ntonic regression [56].",
        "80fbebee-b107-4c12-9f58-68171924bf2e": "2.2. Calibration methods 37\n2.2.5 Nested Dichotomies\nNested Dichotomies (NDs) [76] are a method of transforming a multiclass classification\nproblem into a series of binary classification problems. In this method the set of classes\nis recursively split into subsets using a tree structure, at each node a binary classification\nmodel is used to separate objects into two subsets. Leathartet al. [76] show that nested\ndichotomies exhibit poor calibration, even when the underlying classifiers are well cali-\nbrated. The problem of poor calibration is exarcebated in cases when the underlying bi-\nnary classifiers are not well calibrated. The authors demonstrate [76] that both accuracy\nand calibration (as measured by the Log Loss) can be improved by calibrating both the\nunderlying binary classification models and the full ND structure. By performing exper-\niments on a range of datasets using Na\u00efve Bayes and boosted trees as base classifiers, the\nauthors demonstrate that predictive performance of NDs can be substantially improved\nby applying calibration techniques [76].\n2.2.6 Beta calibration\nBeta calibration is a method developed in Kullet al. [65]. The authors demonstrate that\nparametric assumptions in Platt\u2019s scaler are equivalent to assuming that the scores out-\nputted by the underlying classifier are normally distributed and have the same variance\n(\u03c32) for each class. In addition, experiments in [65] show that using Platt\u2019s scaling to cal-\nibrate output of many classifiers including AdaBoost and Na\u00efve Bayes can result in prob-\nability estimates that are worse than original scores when score distributions have heavy\ntails. Using Platt\u2019s scaling (also known as logistic calibration) as a mapping function for\nwell calibrated cases can result in distortion as logistic function does not include the iden-\ntity function. Whist Platt\u2019s scaling is a parametric method that requires less data than\nisotonic regression (as non-parametric method isotonic regression can overfit on small\ndatasets), logistic calibration can produce bad results when the data model is misspeci-\nfied. The authors demonstrate [65] that \u201cmodel mismatch is a real danger for a range of\nwidely used machine learning models and can make classifiers less calibrated.\u201d\nThe normality and homoscedasticity assumptions in Platt\u2019s scaler are not reasonable",
        "5a5a69ac-d6b5-49d9-b101-6d02615a5c13": "38 Chapter 2. Literature review\nfor many probabilistic classifiers that output scores in the range [0,1] (whilst normal dis-\ntributions have infinite support). To address this, the authors propose to use beta distri-\nbution that allows for modeling of rich class of mapping functions.\nIntheexperimentsKull et al.[65]applylogistic(Platt\u2019s)scaler,isotonicregressionand\nbeta calibration to the class scores produced by a range of underlying classifiers, includ-\ning Na\u00efve Bayes, Adaboost, logistic regression, support vector machines, random forest\nand multi-layer perceptron on 41 datasets from the UCI data repository [33]. The results\nshowed that Beta-calibration performed well on a range of dataset sizes and a variety\nof classifiers with different characteristics. In terms of both the Log loss and the Brier\nloss, beta calibration was often the best calibrator when compared to uncalibrated scores,\ncalibrated scores produced by Platt\u2019s scaler and isotonic regresison. It was also never the\nworstcalibrator,makingitagoodchoicewhencomparedtoisotonicregressiononsmaller\ndatasets and in general often outperforming Platt\u2019s scaler.\n2.2.7 Scaling-binning\nScaling-binning calibrator [66] is a combination approach that uses both histogram bin-\nningandparametricmethods. Astraditionalneuralnetworks[ 57]andmostmoderndeep\nlearning architectures do not output calibrated probabilities out of the box and are mis-\ncalibrated (see, e.g\u201e [44], [64], [94]), researchers and machine learning practitioners use\neasy scaling approaches such as Platt\u2019s scaling [109], isotonic regression [152] or temper-\nature scaling [44]. As shown in [66] such approaches are in practice less calibrated than\nreported in the literature, in addition such approaches \u201chave fundamental limitation as\ntheir true calibration error can not be measured using a finite number of bins\u201d [66].\nIn the scaling-binning approach a simple functiong is first fit to the scoress outputted\nby the underlying machine learning classifier. The input space is then binned with equal\nnumber of inputs ending up in the same bin. As a final step, the values ofg are averaged\nacross each bin. The resulting function can be computed more e\ufb00iciently than in binning\nmethods and as shown in [66] has lower variance than labelsy.\nIn multiclass calibration experiments run onCIFAR-10 [62] andImage-Net [63] com-\nputer vision datasets, scaling-binning calibrator achieved a lower calibration error (35%",
        "09015598-9dce-4b65-a670-4e1b3eb09102": "2.2. Calibration methods 39\nlower onCIFAR-10 and 5x lower on Image-Net) than histogram binning [66].\n2.2.8 Probability Calibration Trees\nProbability calibration trees (PCT) [75] is a local probability calibration approach that\nidentifies regions in the input space to learn local calibration models in order to improve\nperformance.\nThe most commonly used methods such as Platt\u2019s scaling and isotonic regression cali-\nbrate classification scores globally across the object space. Leathartet al.[75] hypothesise\nthat this approach can be improved using a more fine-grained calibration model and pro-\npose a novel approach based on logistic model trees [69]. The PCT approach follows\n[69] by combining decision trees with logistic regression. Such an approach results in an\nadaptivemodelwhereforsimpledatasetsalinearmodelmightgivethebestperformance,\nwhilst for more complicated datasets require a complex tree to be built [75].\nIn the experiments run on 32 UCI datasets [33] probability calibration trees either\nperformed in line with or outpeformed both Platt\u2019s scaling and isotonic regressions when\nNa\u00efve-Bayes was used as the base classifier. For other base classifiers such as boosted\nstumps, boosted decision trees and SVM, probability calibrations trees performed better\nthan Platt\u2019s scaling and better than isonotic regression for all but 3 datasets.\n2.2.9 Bayesian Binning into Quantiles\nNaeniet al.[100]developedanon-parametriccalibrationmethodcalledBayesianBinning\ninto Quantiles (BBQ) using an ensemble of near isotonic regressions with Bayesian infor-\nmationcriterion(BIC). BBQextendshistogrambinningbycombiningoutputsofmultiple\nbinning models. The models differ in the number of bins, the objects are allocated into\nbins based on equal frequency. Using Expected Calibration Error and Maximum Calibra-\ntion Error defined below, the authors evaluate performace of the BBQ in comparison with\nhistogram binning, Platt\u2019s scaling and isotonic regression using support vector machines\nas the base classifier.",
        "05e4a05e-9fbb-41c8-9a4e-8e4993569fb3": "40 Chapter 2. Literature review\nDefinition 2.2.2 (Expected Calibration Error). Expected Calibration Error is computed\nas:\nECE =\nKX\ni=1\nP(i) \u0001 joi \u0000 eij,\nwhere oi is the fraction of positive labels in bini, ei is the mean of probabilities (either\nbefore or after calibration) in the same bini, and P(i) is the empirical frequency of all\nobjects assigned to bini.\nDefinition 2.2.3(MaximumCalibrationError) . MaximumCalibrationErroriscomputed\nas:\nMCE =\nK\nmax\ni=1\n(joi \u0000 eij) .\nIn the experiments on synthetic and 30 real datasets from [33], BBQ performed com-\npetitively in terms of allocating objects to the correct class and often performed better\nthan Platt\u2019s scaling and isotonic regression in terms of calibration using a range of base\nclassifiers including logistic regression, support vector machines and Na\u00efve-Bayes [100].\n2.2.10 Ensemble of Near Isotonic Regression (ENIR)\nNaeni and Cooper [98] extended the approach in Bayesian Binning into Quantiles (BBQ)\nby addressing the monotonicity assumption of the isotonic regresssion. Whilst isotonic\nregression assumes that ranking of classification scores produced by the underlying clas-\nsifier is correct, in practice this is equivalent to assuming that the area under the ROC\ncurve (AUC) is equal to 1, which rarely happens in practice [99]. In comparison BBQ\ndoes not make such assumption, whilst the further key idea in ENIR is to balance the ap-\nproache taken in isotonic regression to that one in Bayesian Binning into Quantiles [98].\nThe assumption in ENIR is that the mapping function converting uncalibrated scores into\ncalibrated probablities is nearly isotonic by allowing score rank violations by the under-\nlying algorithm and later penalizing them via the use or regularization term [98].\nIn an extensive set of experiments this assumption proved to be realistic and unlike\nthe assumption in the isotonic regression not biased [98]. In two sets of experiments on\n40 randomly selected datasets from the UCI [33] using logistic regression, support vec-\ntor machines and Na\u00efve Bayes as base classifiers significant calibration improvement was",
        "74d2a0fd-0d54-45d6-bf4f-5fc7fa9ef628": "2.2. Calibration methods 41\nobserved using RMSE, ECE and MCE as calibration metrics. In addition ENIR outper-\nformed both the isotonic regression and the BBQ method (Platt\u2019s scaling was not used\ndue to its simplicity and also due to the fact that the BBQ method was already shown to\noutperform Platt\u2019s scaling [100].\n2.2.11 Temperature Scaling\nGuo et al. [44] demonstrated that deep convolutional neural networks (CNNs) are mis-\ncalibrated, with the probabilistic error and miscalibration becoming worse even as clas-\nsification error is reduced during the model training. Guo et al. [44] proposed that\nmodern advances in deep neural network architectures such as model complexity, batch-\nnormalization and early-stopping strongly contributed to miscalibration \u2014 the deep neu-\nral networks become increasingly overconfident even as calibration worsens. To address\nthis issue, Guoet al. [44] propose temperature scaling \u2014 a simple extension of Platt\u2019s\nscaling. Temperature scaling was originally used in statistical physics [54] and was later\npopularized by Hintonet al. [51] in the domain of knowledge distillation in deep neural\nnetworks.\nGiven the logit vectorsi, the new calibrated prediction is\u02c6qi = max k \u03c3 (si/T)(k), where\nk istheclassindexand i istheindexofanobject xi. Theparameter T iscalledthe tempera-\nture as it \u201craises the output entropy\u201d [44]. AsT ! 1, the probability\u02c6qi ! 1/K resulting\nin maximum uncertainty. AtT = 1 the original scoressi are recovered and atT ! 0 the\nprobability becomes that of a point mass [44].\nDespite its simplicity and popularity, the temperature scaling method has a number\nof drawbacks, including dampening of overconfidence in both correct and incorrect pre-\ndictions \u2014 as well as making calibration worse under covariance shift [102].\n2.2.12 Entropy penalty\nPereyraet al. [106] systematically explore regularisation of neural networks. Compared\nto earlier techniques that have been used to reduce overfitting by acting on the activations\nor weights of a neural network (e.g., early stopping, dropout [124], batch normalization\n[53]) the proposed method follows entropy penalty approach by penalizing low entropy",
        "16ca16dd-0480-40c3-af35-3d84a93c5182": "42 Chapter 2. Literature review\noutput distributions \u2014 the same approach that has been successfully used in reinforce-\nment learning to encourage exploration [151].\nOverconfident predictions in deep neural networks are associated with output dis-\ntributions that have low entropy, as shown by Szegedyet al. [127]. By penalizing low\nentropy via regularisation term overconfident (peaked) distributions can be mitigated\nleading to better generalization [106]. Pereyraet al. [106] illustrate the effects of various\ntechniques such as dropout and confidence penalty on the softmax probabilities on the\nMNIST validation set. Whilst dropout leads to overconfident predictions that are either 0\nor 1 (the effects also noted in Guoet al. [44]), both label smoothing and the confidence\npenalty via entropy regularizer lead to smoother output distributions and better regular-\nization [106].\nDefinition 2.2.4(Entropy).\nH (p\u03b8(y j x)) = \u0000\nX\ni\np\u03b8 (yi j x) log (p\u03b8 (yi j x))\nTo construct the loss function Pereyraet al.[106] penalize overconfident distributions\nby adding negative entropy to the negative log-likelihood (NLL) as follows:\nL(\u03b8) = \u0000\u03b2H (p\u03b8(y j x)) \u0000\nX\nlog p\u03b8(y j x)\nIn experimental evaluations of the proposed confidence penalty on common benchmark\ndatasetstheauthorsfindthatconfidencepenaltyimprovestheperformancewithoutmod-\nifying hyperparameters [106].\n2.2.13 Maximum Mean Calibration Error (MMCE)\nMethods like temperature scaling [44] and entropy penalty [106] improve calibration by\nclamping confidence [67], but in the process also penalize correct confident predictions.\nKumaret al.[67]proposeaprincipledapproachtominimizecalibrationerrorusingMax-\nimum Mean Calibration Error (MMCE) \u2014 a kernel based measure that is trained in par-\nallel with minimizing the NLL (negative likelihood loss). The MMCE is minimized at\nperfect calibration and enjoys the properties of consistent and fast convergence [67].",
        "b74400cd-d8ad-4526-9380-1d72b2c0cbdf": "2.2. Calibration methods 43\nIn experiments on datasets spanning images, NLP and time-series data and several\nnetwork architectures Kumaret al. find that MMCE minimised calibration metrics whist\npreserving high confidence predictions [67].\n2.2.14 Label smoothing\nTo address the issue of overconfidence in deep neural networks, Muelleret al. [95] pro-\npose label smoothing approach. Neural network training is sensitive to the loss func-\ntion that is being minimized [95], whilst initially Rumelhart [114] derived backpropa-\ngation calculations using quadratic loss function, later on it was discovered that using\ncross entropy results in better performance and faster convergence (see, e.g, [8]). More\nrecently Szegedyet al. [127] introduced label smoothing \u2014 an approach that computes\ncross entropy by using a weighted combination of target labels with the uniform distri-\nbution. Whilst label smoothing has long been used as a practical trick to improve perfor-\nmance of neural networks in image classification and neural language processing (NLP)\ntasks, it was generally not known when and why label smoothing works [95]. Mueller\net al. demonstrate that in addition to improving deep neural network performance label\nsmoothing implicitly calibrates predictions, making prediction confidence better aligned\nwith accuracy maximisation [95].\nIn a neural network trained with label smoothing, instead of using hard labelsyi, the\nsmoothed version of the labels(1 \u0000 \u03b1)yi + \u03b1/K is used to minimise cross entropy [95].\nUsing label-smoothing approach, the actual labely is weighted-averaged with one of the\nK class labels drawn randomly from uniform distribution.\nPereyra et al. have shown [106] that label smoothing is equivalent to confidence\npenalty if the uniform distribution and model outputs in the KL divergence is reversed.\nIn a range of experiments using computer vision and NLP datasets, label smoothing im-\nproved performance and resulted in better generalization and improved calibration of\ndeep neural networks [95]. Moreover, in on NLP tasks label smoothing improved trans-\nlation quality (as measured by BLEU score) despite resulting in worse NLL [95].",
        "363f0946-5808-4310-adf2-905c6d7bdd8e": "44 Chapter 2. Literature review\n2.2.15 Focal loss\nFocal loss is a method that was originally developed to address the class imbalance prob-\nlem encountered in dense object detectors by adding a modulating factor(1 \u0000 pi)\u03b3 to the\ncross entropy loss [80].\nDefinition 2.2.5(Cross Entropy (CE) loss). Cross entropy loss is defined as:\nCE(p, y) = CE ( pt) = \u0000log (pt)\npt =\n8\n>><\n>>:\np if y = 1\n1 \u0000 p if y = 0 .\nDefinition 2.2.6(Focal loss). Focal loss is then defined as:\nF L = \u0000(1 \u0000 pt)\u03b3 log pt.\nWhen an object is misclassified andpi is small, the modulating factor is close to1 and\ndoesnotaffectcrossentropy. Asthemodelbecomesmoreconfidentand pi ! 1,thefactor\ntendstozeroandtheentropyisdown-weightedbydampeningtheeffectofoverconfident\npredictions. The parameter\u03b3 controls the effect of dampening overconfidence with\u03b3 = 0\ncorrespondingtounmodulatedcrossentropyandbyincreasing \u03b3 theeffectofmodulation\nis increased.\nMukhoti et al. [94] extended the idea of using focal loss to improving model calibra-\ntion. Whilst temperature scaling described in Guoet al.[44] is a popular modern variant\nof Platt\u2019s scaling (see2.2.11 for more details) that can improve calibration of a deep neu-\nral network without affecting its accuracy, it has certain drawbacks, including reducing\nconfidence in correct predictions at the same time as dampening overconfidence in incor-\nrect predictions. Mukhotiet al.[94] proposed replacing cross-entropy with focal loss and\nhave found that this is equivalent to minimising a regularised KL divergence between the\npredicted softmax distribution and the target label distribution. The focal loss is direct-\ning neural network\u2019s attention to correctly classified objects for which it is predicting low\nprobabilities. At the same time, when compared to using cross-entropy loss, focal loss",
        "c0303701-5368-4d38-8b54-2d3bd03a1b32": "2.2. Calibration methods 45\nindirectly regularises network weights during training by reducing gradient norms for\nconfident samples [94]. Mukhotiet al.[94] outline a number of interesing findings:\n\u2022 curse of misslassified samples \u2014 the increase in test NLL during training (indicat-\ning overfitting) is only caused by incorrrectly classifed samples, the miscalibration\nobserved during training is linked to overfitting on textNLL in line with findings in\n[44].\n\u2022 peakatthewrongplace\u2014theentropiesforallobjects(includingbothcorrectlyand\nincorrectly classified) decline during the training, \u201cthis indicates that the network\nis becoming more and more confident about its incorrect predictions.\u201d This finding\nis similar to that observed in Pereyraet al.[106] described in more detail in2.2.12.\n\u2022 weight magnification \u2014 the overconfidence by neural network increasing the norm\nof its weights, cross-entropy induces such weight magnification during network\ntraining.\nAnother interesting finding is that whilst minimising cross entropy results in minimi-\nsation of the KL divergence between the softmax and the target distribution, the use of\nfocal loss results in the minimisation of regularised KL divergence [94]:\nF L \u0015 KL(qk\u02c6p) \u0000 \u03b3H[\u02c6p].\nWhere q is the target distribution and\u02c6p is the predicted distribution over the labels. Re-\nplacing cross-entropy with the focal loss therefore has the effect similar to that of adding\nentropy regulariser in [106] (described in2.2.12), with the effect of both minimising KL\ndivergence whilst simultaneuosly trying to increase the entropy of the prediction distri-\nbution \u02c6p. By dampening overconfident distributions, the focal loss improves calibration.\nIn experiments on a range of computer vision and document classification datasets,\nthe focal loss approach was compared to the following baselines using the Brier loss (see\n1.2.4, [10]) \u2014 the MMCE2.2.13 and label smoothing2.2.14. As shown in [29], the Brier\nloss can be decomposed into calibration and refinement terms and also imposes penalty\non incorrect class probabilities [94]. In almost all experiments, using focal loss resulted\nin deep neural networks that were better calibrated than networks trained with baselines.",
        "78b1bce4-40a8-4d90-b353-e55b7e0c478b": "46 Chapter 2. Literature review\n2.2.16 Venn predictors\nAs described in Chapter1, Venn predictors (introduced in [144]) and described in [137,\nChapter 5], it is not possible to estimate the true conditional probabilities of the class\nlabels. Venn predictors (VP), while not as perfect as the true probabilities, are well-\ncalibratedprobabilisticpredictorsthat,givenaclasslabel,outputmulti-probabilitydistri-\nbution foreach class label of the testobject. As shown in [137, Chapter6] Vennpredictors\nachieve the calibration objective in a very strong non-asymptotic case. In general, one\ncould expect Venn predictors to be well calibrated as long as one accepts the assumption\nof randomness [137].\nVenn predictors are constructed by dividing the training set into categories. A test\nobject is then assigned into one of the categories. Since at the time of such assignment\nthe test object\u2019s label is unknown, one can try each label that an object might have and\nthen compute the frequency of the labels in each category that an object has been as-\nsigned to depending on the chosen label. Obtained set of frequencies for the test object\u2019s\nunknown label can then be interpreted as a probability distribution over the test object\u2019s\nlabels ([137]. The procedure obtains several probabilities even in a binary classification\ncase, in practice if the number of examples in each category is large enough, such binary\nprobabilities will be almost identical. Venn predictors output probability distributions\nthat are guaranteed to contain well-calibrated probabilities under the assumption of ran-\ndomness (see, e.g., [137], Theorem 6.6).\nThe main desiderata for Venn predictors are validity, predictive and computational\ne\ufb00iciency. Venn predictors have been successfully combined with several underlying\nmachine learning algorithms such as support vector machines [155], artificial neural net-\nworks [103] and K-nearest neighbours algorithm [25].\n2.2.17 Venn-Abers predictors\nA natural extension of Venn predictors are Venn-Abers predictors (VAP) \u2014 with the\nAbers part of the name formed by combining the surnames of the authors in Ayeret al.\n[5] that has described the underlying algorithm. Venn-Abers predictors can be built on",
        "0d99cc8d-01ca-4be2-899a-be05e989fe3c": "2.2. Calibration methods 47\ntop of a wide range of machine learning classification algorithms that produce classifi-\ncation scores, such scores can then be converted into probabilities using the Venn-Abers\nmethod. As shown in [68], the method of isotonic regression for probabilistic calibration\nintroduced by Zadrozny and Elkan [153] can often lead to mis-calibrated predictions (a\nsimilar conclusion was reached in [56]) and overfit on small datasets [76]. Another limi-\ntation of the isotonic regression is its reliance on the assumption that classification scores\nproduced by the underlying classifier are ranked correctly \u2014 in practice this is equivalent\nto assuming that the AUC score is equal to 1 on the test set which is unrealistic to expect\nin practice [98].\nVenn-Abers predictors are a modification of the isotonic regression [153], and as they\nareavariantofVennpredictors(thathaveanautomaticpropertyofvalidity)theyarealso\nperfectly calibrated. The property of automatic calibration comes with a cost, however, as\nVenn-Abers predictors are multi-probability predictors.",
        "f05a7c7c-fa4a-41bf-a26e-8936d722af55": "48",
        "84a2dfe0-1415-418a-9973-0423c8b4aba4": "49\nChapter 3\nProbabilistic classification\nThis chapter introduces a new method of probabilistic prediction in multi-class classification set-\nting. The method coverts two computationally e\ufb00icient calibration algorithms IV AP and CV AP\ninto multi-class probabilistic predictors. The proposed multi-class predictors improve calibration\nfor most classifiers and depending on the data set are often more experimentally accurate than clas-\nsical calibration methods such as Platt\u2019s scaler and isotonic regression.\n3.1 Introduction\nMulti-class classification is the problem of classifying objects into one of the more than\ntwo classes. The goal of classification is to construct a classifier which, given a new test\nobject, predicts the class label from the set of possiblek classes. In an \u201cordinary\u201d classifi-\ncation problem, the objective is to minimize the loss function by predicting correct labels\non the test set. In contrast, a probabilistic classifier outputs, for a given test object, the\nprobability distribution over the set ofk classes. Probabilistic classifiers allow to express\na degree of confidence about the classification of the test object. This is useful in a range\nof applications and industries where uncertainty around predictions needs to be quanti-\nfied, including in life sciences, pharmaceutical R&D, self-driving cars, finance, robotics,\nelection forecasting [131] and forecasting of pandemics such as COVID-19.\nA number of techniques are available to solve binary classification problems. In logis-\ntic regression, the dependent variable is a categorical variable indicating one of the two\npossible classes. Logistic regression model is used to estimate the probability of a binary\nresponse based on a number of independent variables (features). In a large empirical",
        "0e6669c3-f0f1-44af-becf-d498bb5a5b3e": "50 Chapter 3. Probabilistic classification\ncomparison of supervised learning algorithms by Caruana and Mizil [16], the logistic\nregression, whilst not competitive with the best methods, was the best model for some\nperformance metrics for specific problems. Another technique that can be used for both\nbinary and multi-class classification problem is the artificial neural network (ANN). Ar-\ntificial neural network is a computational model based on a large collection of connected\nunits called artificial neurons. Depending on the problem setting, an ANN could be used\neither to predict one of thek classes directly (by havingk neurons in the final layer of\nthe neural network) or indirectly by building separate networks for each of thek classes.\nANNsarealsoabletoestimatemulti-classprobabilitiesdirectlybyusingsoftmaxfunction\nin the final layer of a neural-based classifier.\nIn a probabilistic classification setting where the loss function uses exact class prob-\nabilities, calibrating classification scores produced by the underlying classifier (whether\nmachine learning or statistical) improves overall model performance. As shown in Caru-\nana and Mizil [17], maximum margin methods such as support vector machines and\nboosted trees result in sigmoid-shaped distortion of the predicted probabilities. Accord-\ningtoCaruanaandMizil[ 17],othermethodssuchasneuralnetworksandlogisticregres-\nsion do not suffer from these biases and result in better-calibrated probabilities, however\nmorerecentstudies(see, e.g., [57])haveshownthattraditionalneuralnetworksareoften\nmiscalibrated.\nPlatt\u2019sscaling[ 109](alsoknownaslogisticscalingorlogisticcalibrationanddescribed\ninmoredetailinSection 2.2.2ofChapter 2[109])iseffectivewhentheunderlyingmachine\nlearning algorithm produces sigmoid-shaped distortions in the predicted class scores \u2014\nas this method was originally developed to address distortions in classification scores\nproduced by support vector machines (SVM).\nPlatt\u2019sscaling,however,hasseveraldisadvantages. AsshowninPereira et al.[105]itis\nadequate for calibration of SVM\u2019 output that produces characteristic sigmoid-shaped dis-\ntortion in classificaiton scores. Further, Kullet al.[65] show that parametric assumptions\nin Platt\u2019s scaler are equivalent to assuming that classifier scores are normally distributed\nwiththesamevariance( \u03c32)foreachclass. Experimentshaveshown(see,e.g.,[ 65],[105])\nthat using Platt\u2019s scaling to calibrate output of many classifiers including AdaBoost and",
        "14782e98-80ca-4ebf-8214-c0f309682060": "3.1. Introduction 51\nNa\u00efveBayescanresultinprobabilityestimatesthatareinfactworsethanscoresproduced\nby the underlying classifier in cases where score distributions have heavy tails.\nUsing Platt\u2019s scaling as a mapping function for well calibrated cases can result in\nmodel distortion as it does not include the identity function [65] (hence for well cali-\nbrated scores where no transformation is needed the well-calibrated scores produced by\nthe underlying classifier will instead be distorted into poorly calibrated scores). Whist\nPlatt\u2019s scaling is a parametric method and requires less data than isotonic regression (be-\ning a non-parametric method, isotonic regression can overfit on small datasets), it can\nresult in miscalibration in cases where the score distribution model is misspecified. Kull\net al. [65] demonstrate that \u201cmodel mismatch is a real danger for a range of widely used\nmachine learning models and can make classifiers less calibrated.\u201d In addition, Platt\u2019s\nscaling, being a variant of temperature scaling approach, suffers from the same draw-\nbacks as temperature scaling, including dampening of confidence for both correct and\nincorrect predictions and making calibration worse under the covariat shift [102].\nAnotheralgorithmforprobabilitycalibrationisisotonicregression(describedinmore\ndetail in Section2.2.3 of Chapter2, [152]). The disadvantages of the isotonic regression\ninclude the assumption that the underlying classifier correctly ranks classification scores\n(this in turn is equivalent to assuming the ROC AUC score of 1 on the test dataset \u2014 the\nunrealistic assumption for most real life datasets. In addition, the isotonic regression is\npronetooverfitting, especiallywhenthedataisscarce. Inthestudyofindividualpatients\nby Pereiraet al. [105] isotonic regression performed better than Platt\u2019s scaling for classi-\nfiers other than SVM, however produced poor results when using decision trees as the\nunderlying classifier .\nBoth classical and modern deep neural networks are miscalibrated (see, e.g., [57],\n[44], [89], [94], [105]). Despite increasing classification accuracy of modern deep learn-\ningneuralarchitecturessuchasResNet[ 45],theirperformance\u2014asmeasuredbyclassifi-\ncationaccuracy\u2014doesnottranslateintoimprovementsintermsofqualityofprobabilistic\npredictions. As shown in Guoet al. [44], modern neural networks become significantly\nmiscalibrated even as their classification accuracy improves. The authors suggest that\nthe effects of miscalibration appear to be linked to recent advancements in deep learning",
        "91bf7be7-d8e6-46e2-b839-35070b2248e3": "52 Chapter 3. Probabilistic classification\ndesigned to improve training and accuracy of neural networks \u2014 with both batch nor-\nmalization and regularization having strong impact on network miscalibration [44] (see\nChapter2fordetailedoverviewofstate-of-the-artincalibrationmethodsforbothclassical\nmachine learning models and deep learning networks).\nMore recently, Vovket al.[141] introduced two new computationally e\ufb00icient proba-\nbilistic predictors: IVAP (inductive Venn-Abers predictors) and CVAP (cross Venn-Abers\npredictors). IVAP can be considered a regularized form of calibration based on the iso-\ntonic regression. Whilst IVAP is constructed using two isotonic regression using different\nlabelsforthetestobject,themultiprobabilitypredictionoutput( p0,p1)computedbyIVAP\nis a form of regularisation and the interval width indicates confidence in prediction. Due\nto its regularized nature, IVAP are less prone to overfitting than isotonic regression (iso-\ntonic regression is described in more detail in Section2.2.3 of Chapter2). As IVAP are\na special case of Venn-Abers predictors that have automatic guarantees of validity [141],\nIVAParealsoautomaticallywell-calibrated. CVAPareanextensionofIVAPusingtheidea\nof cross-validation [141]. In empirical studies of pairwise classification problem Vovket\nal. [141] demonstrated consistent accuracy of CVAP compared to the existing methods\nsuchasisotonicregressionandPlatt\u2019sscaling. Inarecentbiomedicalinformaticsstudyby\nPereiraet al.[105], Venn-Abers predictors were shown to reduce both false positives and\nfalsenegativesbypushinguncertaintyestimatesforincorrectlyclassifiedcasestothecen-\nter of probability prediction intervals. Given that conformal predictors are automatically\nvalid and validity of Venn-Abers predictors and IVAPs is also theoretically guaranteed,\nthe study concluded [105] that conformal predictors and Venn-Abers predictors are the\npreferable methods to uncertainty estimation for individual patient predictions.\nAll probability calibration methods described so far were designed for binary classi-\nfication problems. For a multi-class classification problem there are several techniques\nof assigning test objects to one of thek classes. If the conditional probabilities for each\nof thek classes are known or can be estimated, the multi-class classification problem is\nreduced to a trivial task of finding the numberi of the class maximizing the conditional\nprobability pi(x) computed for each of thek classes. In practice, estimating conditional\nclass probabilities is a hard task, especially in high-dimensional setting with limited data",
        "b0df97e1-312e-4bdf-a204-ed4cc5b4ed34": "3.1. Introduction 53\n(the curse of dimensionality). In a binary classification task, finding a good separating\nfunction instead of conditional probabilities often gives better prediction results.\nBinary probabilistic classifiers output class probabilities or decision scores. We use\nPlatt\u2019s scaling, IVAP and CVAP to convert output of binary classifiers into calibrated bi-\nnaryclassprobabilities. WethenusethePKPDmethoddescribedinSection 3.2toconvert\ncalibrated binary class probabilities into the multi-class probability distribution over the\nk classes.\nThe classical approach to multi-class classification is to consider a collection of binary\nclassification problems and then combine their solutions (when solutions include pair-\nwise class probabilities) to obtain multi-class probabilities. A number of methods for\nconverting output of binary classifiers into multi-class probabilities are available. In a\nsimple one-versus-all approach, k classifiers are built with thekth classifier separating all\nobjects in theith class from the objects in all otherk \u0000 1 classes. The multi-class classi-\nfier f(x) is then a function attainingarg maxi fi(x), where fi(x) is a classifier in binary\nclassification problem of separatingith class from all the other classes. In anotherone-\nversus-one method (also calledall-pairs classification) k(k\u22121)\n2 binary classifiers are built to\nclassify test objects between each pair of theith and jth classes. If such classifiers are\ndenoted asfij, the multi-class classification problem is reduced to findingfi(x) such that\nfi(x) = arg max j\nPfij(x). Both one-versus-all and one-versus-one approaches generally\nperform well and the suitability of a method to the specific problem or application de-\npendsonthetimeneededtobuildaparticularclassifierincomparisonwithtimerequired\nto repeat the classification task. For one-versus-one (OVO) algorithm, the number of rep-\netitions is O \u0000N2\u0001, whilst for one-versus-all (OVA) the number of repetitions isO (N).\nHowever, if the time required to build a classifier is super-linear in terms of the number\nof objects, one-versus-one (OVO) is a more e\ufb00icient choice.\nAs an alternative to solving multi-class classification by combining solutions to bi-\nnary classification problems, approaches such assingle machine and the error correcting\ncode can be used. In the single machine approach in Weston and Watkins [150], a sin-\ngle optimization problem is solved by training a multi-class support vector machines to\nsolve generalised binary support vector machines problem with the decision function",
        "01ad40b8-39ad-4781-b41e-ba2b81aee8be": "54 Chapter 3. Probabilistic classification\nf(x) = arg max\ni\n(wix + bi). This approach can be used for simultaneous multi-class sepa-\nrationinsituationswherebinaryclassificationbyone-versus-allandone-versus-onefails.\nAs an additional benefit, the single machine approach results in reducing the number of\nsupport vectors and more e\ufb00icient kernel computations. The benefits of the single ma-\nchine are, however, limited to the situations where it is hard to separate the data whilst\nat the same time meaningful subsets exist that allow for assigning a higher value to the\ndecision function for the correct class as compared to other classes.\nOther methods for solving multi-class classification problem include voting [111] and\nvarious methods based on combining binary probabilities to obtain multi-class probabil-\nities. Such methods rely on obtaining estimatesrij of pairwise probabilities\u00b5ij = P(y =\ni j y 2 f i, jg, x). Estimates rij are obtained by building binary classifiers for each of the\npairwise unions of theith andjth classes and usingrij as an approximation for\u00b5ij. In\nthe next section, we describe a method of converting estimatesrij of pairwise class prob-\nabilities into estimatespi of multi-class probabilities fork classes.\n3.2 Obtaining multi-class probabilities from pairwise classifica-\ntion\nIn order to obtain estimates of multi-class probabilities we convert binary class probabil-\nities using the method in Priceet al.[111]:\npPKPD\ni = 1P\nj:j\u0338=i\n1\nrij\n\u0000 (k \u0000 2). (3.1)\nClass probabilitiespPKPD\ni computed using3.1 need to be normalized to ensure that they\nsum to one. We will refer to this method as the \u201cPKPD\u201d method. We use this method to\nobtainmulti-classprobabilitiesfrompairwiseclassprobabilitiesproducedbytheunderly-\ningalgorithms(weuselogisticsregression,supportvectormachinesandneuralnetwork)\nandcalibratedusingPlatt\u2019sscaling,IVAPandCVAP.Wethenusemulti-classprobabilities\ncomputed using the \u201cPKPD\u201d method3.1to assign test object to one of thek classes which",
        "3766ca60-c102-48a4-a4de-da38dd3919eb": "3.3. Inductive and cross Venn-Abers predictors 55\nallows us to compute loss metrics on the test dataset and compare them across different\nunderlying machine learning methods and calibration algorithms.\n3.3 Inductive and cross Venn-Abers predictors\nAs described in Chapter1, Venn predictors (introduced in [144]) and described in [137,\nChapter 5], it is not possible to estimate the true conditional probabilities of the class la-\nbels. Vennpredictors(VP)(see 2.2.16formoredetails),whilenotasidealasthetrueprob-\nabilities, arewell-calibratedprobabilisticpredictorsthat, givenaclasslabel, outputmulti-\nprobabilitydistributionforeachclasslabelofthetestobject. Asshownin[ 137,Chapter6]\nVenn predictors achieve the calibration objective in a very strong non-asymptotic case. In\ngeneral, one could expect Venn predictors to be well calibrated as long as we accept the\nassumption of randomness [137].\nA natural extension of Venn predictors is Venn-Abers predictors described in [141].\nVenn-Abers predictors are built on top of a wide range of machine learning classification\nalgorithms that produce classification scores, the calibration scores produced by the un-\nderlying machine learning algorithms can then be converted into probabilities using the\nVenn-Abers method. Venn-Abers predictors are a modification of the isotonic regression\n[153] and unlike the classification scores produced using Platt\u2019s scaling method or the\nmethod of the isotonic regression, the classification scores produced Venn-Abers predic-\ntors have theoretical guarantees or validity and are perfectly calibrated.\nCalibration algorithms IVAP (inductive Venn-Abers predictor) and CVAP (cross\nVenn-Abers predictor) are computationally e\ufb00icient versions of Venn-Abers predictors\nstudied in [141]. Whilst IVAP and CVAP are based on the calibration method used by\nthe isotonic regression [152], IVAP and CVAP avoid problems associated with isotonic\nregression such as lack of calibration guarantees or overfitting when the data is scarce.\nThis is achieved by assigning each of the test objects with two potential labels of0 and\n1 to fit two isotonic regressions instead of one. As Venn-Abers predictors are a special\ncase of Venn predictors, they inherit the property of perfect calibration from Venn pre-\ndictors [141]. As shown by Vovket al. [141], IVAP are automatically perfectly calibrated\nand the experimental results reported in the paper suggest that this property is inherited",
        "d64b0603-2bee-4ae3-9f42-5901e4475e22": "56 Chapter 3. Probabilistic classification\nby CVAP [141]. The property of automatic calibration comes with a cost, however, as\nVenn-Abers predictors are multi-probability predictors. IVAP and CVAP are computa-\ntionally e\ufb00icient algorithms with predictive e\ufb00iciency depending on the e\ufb00iciency of the\nunderlying algorithms [141].\n3.3.1 Computational details of IVAP and CVAP\nIn binary classification problems, machine learning classifiers output prediction scores\nfor each test object, class predictions are then obtained by comparing the score for each\ntest objects to a specified threshold. To obtain calibrated scores, functiong(s(x)) is then\napplied to convert uncalibrated scoress to calibrated probabilities. Assuming that the\nunderlying classifier ranks test objects correctly (with objects that are more likely to end\nup in class1 being assigned higher classification scores), such functiong(s) should be\nisotonic (non-decreasing) function in order to preserve original ranking produced by the\nclassifier. Figure3.1showsageneralillustrationofanisotonicregressionfittoandataset.\nIsotonic functiong(s(x)) can be constructed by fitting isotonic regression to the set of\npoints (s (xi) , yi), wheres (xi) is the classification score for theith object in the calibra-\ntion data set andyi is the actual label of this object. This is achieved by maximizing the\nlikelihood on the calibration set, whereK is the size of the calibration set\nY\ni=1,2,...,K\npi (3.2)\nand pi is defined as follows:\npi =\n8\n>><\n>>:\ng (s (xi)) if yi = 1\n1 \u0000 g (s (xi)) if yi = 0\n(3.3)\nThe most widely used algorithm for learning isotonic function g(s) is the pool-\nadjacent-violators algorithm (PAVA, [7]). PAVA has linear time and memory complex-\nity and works by scanning uncalibrated scoressi starting with the first object in the test\ndata set and each time comparingsi\u22121 with si to find violations of monotonicity. Every\ntime such monotonicity violation is found, bothsi\u22121 with si are replaced with their mean",
        "dee48463-e5a8-4460-8d6c-2d9bc3f0cc71": "3.3. Inductive and cross Venn-Abers predictors 57\nFIGURE 3.1: Isotonic regression, Wikipedia\n(si\u22121 + si)/2. If such replacement results in violations for test objects scanned earlier, the\nalgorithmneedstogobacktofixsuchearliermonotonicityviolationsviaaveragingbefore\nproceeding to next object. Figure3.2 is an example of static snapshot of PAVA algorithm\nreplacing monotonicity violations with values of their mean(si\u22121 + si)/2.\nFIGURE 3.2: PAVA algorithm for isotonic regression\nVenn-Abers predictors are based on the idea of isotonic regression [141]. Venn-Abers\npredictors are essentially a regularized version of isotonic regression [141], but instead of\nfitting isotonic regression to the calibration set once, it is fitted twice to the calibration set\naugmented with test object, each time assuming that the label of a test object is known it\ncan be either0 and 1 or accordingly. The first isotonic regressiong0(s) is therefore fitted",
        "49caed2b-72a4-46d0-ad67-002081609d95": "58 Chapter 3. Probabilistic classification\nto the set of points\n((s0 (x1) , y1) , (s0 (x2) , y2) , . . . , (s0 (xK) , yK) , (s0(x), 0)) (3.4)\nwhilst the second isotonic regressiong1(s) is fitted to the set of points\n((s1 (x1) , y1) , (s1 (x2) , y2) , . . . , (s1 (xK) , yK) , (s1(x), 1)) (3.5)\nInductive Venn\u2013Abers predictors (IVAP) uses classification scoress1, . . . , sK computed\nbytheunderlyingmachinelearningorstatisticalclassificationalgorithmonthecalibration\nset of sizeK (calibration set can be obtained by reserving a part of the training set, the\nother remaining part of the original training set that we will refer to asthe proper training\nset is used for training the underlying machine learning or statistical model) and also the\nscore s computed for each new test object. The isotonic regression is then fitted twice\nto the set of computed scoress1, . . . , sK, s (used as the independent variables), and two\nsetsofdependentvariablesformedbycombiningthelabelsofthecalibrationobjectswith\nthe two (either 0 or 1) potential labels for the test object. By fitting isotonic regression\ntwice, IVAP computes multi-probability prediction (p0,p1) for the test object that can be\ninterpreted as the lower and the upper probability respectively of the object belonging\nto the class labelled as 1. IVAP computes (p0,p1) e\ufb00iciently for each of the potential test\nobjects by pre-computing two vectorsF0 andF1 which storef0(s) andf1(s), respectively,\nforallpossiblevaluesofs. AsshownbyVovk et al.[141],giventhescores s1, . . . , sK ofthe\ncalibration objects computed by the underlying algorithm, the IVAP prediction rule can\nbe computed in timeO(K log K) and spaceO(K), whereK is the size of the calibration\nset.\nA cross Venn-Abers predictor (CVAP) is just a combination ofK IVAP, whereK is\nthe number of folds in the training set. To obtain class probabilities using CVAP, Vovket\nal. [141] propose to use minimax method to mergeK multiprobability predictions com-\nputedby K IVAP. FortheLog-loss,themultiprobabilitypredictionforCVAPisaninterval\n(1 \u0000 GM(1 \u0000 p0), GM(p1)) obtained by computing geometric means of multiprobability",
        "32eae5c7-8be3-467f-bdc3-3ef55e358c02": "3.4. Experiments on multi-class data sets 59\npredictionsarisingoutofrepeatedapplicationofIVAPto K folds(GM(p1) isthegeomet-\nricmeanof p1\n1, . . . , pK\n1 andGM(1\u0000p0) isthegeometricmeanof 1\u0000p1\n0, . . . , 1\u0000pK\n0 ). Forthe\nBrierloss,themergedprobabilityisgivenbyformula p = 1\nK\nPK\nk=1\n\u0000pk\n1 + 1\n2 (pk\n0)2 \u0000 1\n2 (pk\n1)2\u0001.\nThe minimax method can also be applied to IVAP to obtain single probability predic-\ntion by combining multi-probability prediction as follows:p := p1/(1\u0000p0 +p1) (see Vovk\net al.[141] for more details).\n3.4 Experiments on multi-class data sets\nWe present the experimental results using several multi-class data sets: waveform,\niris and abalone from the UCI Machine Learning Repository [33], satimage and\nvehicle silhouettes from the Statlog collection [90], balance-scale, pasture and eye\nmovements from [132]. To measure the quality of probabilistic predictions and calibrators\nwe follow Gneitinget al.[40] and use the proper scoring rules - the Log loss1.2.3 and the\nBrier loss1.2.4 (see Section1.2.1 of Chapter1 for more details).\nIn both casesp is the vector of class probabilities andy is the vector of true labels\none-hot encoded across theK classes. Both the Log loss and the Brier loss are computed\nby taking arithmetic mean of the respective losses on the individual objects from the test\ndataset. OneadvantageoftheBrierlossisthatitisstillpossibletocomparequalityofpre-\ndictions in cases where a prediction algorithm produces infinite Log loss. In this section\nwe compare the performance of IVAP and CVAP with that of Platt\u2019s scaling [109] and the\nisotonic regression [153].\nFor all experiments, we use the same underlying algorithms: Na\u00efve Bayes, KNN,\nSupport-vector machine (SVM), the logistic regression, simple neural network, Random\nForest, LightGBM, XGBoost, CatBoost and Ada Boost. With the exception of LightGBM\n[58], XGBoost [20], CatBoost [32], all the other algorithms are available in Scikit-learn\n[11]. All the underlying classifiers have been used with the default parameters. The\nunderlying algorithms output binary classification scores which are then calibrated by\napplying Platt\u2019s scaling, isotonic regression, IVAP and CVAP. We have used implementa-\ntions of Platt\u2019s scaling and the isotonic regression from Scikit-learn [11].",
        "36a5545b-ece9-4cfb-822a-b149235da919": "60 Chapter 3. Probabilistic classification\nTo obtain pairwise classification scores, we run the ten underlying machine learning\nalgorithms: Na\u00efve Bayes, KNN, Support-vector machine (SVM), the logistic regression,\nsimple neural network, Random Forest, LightGBM, XGBoost, CatBoost and Ada Boost.\nWe then apply the PKPD (method described in3.1) to convert pairwise classification\nscores into calibrated multi-class probabilities.\nWe use OpenML [132] to access all the datasets to ensure consistency and re-\nproducibility of the experiments: waveform (ID:60), satimage (ID:182) , vehicle\nsilhouettes (ID:54) , iris(ID:61), abalone (ID:1557 , balance-scale (ID:11) , eye\nmovements (ID:1044) and pasture (ID:462) .\nFrom each dataset, three datasets were created. We use scikit-learn functionality to\ncreatetestsetusing 20% ofalldata,andusetheremainingtrainingsettocreatecalibration\nset equal in length to the test set. The remaining data is used for proper training set used\nto train underlying classifiers. To ensure reproducibility we use random seed of42 and\ndefault values in scikit-learn across all experiments.\nThe data sets and the results of the experiments are described below.\n3.4.1 Waveform data set\nWaveform is an artificial data set [33] containing three different classes of waves with a\ntotal of 5,000 instances and 40 attributes. Each class is generated by combining two or\nthree base waves and adding noise to each attribute. The waveform dataset is balanced\nwith about one third of objects in each of the three classes.\nWe split thewaveform dataset as described in section3.4 into the training set of 4,000\nobservations)andthetestset(1000observations). Wefurthersplitthetrainingsetintothe\nproper training set and the calibration set equal in length to the test set. This split results\nin 3,000 objects allocated to the proper training set and 1,000 objects in both calibration\nand test set each. Tables3.1 and 3.2 refer to the results of experiments.\nBasedontheLogloss,usingCVAPtocalibratepairwiseclassificationscores,resultsin\nperformance improvements for seven out of the ten classifiers. Whilst isotonic regression\nimproves calibration for KNN and neural network and Platt\u2019s scaler improves calibration\nforRandomForest,CVAPdeliversthebestoverallimprovementincalibrationforfourout",
        "5bc2c20f-d14d-4c08-a233-07d31eb1e09c": "3.4. Experiments on multi-class data sets 61\nTABLE 3.1: The Log loss for thewaverfont data set\nno calibration sigmoid isotonic ivap cvap\nNa\u00efve Bayes 0.5096 0.2756 0.3189 0.2279 0.2267\nKNN 0.4759 0.2686 0.2655 0.3644 0.3506\nSupport Vector Machine 0.1881 0.1927 0.2536 0.1985 0.1914\nlogistic regression 0.1963 0.1987 0.2576 0.2052 0.2037\nneural network 0.4172 0.3601 0.3333 0.4036 0.3925\nRandom Forest 0.2491 0.2002 0.2192 0.2066 0.2054\nLightGBM 0.2377 0.2534 0.2331 0.2199 0.2080\nXGBoost 0.2657 0.2510 0.2319 0.2189 0.2103\nCatBoost 0.1948 0.2308 0.2246 0.2158 0.2013\nAda Boost 0.5808 0.2447 0.3360 0.2498 0.2278\nTABLE 3.2: The Brier loss for thewaveform data set\nno calibration sigmoid isotonic ivap cvap\nNa\u00efve Bayes 0.1114 0.0847 0.0689 0.0698 0.0694\nKNN 0.0826 0.0843 0.0849 0.1080 0.1024\nSupport Vector Machine 0.0586 0.0595 0.0605 0.0603 0.0577\nlogistic regression 0.0605 0.0610 0.0616 0.0617 0.0611\nneural network 0.1063 0.1136 0.1075 0.1280 0.1247\nRandom Forest 0.0741 0.0631 0.0626 0.0639 0.0621\nLightGBM 0.0719 0.0751 0.0683 0.0683 0.0639\nXGBoost 0.0752 0.0738 0.0681 0.0679 0.0645\nCatBoost 0.0626 0.0695 0.0660 0.0665 0.0610\nAda Boost 0.1975 0.0766 0.0764 0.0771 0.0692",
        "b80def3c-79f8-49e1-9ba7-8a155dd94953": "62 Chapter 3. Probabilistic classification\nTABLE 3.3: The Log loss for thesatimage data set\nsigmoid isotonic ivap cvap\nNa\u00efve Bayes 1.0158 0.1651 0.1627 0.1555 0.1529\nKNN 0.1849 0.0802 0.1140 0.2691 0.2717\nSupport Vector Machine 0.0740 0.0794 0.1403 0.0891 0.0907\nlogistic regression 0.0941 0.0990 0.1409 0.1064 0.1050\nneural network 0.1035 0.0923 0.1139 0.1801 0.1766\nRandom Forest 0.0748 0.0768 0.1331 0.0947 0.0948\nLightGBM 0.0732 0.0806 0.1153 0.0787 0.0775\nXGBoost 0.0709 0.0845 0.1180 0.0832 0.0800\nCatBoost 0.0639 0.0785 0.1373 0.0824 0.0812\nAda Boost 0.3098 0.0914 0.1494 0.0933 0.0915\nten classifers: Na\u00efve Bayes, LightGBM, XGBoost and Ada Boost. Based on the Brier loss,\nusing CVAP to calibrate pairwise classification scores results in performance improve-\nments for seven out of the ten classifiers. Whilst isotonic regression improves calibration\nfor Na\u00efve Bayes, CVAP delivers the best overall improvement in calibration for five out\nten classifers: Support Vector Machine, LightGBM, XGBoost, CatBoost and Ada Boost.\n3.4.2 Satimage data set\nThe satimage data set [33] contains images representing seven different classes of soil,\nranging from red or gray soil to soil containing crops such as cotton or vegetation stubble.\nThe data set was collected to predict the soil type from the new satellite images, given the\nmulti-spectral values. The number of attributes is36 and the number of instances is6 435.\nTables3.3 and 3.4 refer to the results of experiments.\nBasedontheLogloss,usingCVAPtocalibratepairwiseclassificationscores,resultsin\nsignificant performance improvement for Na\u00efve Bayes and also Ada Boost. Platt\u2019s scaler\nprovides significant improvement for KNN and neural network, whilst for other under-\nlying classifiers no significant improvement in calibration occurs by using any of the four\nconsidered calibration methods. The situation is similar when using Brier loss \u2014 most\nunderlying algorithms do not see improvement from additional calibration.\nBasedontheBrierloss,usingCVAPtocalibratepairwiseclassificationscoresresultsin\nperformance improvements for for Na\u00efve Bayes, Support-Vector Machine and also neural",
        "9c11b565-aef7-4f43-aa09-3fce8a5569c3": "3.4. Experiments on multi-class data sets 63\nTABLE 3.4: The Brier loss for thesatimage data set\nsigmoid isotonic ivap cvap\nNa\u00efve Bayes 0.0617 0.0478 0.0434 0.0445 0.0431\nKNN 0.0211 0.0227 0.0223 0.0785 0.0784\nSupport Vector Machine 0.0215 0.0226 0.0231 0.0240 0.0237\nlogistic regression 0.0291 0.0300 0.0299 0.0303 0.0297\nneural network 0.0233 0.0244 0.0232 0.0464 0.0446\nRandom Forest 0.0213 0.0218 0.0226 0.0238 0.0228\nLightGBM 0.0195 0.0214 0.0199 0.0210 0.0198\nXGBoost 0.0186 0.0226 0.0219 0.0224 0.0205\nCatBoost 0.0190 0.0217 0.0211 0.0220 0.0209\nAda Boost 0.0933 0.0268 0.0253 0.0256 0.0245\nnetwork.\n3.4.3 Vehicle silhouettes data set\nVehicle silhouettes data set from the Statlog collection [33] was designed to find a\nmethod of distinguishing between 3D objects within a 2D image by application of an en-\nsemble of shape feature extractors to the 2D silhouettes of the objects. Four Corgie model\nvehicles were used: Chevrolet van, SAAB 9000, double-decker bus and Opel Manta 400.\nThis particular combination of vehicles was chosen with the expectation that buses, vans\nand either one of the car types would be readily distinguishable, but it would be more\ndi\ufb00icult to distinguish between the two car classes. The data set contains 946 instances\nand 18 attributes. We run the same underlying algorithms and calibration methods as for\nall the previous data sets. Tables3.5 and 3.6 refer to the results of experiments.\nBased on the Log loss, using CVAP to calibrate pairwise classification scores, results\nin significant performance improvement for Na\u00efve Bayes and also neural network. Platt\u2019s\nscaler provides significant improvement for KNN and isotonic regression for Ada Boost.\nCVAPprovidessignificantimprovementforNa\u00efveBayes, KNN,neuralnetwork, XGBoost\nand Ada Boost.",
        "5d724b84-03d5-4c96-8525-5a3651a78079": "64 Chapter 3. Probabilistic classification\nTABLE 3.5: The Log loss for thevehicle data set\nsigmoid isotonic ivap cvap\nNa\u00efve Bayes 1.1130 0.4949 0.7742 0.4738 0.4465\nKNN 0.7008 0.3695 0.8102 0.4382 0.4051\nSupport Vector Machine 0.4350 0.4707 0.8722 0.4552 0.4370\nlogistic regression 0.1835 0.2224 0.6953 0.2328 0.2316\nneural network 0.5645 0.5790 0.5791 0.5693 0.5634\nRandom Forest 0.2293 0.2249 0.6256 0.2530 0.2474\nLightGBM 0.2142 0.2319 0.5319 0.2451 0.2455\nXGBoost 0.2672 0.2367 0.4394 0.2528 0.2490\nCatBoost 0.2505 0.2316 0.5384 0.2550 0.2517\nAda Boost 0.4547 0.2438 0.2034 0.2579 0.2483\nTABLE 3.6: The Brier loss for thevehicle data set\nsigmoid isotonic ivap cvap\nNa\u00efve Bayes 0.2137 0.1629 0.1628 0.1566 0.1457\nKNN 0.1135 0.1162 0.1160 0.1414 0.1280\nSupport Vector Machine 0.1437 0.1566 0.1552 0.1511 0.1434\nlogistic regression 0.0590 0.0674 0.0616 0.0683 0.0678\nneural network 0.1883 0.1937 0.1937 0.1901 0.1879\nRandom Forest 0.0737 0.0705 0.0703 0.0755 0.0739\nLightGBM 0.0727 0.0729 0.0712 0.0738 0.0740\nXGBoost 0.0856 0.0753 0.0719 0.0762 0.0753\nCatBoost 0.0746 0.0723 0.0727 0.0770 0.0752\nAda Boost 0.1465 0.0759 0.0668 0.0769 0.0730",
        "7438c654-406c-494b-abca-55b0fcb43031": "3.4. Experiments on multi-class data sets 65\nTABLE 3.7: The Log loss for thebalance scale data set\nsigmoid isotonic ivap cvap\nNa\u00efve Bayes 0.2964 0.2573 0.5345 0.2896 0.2291\nKNN 0.9938 0.2953 0.3743 0.3998 0.3657\nSupport Vector Machine 0.1539 0.2190 2.3633 0.2373 0.2106\nlogistic regression 0.1761 0.2025 0.3493 0.2416 0.2118\nneural network 0.2173 0.2047 0.5427 0.2657 0.2765\nRandom Forest 0.2763 0.3157 1.3064 0.3216 0.2910\nLightGBM 0.2092 0.2986 2.4583 0.3091 0.2554\nXGBoost 0.1808 0.2902 1.0373 0.2699 0.2250\nCatBoost 0.2717 0.2636 2.0713 0.2850 0.2399\nAda Boost 0.5842 0.1352 1.1294 0.1388 0.1512\nTABLE 3.8: The Brier loss for thebalance scale data set\nsigmoid isotonic ivap cvap\nNa\u00efve Bayes 0.0823 0.0771 0.0967 0.0876 0.0618\nKNN 0.0756 0.0867 0.0868 0.1196 0.1030\nSupport Vector Machine 0.0468 0.0708 0.0862 0.0707 0.0578\nlogistic regression 0.0529 0.0611 0.0570 0.0689 0.0569\nneural network 0.0547 0.0575 0.0616 0.0712 0.0720\nRandom Forest 0.0822 0.0961 0.1017 0.0967 0.0815\nLightGBM 0.0615 0.0900 0.1185 0.0944 0.0699\nXGBoost 0.0587 0.0830 0.0931 0.0817 0.0614\nCatBoost 0.0722 0.0809 0.1008 0.0851 0.0654\nAda Boost 0.1988 0.0397 0.0405 0.0351 0.0347\n3.4.4 Balance scale data set\nBalance scale is a synthetic data set generated to model psychological experimental re-\nsults. Each object is classified as having the balance scale tip to the right, tip to the left,\nor be balanced. The attributes are the left weight, the left distance, the right weight, and\nthe right distance [33]. The data set contains625 distinct values,4 features and3 classes.\nTables3.7 and 3.8 refer to the results of the experiments.\nBased on the Log loss, using CVAP to calibrate pairwise classification scores results\nin significant performance improvement for Na\u00efve Bayes and also CatBoost. Platt\u2019s scaler\nprovides improvement for KNN, neural network and Ada Boost. CVAP provides signifi-\ncant improvement for Na\u00efve Bayes, KNN, neural netrowk, XGBoost and Ada Boost.",
        "ad5a36c2-ccf7-42bf-bec4-18548a94bb3e": "66 Chapter 3. Probabilistic classification\nTABLE 3.9: The Log loss for theeye movement data set\nsigmoid isotonic ivap cvap\nNa\u00efve Bayes 1.2293 0.6141 0.6360 0.6025 0.6015\nKNN 1.5009 0.5810 0.5796 0.5853 0.5832\nSupport Vector Machine 0.6004 0.6031 0.6027 0.5912 0.5900\nlogistic regression 0.5788 0.5799 0.6046 0.5758 0.5728\nneural network 0.6316 0.6316 0.6446 0.6306 0.6311\nRandom Forest 0.4705 0.4490 0.5055 0.4506 0.4393\nLightGBM 0.3996 0.4164 0.4364 0.4162 0.3965\nXGBoost 0.3823 0.4197 0.4390 0.4181 0.3850\nCatBoost 0.4031 0.4208 0.4675 0.4220 0.4078\nAda Boost 0.6279 0.5208 0.5362 0.5151 0.5063\nTABLE 3.10: The Brier loss for theeye movement data set\nsigmoid isotonic ivap cvap\nNa\u00efve Bayes 0.2560 0.2127 0.2083 0.2081 0.2077\nKNN 0.2136 0.1996 0.1991 0.2008 0.2000\nSupport Vector Machine 0.2067 0.2072 0.2040 0.2037 0.2033\nlogistic regression 0.1973 0.1978 0.1972 0.1971 0.1959\nneural network 0.2196 0.2195 0.2195 0.2196 0.2199\nRandom Forest 0.1518 0.1468 0.1470 0.1472 0.1431\nLightGBM 0.1294 0.1358 0.1359 0.1362 0.1287\nXGBoost 0.1237 0.1359 0.1363 0.1364 0.1236\nCatBoost 0.1304 0.1373 0.1384 0.1384 0.1329\nAda Boost 0.2184 0.1757 0.1744 0.1746 0.1709\nBased on the Brier loss, using CVAP improves to calibrate pairwise classification\nscores, results in calibration improvements for Na\u00efve Bayes, Random Forest, CatBoost\nand Ada Boost.\n3.4.5 Eye movement data set\nThe Eye movement data set is from the \u201cInferring relevance from eye movements\u201d chal-\nlenge [116]. The objective of the challenge was to predict, based on eye movement data,\nwhether a reader finds a text relevant. The data set contains10/, 936 distinct values and\n22 features.Tables3.9 and 3.10 refer to the results of the experiments.",
        "17cbc134-478c-4921-bd41-e7d33cc20edb": "3.4. Experiments on multi-class data sets 67\nTABLE 3.11: The Log loss for thepasture data set\nsigmoid isotonic ivap cvap\nNaive Bayes 4.1875 0.4446 6.5789 0.4898 0.5215\nKNN 0.4272 0.5852 3.7178 0.5654 0.5684\nSupport Vector Machine 0.4830 0.9266 6.5789 0.5188 0.5621\nlogistic regression 6.6237 0.7512 10.0278 0.5879 0.5813\nneural network 0.6583 0.6981 0.7352 0.6635 0.6438\nRandom Forest 0.4217 0.3991 0.2978 0.5301 0.4955\nLightGBM 0.6583 0.6981 0.7352 0.6635 0.6438\nXGBoost 0.3724 0.4631 3.3375 0.6231 0.6214\nCatBoost 0.4200 0.4638 3.3498 0.5006 0.4900\nAda Boost 1.1328 0.4297 3.2894 0.6230 0.6312\nBased on the Log loss, using CVAP to calibrate pairwise classification scores, results\nin performance improvements for eight out of the ten classifiers. Whilst isotonic regres-\nsion delivers the best calibration result for KNN, IVAP is the best calibration approach\nfor neural network and CVAP delivers the best overall improvement in calibration for the\nremaining eight out of ten classifiers.\nBased on the Brier loss, using CVAP to calibrate pairwise classification scores, results\nin the best performance improvements for seven out of the ten classifiers. Whilst isotonic\nregressiondeliversthebestcalibrationresultforKNNandneuralnetwork,CVAPdelivers\nthe best overall improvement in calibration for the seven out of ten classifiers.\n3.4.6 Pasture data set\nTheobjectiveofthedatasetwastopredictpastureproductionfromavarietyofbiophysical\nfactors [132]. The dataset contains vegetation and soil variables from areas of grazed\nNorthIslandhillcountrywithdifferentvariablesselectedaspotentiallyusefulbiophysical\nindicators. Tables3.11 and 3.12 refer to the results of the experiments.\nBased on the Log loss, whilst isotonic regression delivers the best calibration result\nfor Random Forest, CVAP is the best calibration approach for neural network and CVAP\ndeliversthebestoverallimprovementincalibrationforlogisticregression,neuralnetwork\nand LightGBM.",
        "28648c05-2c7a-46fa-8562-1874567ba9fb": "68 Chapter 3. Probabilistic classification\nTABLE 3.12: The Brier loss for thepasture data set\nsigmoid isotonic ivap cvap\nNaive Bayes 0.1905 0.1435 0.1905 0.1594 0.1704\nKNN 0.1500 0.2112 0.2573 0.1931 0.1923\nSupport Vector Machine 0.1633 0.1955 0.1905 0.1706 0.1891\nlogistic regression 0.3786 0.2588 0.3452 0.2062 0.1999\nneural network 0.2320 0.2466 0.2585 0.2336 0.2255\nRandom Forest 0.1273 0.1241 0.1080 0.1761 0.1595\nLightGBM 0.2320 0.2466 0.2585 0.2336 0.2255\nXGBoost 0.1018 0.1478 0.1036 0.2156 0.2151\nCatBoost 0.1286 0.1488 0.1108 0.1626 0.1569\nAda Boost 0.0952 0.1325 0.0952 0.2164 0.2197\nTABLE 3.13: The Log loss for theabalone data set\nsigmoid isotonic ivap cvap\nNaive Bayes 1.0651 0.5350 0.5877 0.5053 0.5026\nKNN 1.4360 0.4756 0.4707 0.5008 0.4929\nSupport Vector Machine 0.4631 0.4728 0.5503 0.4669 0.4604\nregression 0.4774 0.4777 0.5625 0.4798 0.4773\nneural network 0.4524 0.4601 0.4999 0.4546 0.4525\nRandom Forest 0.4454 0.4527 0.5316 0.4514 0.4450\nLightGBM 0.4726 0.4715 0.5701 0.4653 0.4499\nXGBoost 0.5015 0.4712 0.5110 0.4641 0.4458\nCatBoost 0.4423 0.4520 0.5845 0.4480 0.4433\nAda Boost 0.6075 0.4977 0.5625 0.4787 0.4598\nBased on the Brier loss, using CVAP to calibrate pairwise classification scores, results\nin the best performance improvements for seven out of the ten classifiers. Whilst isotonic\nregressiondeliversthebestcalibrationresultforKNNandneuralnetwork,CVAPdelivers\nthe best overall improvement in calibration for the seven out of ten classifiers.\n3.4.7 Abalone data set\nAbalonedatasetwascreatedtofacilitatepredictingtheageofabalonefromphysicalmea-\nsurements [34]. Tables3.13 and 3.14 refer to the results of the experiments.",
        "db773932-8da0-4dff-ba9a-128ccf5ebae8": "3.5. Conclusion 69\nTABLE 3.14: The Brier loss for theabalone data set\nsigmoid isotonic ivap cvap\nNaive Bayes 0.2187 0.1792 0.1701 0.1704 0.1696\nKNN 0.1656 0.1570 0.1553 0.1647 0.1614\nSupport Vector Machine 0.1528 0.1565 0.1560 0.1560 0.1530\nlogistic regression 0.1590 0.1596 0.1603 0.1602 0.1594\nneural network 0.1500 0.1515 0.1505 0.1508 0.1505\nRandom Forest 0.1481 0.1487 0.1481 0.1488 0.1467\nLightGBM 0.1556 0.1555 0.1541 0.1544 0.1490\nXGBoost 0.1630 0.1551 0.1538 0.1538 0.1471\nCatBoost 0.1469 0.1485 0.1488 0.1482 0.1465\nAda Boost 0.2093 0.1626 0.1598 0.1595 0.1526\nBasedontheLogloss,usingCVAPresultsinthebestresultsforsevenoutoftenunder-\nlying classifiers, including Na\u00efve Bayes, SVMs, logistic regression, Random Forest, Light-\nGBM, XGBoost and Ada Boost.\nSimilar performance improvements are observed using the Brier loss, using CVAP to\ncalibrate pairwise classification scores results in performance improvements for six out of\nthe ten classifiers.\n3.5 Conclusion\nMachine learning has made a remarkable progress. A number of classical machine learn-\ning methods such as boosted trees, random forest and support vector machines demon-\nstrateexcellentperformanceexceedingtheperformanceofearlierclassicalmachinelearn-\ning algorithms such as K-means or the logistic regression. Calibration of classification\nscores using traditional calibration methods such as Platt\u2019s scaling or the isotonic regres-\nsion can improve the performance of the underlying algorithms.\nMore recently, a growing body of research demonstrated that modern deep neural\nnetworks are no longer well calibrated (see, e.g., [44], [89], [94], [105]). Whilst very\nlarge deep computer vision models such as ResNet [45] are much more accurate than\nprevious classical architectures such as LeNet [77], such modern deep neural networks\narchitectures become significantly miscalibrated even as classification accuracy continues",
        "d8a776fe-05c4-4951-ab9c-cf48267c63aa": "70 Chapter 3. Probabilistic classification\nto improve. As demonstrated in Guoet al.[44] recent advances in deep learning such as\nmodel capacity, batch normalization, weight decay have strong negative effects on net-\nwork calibration.\nWhilst the earlier publications considered traditional neural networks well-calibrated\n[17], recent research has demonstrated [57] that both single neural network and ensem-\nbles of traditional neural networks are often poorly calibrated [57].\nThe calibration of both classical machine learning algorithms, neural and deep neural\nnetworks is becoming increasing important, especially in many real-world application\nsuch as healthcare [105] and self-driving cars [94]. In such critical applications obtaining\naccurate class probabilities significantly affects decision-making, such as whether to stop\na self-driving car when the algorithm is unsure about its prediction about whether there\nis a pedestrian on the road [94].\nThe main contribution of this chapter is the empirical study of the performance of two\ncomputationally e\ufb00icient calibration algorithms IVAP and CVAP in the multi-class clas-\nsification setting. Multi-class probabilistic predictors based on IVAP and CVAP perform\nwell, improving calibration in general and often resulting in performance improvements\nin comparison with the results obtained from using Platt\u2019s scaling and isotonic regres-\nsion. The improvements in performance in comparison with the results produced by the\nunderlying algorithms in multi-class classification problems are comparable to improve-\nments reported for binary classification problems (see, e.g., Vovket al. [141] and Pereira\net al.[105]). The proposed multi-class predictors improve calibration for most classifiers\nand depending on the data set are often more experimentally accurate than classical cali-\nbration methods such as Platt\u2019s scaler and isotonic regression.",
        "2807dcfd-8056-477c-819a-fa3ff37ae5b5": "71\nChapter 4\nProbabilistic regression\nThis chapter focuses on the study of probabilistic regression and applies conformal prediction to\nderive predictive distribution functions that are valid under a non-parametric assumption.\n4.1 Introduction\nChapter 3 introduced a new method of probabilistic prediction in the multi-class classi-\nfication setting. In this and further chapters (Chapters4\u20136), the focus is on probabilistic\nprediction for machine learning regression problems. We introduce and develop non-\nparametricapproachtopredictivedistributionfunctionsusingconformalprediction. This\napproach results in predictive distribution functions that are always valid for general ma-\nchine learning assumption (IID observations) in terms of guaranteed coverage. We de-\nfine predictive distribution functions in line with definitions and terminology in Shenet\nal. [121] and Schwederet al.[117].\nIn statistics, the theory of predictive distributions (see, e.g., [117], [121]) is based on\nthe assumption that samples are generated from a parametric model. Our novel contri-\nbution extends statistical \u201cpredictive confidence distributions\u201d in terms of their ability\nto generate probabilistic predictions using only limited assumptions customary of ma-\nchinelearning\u2014 namelythat theobservationsaregeneratedusing theIID model. Unlike\nin the parametric approach of statistical predictive distributions, our novel approach is\ncompletely data-driven non-parametric approach that does not require specification of\nthe data model.",
        "988a849e-d5ec-4d3f-84c5-19766b54645a": "72 Chapter 4. Probabilistic regression\nOurapproachgeneralizestheclassicalDempster-Hillprocedure(furtherformallyde-\nfined in Section4.8). Predictive distributions are briefly reviewed in4.2, for a more de-\ntailed review of predictive distributions, we refer the reader to [73].\n4.2 Predictive distributions\nLawless and Fredette [73] consider the general parametric prediction framework based\non the family of models:\nF(y j x; \u03b8) = P(Y \u2a7d y j X = x; \u03b8) (4.1)\nWhere following our terminologyY and X represent sets of objects and labels accord-\ningly. In this parametric approach, the distribution functionF is specified by the pa-\nrameter vector\u03b8. Given the rather general model specification,X can specify both cross-\nsectional and time-series data. If\u03b8 is known, it fully quantifies cumulative predictive dis-\ntributionof Y allowingtomakeprobabilisticstatementsabout Y givenX. If\u03b8 isunknown,\nit must be estimated from the data.\nDefinition 4.2.1(Coverage Probability). Coverage probability is defined as:\nCP (\u03b8) = P(fL1(X) \u2a7d Y \u2a7d L2(X); \u03b8g)\nPrediction intervals have well defined frequentist probability interpretation [73]. In\nthis and further chapters (Chapters4\u20136), the focus is on probabilistic prediction for ma-\nchine learning regression tasks using frequentist methods.\nLawlessandFredette[ 73]providedaunifiedtreatmentforbothfrequentistprediction\nintervals and predictive distributions by developing a definition of a predictive distribu-\ntion as a confidence distribution, as well as outlining the method of obtaining predic-\ntive distributions using pivotal quantiles, the method they have referred to as thepivotal\nmethod. The benefits of such an approach include obtaining prediction intervals and pre-\ndictive distributions that are both well-calibrated and have clear frequentist probability\ninterpretations. Such predictive distributions can be generated e\ufb00iciently and \u201cpossess",
        "62ee6289-95bb-4759-9f8a-b20558779b11": "4.3. Predictive distribution functions 73\ngood properties when considered as estimators of the true distributions ofY given X\u201d\n[73].\nShenet al.[121]furtherdeveloptheapproachinLawlessandFredette[ 73]bydefining\na general approach for constructing predictive distribution ofY using a confidence distri-\nbutionoftheunknownparameter \u03b8 asfollows(where H (\u03b8; y) isaconfidencedistribution\nfor \u03b8 derived from the training set):\nQ(z1, . . . , zn, (xn+1, y)) =\nZ\n\u03b8\u2208\u0398\nF\u03b8 (y) dH (\u03b8; y)\n4.3 Predictive distribution functions\nMany probabilistic regression applications require prediction of the labelyn+1 (we usey\nandyn+1 interchangeablytodescribethelabelofthe (n+1)stobservation)givena training\nsequence of observationszi = ( xi, yi), i = 1 , . . . , n and a new test objectxn+1 2 Rp. Given\nthe rather general model specification,X covers both regression problems withobjects x\nfrom the setX containing explanatory variables forlabels y from the setY, as well as time-\nseries problems with objectsx containing previous history and time-series based features\nof the labely.\nWe consider the regression problem withp attributes. Objects x, wherex 2 Rp, and\nlabels y, wherey 2 R, are IIDobservations zi = ( zi, yi) from theobservation spaceZ (Z 2\nRp+1 = Rp \u0002 R). The regression task is to predictyn+1, given a training sequence of\nobservations zi = ( zi, yi) and a new test objectxn+1 2 R.\n4.4 Randomized and conformal predictive distributions\nThestatisticalpredictivedistributionsin[ 73]and[ 121]relyonparametricmodels. Inthis\nchapterweuseconformalpredictionframeworktodevelopnovelmethodsofconstructing\npredictive distributions for the general non-parametric case. We initially define distribu-\ntion functions following Shenet al. [121, Definition 1], but allowing for randomization.",
        "32e845a3-7e98-4aa9-a814-75e3e17e63f0": "74 Chapter 4. Probabilistic regression\nRandomization is an inherent feature of conformal predictive distributions (CPDs), how-\never in practice they are affected little by it. LetU be the uniform probability measure on\nthe interval[0, 1].\nDefinition 4.4.1(Randomized Predictive System). A functionQ : ( Rp+1)n+1 \u0002 [0, 1] !\n[0, 1] iscalleda randomized predictive system(RPS)ifitsatisfiesthefollowingthreerequire-\nments:\nR1a For each training sequence(z1, . . . , zn) 2 (Rp+1)n and each test objectxn+1 2 Rp, the\nfunction Q(z1, . . . , zn, (xn+1, y), \u03c4) is monotonically increasing both iny 2 R and\nin \u03c4 2 [0, 1] (where \u201cmonotonically increasing\u201d is understood in the wide sense\nallowing for intervals of constancy). In other words, for each\u03c4 2 [0, 1], the function\ny 2 R 7! Q(z1, . . . , zn, (xn+1, y), \u03c4)\nis monotonically increasing, and for eachy 2 R, the function\n\u03c4 2 [0, 1] 7! Q(z1, . . . , zn, (xn+1, y), \u03c4)\nis monotonically increasing.\nR1b For each training sequence(z1, . . . , zn) 2 (Rp+1)n and each test objectxn+1 2 Rp,\nlim\ny\u2192\u2212\u221e\nQ(z1, . . . , zn, (xn+1, y), 0) = 0 (4.2)\nand\nlim\ny\u2192\u221e\nQ(z1, . . . , zn, (xn+1, y), 1) = 1 .\nR2 As the function of random training observationsz1 \u0018 P,\u2026, zn \u0018 P, a random test\nobservation zn+1 \u0018 P, and a random number\u03c4 \u0018 U, all assumed independent, the\ndistribution ofQ is uniform:\n8\u03b1 2 [0, 1] : P fQ(z1, . . . , zn, zn+1, \u03c4) \u0014 \u03b1g = \u03b1.",
        "59d6af4f-2179-481d-b6ca-50d4649d1c13": "4.4. Randomized and conformal predictive distributions 75\nThe output of the randomized predictive systemQ on a training sequencez1, . . . , zn\nand a test objectxn+1 is the function\nQn : ( y, \u03c4 ) 2 R \u0002 [0, 1] 7! Q(z1, . . . , zn, (xn+1, y), \u03c4), (4.3)\nwhich will be called therandomized predictive distribution (function)(RPD). Thethickness\nof an RPDQn is the infimum of the numbers\u03f5 \u0015 0 such that the diameter\nQn(y, 1) \u0000 Qn(y, 0) (4.4)\nof the set\nfQn(y, \u03c4 ) j \u03c4 2 [0, 1]g (4.5)\nis at most\u03f5 for ally 2 R except for finitely many values. Theexception sizeof Qn is the\ncardinality of the set ofy for which the diameter (4.4) exceeds the thickness ofQn. Notice\nthat a priorithe exception size can be infinite [146]. Of primary interest are RPDs of thick-\nness 1\nn+1 everywhere but at mostn points on axisy, wheren is the size of the training set\nwith Q(z1, . . . , zn, zn+1, \u03c4) being a continuous function of\u03c4. The set (4.5) will therefore\nbe a closed interval in[0, 1].\nFour examples of predictive distributions are shown in Figure4.1 below as shaded\nareas. The length of the training sequence for these plots isn = 10 (see Section4.9 for\ndetails). The plots are examples of instance ofQ10, such instance has the width1/11\neverywhere but 10 points on axis y. The width is given by the width of the interval\n[Q(y, 0), Q(y, 1)], whereQ := Q10.\nIn conformal prediction, the starting point is the definition and choice of the confor-\nmity measure, we start by defining the conformity measure.\nDefinition 4.4.2(Conformity measure). The conformity measure is a measurable func-\ntion A: ( Rp+1)n+1 ! R that is invariant with respect to permutations of the firstn obser-\nvations: for any sequence(z1, . . . , zn) 2 (Rp+1)n, anyzn+1 2 Rp+1, and any permutation\n\u03c0 of f1, . . . , ng,\nA(z1, . . . , zn, zn+1) = A \u0000z\u03c0(1), . . . , z\u03c0(n), zn+1\n\u0001.",
        "5b72a45c-7213-4135-a996-0cfc11b7dba0": "76 Chapter 4. Probabilistic regression\n\u22124 \u22122 0 2 4\n0.0 0.2 0.4 0.6 0.8 1.0\ny\nQ (y)\n\u22124 \u22122 0 2 4\n0.0 0.2 0.4 0.6 0.8 1.0\ny\nQ (y)\n\u22124 \u22122 0 2 4\n0.0 0.2 0.4 0.6 0.8 1.0\ny\nQ (y)\n\u22124 \u22122 0 2 4\n0.0 0.2 0.4 0.6 0.8 1.0\ny\nQ (y)\nFIGURE4.1: Examplesoftruepredictivedistributionfunctions(black), their\nconformal estimates (represented by the shaded areas), and the distribu-\ntion functions output by simplified Oracle I (red) and Oracle II (blue) for a\ntiny training sequence (of length 10 with two attributes, the first one being\nthedummyall-1attribute); inblackandwhite, thetruepredictivedistribu-\ntion functions are the thick lines, and Oracle I is always farther from them\nin the uniform metric than Oracle II is\nFunction A measureshow largethe label yn+1 in zn+1 is, based on seeing the obser-\nvations z1, . . . , zn and the objectxn+1 of zn+1. A simple example of conformity measure\nis\nA(z1, . . . , zn+1) := yn+1 \u0000 \u02c6yn+1, (4.6)\nWhere \u02c6yn+1 is the prediction foryn+1 computed fromxn+1 and z1, . . . , zn as training se-\nquence.",
        "d0e12969-76a5-4ca4-ac48-0d70b3a70782": "4.4. Randomized and conformal predictive distributions 77\nDefinition 4.4.3(Conformity score). The conformity score\u03b1y\ni is defined by\n\u03b1y\ni := A(z1, . . . , zi\u22121, zi+1, . . . , zn, (xn+1, y), zi), i = 1 , . . . , n,\n\u03b1y\nn+1 := A(z1, . . . , zn, (xn+1, y)). (4.7)\nHaving given the definitions of conformity measure4.4.2 and conformity score4.4.3\nwe can proceed to define the conformal transducer as follows:\nDefinition 4.4.4(Conformaltransducer). Theconformaltransducerdeterminedbyacon-\nformity measureA is\nQ(z1, . . . , zn, (xn+1, y), \u03c4) := 1\nn + 1\n\f\f\bi = 1 , . . . , n + 1 j \u03b1y\ni < \u03b1y\nn+1\n\t\f\f\n+ \u03c4\nn + 1\n\f\f\bi = 1 , . . . , n + 1 j \u03b1y\ni = \u03b1y\nn+1\n\t\f\f, (4.8)\nA function is a conformal transducer if it is the conformal transducer determined by\nsome conformity measure [143].\nDefinition 4.4.5(Conformal predictive system). Conformal predictive system (CPS) is a\nfunction which is both a conformal transducer and a randomized predictive system.\nDefinition 4.4.6(Conformal predictive distribution). Conformal predictive distribution\n(CPD) is a functionQn defined by (4.3) for a conformal predictive systemQ.\nDefinition 4.4.7(Conformal predictor). Any conformal transducerQ and Borel setA \u0012\n[0, 1] defines the Conformal predictor:\n\u0393A(z1, . . . , zn, xn+1, \u03c4) := fy 2 R j Q(z1, . . . , zn, (xn+1, y), \u03c4) 2 Ag.\nConformal transducers possess the standard property of validity - the valuesQ are\ndistributed uniformly on[0, 1] under the assumption thatz1, . . . , zn+1 are IID and\u03c4 is\ngenerated from the uniform probability distributionU on [0, 1] and independently of\nz1, . . . , zn+1 from the uniform probability distribution U on [0, 1] (see [137, Proposi-\ntion 2.8]). This property of conformal transducers is equivalent to the requirementR2\nin the definition4.4.1 of a randomized predictive system (RPS) and ensures validity.",
        "5b265a05-2e10-4b13-b79e-f883843c5feb": "78 Chapter 4. Probabilistic regression\nIn conformal prediction, the customary interpretation of (4.8) is that it is a random-\nized p-value for testing the null hypothesis of the observations being IID. In the case of\nconformal predictive distributions the situation is slightly different \u2014 the informal al-\nternative H1 hypothesis is thatyn+1 = y is smaller than expected under the IID model.\nUnder such alternative hypothesis the (4.8) can be interpreted as a degree of conformity\nof the observation(xn+1, yn+1) tothe remaining observations. One should note, that with\nthe one-sided nature of this notion of conformity a labely can only be non-conforming\nif it is too small. A large label is never non-conforming (\u201cstrange\u201d). Whilst such notion\nof conformity using a label can only be strange (non-conforming) if it is too small, large\nvalueoflabelisneverstrange. Thisnotionofconformityspecifiedbyconformitymeasure\n(4.10) is somewhat counterintuitive, and will only be used as a technical tool.\n4.5 Monotonic conformity measures\nTounderstandpropertiesofconformalpredictivedistributions, wefirstexplore monotonic\nconformity measures that are defined as follows:\nDefinition 4.5.1(Monotonicconformitymeasures) . Aconformitymeasure A ismonotonic\nif A(z1, . . . , zn+1) if it is either monotonically increasing inyn+1,\nyn+1 \u0014 y\u2032\nn+1 =) A(z1, . . . , zn, (xn+1, yn+1)) \u0014 A(z1, . . . , zn, (xn+1, y\u2032\nn+1));\nor monotonically decreasing iny1,\nyn+1 \u0014 y\u2032\nn+1 =) A(z1, . . . , zn, (xn+1, yn+1)) \u0015 A(z1, . . . , zn, (xn+1, y\u2032\nn+1)); (4.9)\nConformalpredictivedistributionsdefinedusingmonotonicconformitymeasuressat-\nisfy condition R1a of the definition4.4.1 by Lemma 1 below. An simple example of a\nmonotonic conformity measure is:\nA(z1, . . . , zm, (x, y)) := y \u0000 \u02c6y (4.10)",
        "b9000110-602e-4c13-bfd3-e87c8d5efccc": "4.5. Monotonic conformity measures 79\nWhere \u02c6yn+1 is produced by theK-nearest neighbours regression algorithm with value of\n\u02c6yn+1 calculated as the average value of labels of theK nearest neighbours ofxn+1 (here\ny(1), . . . , y(n) is the sequencey1, . . . , yn sorted in the order of increasing distances between\nxi and xn+1):\n\u02c6yn+1 := 1\nK\nKX\nk=1\ny(k) (4.11)\nIn the case of\u02c6yn+1 defined by 4.11, the conformity measure is not only monotonically\nincreasing but satisfies additionally the following condition:\nlim\ny\u2192\u00b1\u221e\nA(z1, . . . , zn, (xn, y)) = \u00061\nTheconformaltransducerdefinedusingconformitymeasure( 4.10)where \u02c6yn+1 isdefined\nusing4.11therefore also satisfies conditionR1b of the definition and so is both a random-\nized predictive system (RPS) and a conformal predictive system (CPS).\nCriterion of being a CPS\nUnfortunately, many important conformity measures are not monotonic, and the next\nlemma introduces a weaker su\ufb00icient condition for a conformal transducer to be an RPS.\nLemma 1. The conformal transducer determined by a conformity measureA satisfies condition\nR1a if, for eachi 2 f 1, . . . , ng, each training sequence(z1, . . . , zn) 2 (Rp+1)n, and each test\nobject xn+1 2 Rp, \u03b1y\nn+1 \u0000 \u03b1y\ni is a monotonically increasing function ofy 2 R (in the notation\nof (4.7)).\nWe can strengthen the conclusion of the lemma to the conformal transducer deter-\nmined byA being an RPS (and, therefore, a CPS) if, e.g.,\nlim\ny\u2192\u00b1\u221e\n\u0000\u03b1y\nn+1 \u0000 \u03b1y\ni\n\u0001= \u00061.",
        "943fe534-56d1-4011-b8b6-288ec39fc4f0": "80 Chapter 4. Probabilistic regression\n4.6 Least Squares Prediction Machine\nIn this section the Least Squares Prediction Machine (LSPM) is introduced. The LSPM\nis similiar to the Ridge Regression Confidence Machine (RRCM) described in [137, Sec-\ntion 2.3], with the key distinction that it produces predictive distribution functions rather\nthan prediction intervals.\nDefinition 4.6.1(Ordinary LSPM). We define theordinary LSPMas the conformal trans-\nducer with the conformity measure\nA(z1, . . . , zn+1) := yn+1 \u0000 \u00cayn+1 (4.12)\nWhereyn+1 and \u00cayn+1 arethelabelandthepredictionfor yn+1 accordingly. Thepredic-\ntion \u00cayn+1 is computed from the training sequencez1, . . . , zn+1 using the linear regression.\nThe important distinction of the LSPM from the ordinary linear regression is that in the\nordinary LSPM the training sequencez1, . . . , zn+1 includes zn+1.\nThe residual (4.12) in the definition4.6.1of the ordinary LSPM is the ordinary regres-\nsion residual, however similar to how residuals are considered in statistics we consider\nthree kinds of LSPM \u2014ordinary LSPM, deleted LSPMand studentized LSPM.\nDefinition 4.6.2(DeletedLSPM). ThedeletedLSPMisdefinedusingtheconformitymea-\nsure:\nA(z1, . . . , zn+1) := yn+1 \u0000 \u02c6yn+1, (4.13)\nThe deleted LSPM differs from the ordinary LSPM in that\u00cayn+1 is replaced by the\nprediction \u02c6yn+1 foryn+1 computed using the ordinary linear regression (OLS) fromxn+1\nand z1, . . . , zn as training sequence.\nUnlike the definition4.6.1 of ordinary LSPM in the definition4.6.2 of deleted LSPM\nthe training sequence does not includezn+1. The studentized LSPM is somewhat mid-\nway between ordinary and deleted LSPM and is defined in4.6.4. Both the ordinary and\ndeleted LSPM are not RPS, as their outputQn (see (4.3)) is not necessarily monotonically\nincreasing iny as required inR1a of the definition4.4.1. Whilst this is unfortunate, below\nwe will show that this can happen only in the presence of high-leverage points.",
        "75ac724d-a6b5-46d6-b81f-5050c1ee242b": "4.6. Least Squares Prediction Machine 81\n4.6.1 High leverage points\nWe denote by\u00afX the data matrix that has dimensions of(n+ 1)\u0002p, each row of the matrix\n\u00afX represents theith object (note that the data matrix\u00afX includes both the training set and\nthe test object as it hasn + 1 rows.\nDefinition 4.6.3(The hat matrix). The hat matrix for then + 1 observations z1, . . . , zn+1\nis defined as:\n\u00afH = \u00afX( \u00afX\u2032 \u00afX)\u22121 \u00afX\u2032. (4.14)\nWe use\u00afhi,j to denote the elements of this hat matrix wherei refers to theith row\nand j refers to thejth column. For the diagonal elements of the hat matrix we use the\nshorthand expression\u00afhi. The following proposition can be derived from Lemma1 in the\nexplicit form (analogous to Algorithm1 below but using (4.23)) of the ordinary LSPM.\nProposition 1.The functionQn that is output by the ordinary LSPM (see(4.3)) is monotonically\nincreasing iny provided that\u00afhn+1 < 0.5.\nThe necessary condition for theQn to be monotonically increasing,\u00afhn+1 < 0.5 de-\nfines test objectxn+1 as not a very influential pointfollowing the terminology in Chatterjee\nand Hadi[18, Section 4.2.3.1]. The assumption of the test objectxn+1 being not a very\ninfluential point (\u00afhn+1 < 0.5) in Proposition1 turns out to be essential:\nProposition 2. Proposition 1 ceases to be true if the constant0.5 in it is replaced by a larger\nconstant.\nThe next two propositions demonstrate that for the case of deleted LSPM, defined by\n(4.6.2), even stricter condition is required than for the ordinary LSPM:\u00afhi < 0.5 for all\ni = 1 , . . . , n.\nProposition 3. The functionQn that is output by the deleted LSPM according to(4.3) is mono-\ntonically increasing iny provided thatmaxi=1,...,n \u00afhi < 0.5.\nFor the deleted LSPM the following analogue of Proposition2 is as follows.\nProposition 4. Proposition 3 ceases to be true if the constant0.5 in it is replaced by a larger\nconstant.",
        "3eb9ee30-31d8-47f2-ab0a-1719518d3479": "82 Chapter 4. Probabilistic regression\nStudentized LSPM\nFromthe point of viewof predictivedistributions, the bestchoice is thereforestudentized\nLSPM.\nDefinition 4.6.4 (Studentized LSPM). The studentized LSPM is defined using the con-\nformity measure:\nA(z1, . . . , zn+1) = yn+1 \u0000 \u00cayn+1p\n1 \u0000 \u00afhn+1\nAs deleted residualsyi \u0000 \u02c6yi can be represented as(yi \u0000 \u00cayi)/(1 \u0000 \u00afhi), where \u02c6yi is the\nprediction foryi computed usingz1, . . . , zi\u22121, zi+1, . . . , zn+1 as training sequence, the stu-\ndentized LSPM is in a way a form of an intermediate residual between those residuals\nin the definitions of the ordinary and deleted LSPM. An important advantage of studen-\ntized LSPM is that no additional assumptions of low leverage are required to satisfy the\nrequirements of them being predictive distributions. Accordingly, the following proposi-\ntion holds:\nProposition 5. The studentized LSPM is an RPS and, therefore, a CPS.\nThe studentized LSPM in an explicit form\nAlgorithms 1 and 2 derive two explicit forms for the studentized LSPM (Algorithms1\nand 2). The versions for the ordinary and deleted LSPM are similar, the explicit form is\ngiven only for the ordinary version of the LSPM and is particularly intuitive. Predictive\ndistributions (4.3) are represented in the form:\nQn(y) := [ Qn(y, 0), Qn(y, 1)]\nThe functionQn maps each potential labely 2 R to a closed interval ofR. In the ordinary\nLeast Squares regression, the vector(\u00cay1, . . . , \u00cayn+1)\u2032 of predictions can be represented as\nthe product of the hat matrix\u00afH and the vector(y1, . . . , yn+1)\u2032 of labels. Therefore, we can\nrepresent the studentized residuals as:\n\u03b1y\nn+1 \u0000 \u03b1y\ni = Biy \u0000 Ai, i = 1 , . . . , n, (4.15)",
        "3b708d76-3909-45f2-85ff-f4774206ea06": "4.6. Least Squares Prediction Machine 83\nAlgorithm 1Least Squares Prediction Machine\nRequire: A training sequence(xi, yi) 2 Rp \u0002 R, i = 1 , . . . , n.\nRequire: A test objectxn+1 2 Rp.\n1: Set \u00afX to the data matrix for the givenn + 1 objects.\n2: Define the hat matrix\u00afH by (4.14).\n3: for i 2 f1, 2, . . . , ng do\n4: Define Ai and Bi by (4.17) and (4.16), respectively.\n5: Set Ci := Ai/Bi.\n6: end for\n7: Sort C1, . . . , Cn in the increasing order obtainingC(1) \u0014 \u0001 \u0001 \u0001 \u0014 C(n).\n8: Return the predictive distribution (4.18) foryn+1.\nWhere, following the notation of (4.7), y is the label of the(n + 1)st objectxn+1 and\nBi :=\n\u00bb\n1 \u0000 \u00afhn+1 +\n\u00afhi,n+1p\n1 \u0000 \u00afhi\n, (4.16)\nAi =\nPn\nj=1 \u00afhj,n+1yj\np\n1 \u0000 \u00afhn+1\n+\nyi \u0000 Pn\nj=1 \u00afhi,jyj\np\n1 \u0000 \u00afhi\n(4.17)\nAll the Bi are assumed to be defined and positive. We further setCi := Ai/Bi for all\ni = 1 , . . . , n and then sort allCi in the increasing order to obtain the sequence beC(1) \u0014\n\u0001 \u0001 \u0001 \u0014 C(n). LetC(0) := \u00001 and C(n+1) := 1. The predictive distribution is computed as:\nQn(y) :=\n8\n>><\n>>:\n[ i\nn+1 , i+1\nn+1 ] if y 2 (C(i), C(i+1)) for i 2 f0, 1, . . . , ng\n[i\u2032\u22121\nn+1 , i\u2032\u2032+1\nn+1 ] if y = C(i) for i 2 f1, . . . , ng,\n(4.18)\nwhere i\u2032 := min fj j C(j) = C(i)g and i\u2032\u2032 := max fj j C(j) = C(i)g. From4.18 it is clear that\nthe thickness of this CPD is 1\nn+1 with the exception size equal to the number of distinct\nCi, at mostn. The overall computation for the Least Square Prediction Machine (LSPM)\nis described in the Algorithm1, where the data matrix\u00afX has x\u2032\ni, i = 1 , . . . , n + 1, as itsith\nrow, the data matrix dimensions are(n + 1) \u0002 p.\nThe assumed condition that allBi are defined and positive,i = 1 , . . . , n is satisfied\nby using the result from [18, Property 2.6(b)]: \u00afhn+1 = 1 implies that \u00afhi,n+1 = 0 for\ni = 1 , . . . , n. Henceforth this condition is equivalent to\u00afhi < 1 for alli = 1 , . . . , n + 1. By\n[93, Lemma 2.1(iii)], this means that the rank of the extended data matrix\u00afX is p and it\nremains p after removal of any one of itsn + 1 rows. If this condition is not satisfied, we",
        "124d9bac-8627-44ef-a68f-02cc9dbbea92": "84 Chapter 4. Probabilistic regression\ndefine Qn(y) := [0 , 1] for ally. This ensures that the studentized LSPM is a CPS.\nThe batch version of the studentized LSPM\nWhen test set consists of a sequence of objectsxn+1, . . . , xn+m (instead of just one test\nobject xn+1), a much more e\ufb00icient implementation of the LSPM can be designed by pre-\ncomputing the hat matrix for the training objectsx1, . . . , xn, and then updating compu-\ntations for each test objectxn+j based on the results from Sherman-Morrison-Woodbury\ntheorem: see, e.g., Chatterjee and Hadi [18, p. 23, (2.18)\u2013(2.18c)].\nThe computational update proceeds as follows. Let\u2019s set\ngi := x\u2032\ni(X\u2032X)\u22121xn+1, i = 1 , . . . , n + 1. (4.19)\nWhere X is the data matrix for the the firstn observations (matrix dimension isn \u0002 p,\ndenoteits ithrowas x\u2032\ni,i = 1 , . . . , n. Finally,letH bethecorrespondinghatmatrix( n\u0002n)\nfor the firstn objects:\nH := X(X\u2032X)\u22121X\u2032 (4.20)\nWith elements denotedhi,j and hi,i sometimes abbreviated tohi. The full hat matrix\u00afH is\ntherefore larger thanH, with the extra entries computed as follows:\n\u00afhi,n+1 = \u00afhn+1,i = gi\n1 + gn+1\n, i = 1 , . . . , n + 1. (4.21)\nThe remaining entries of\u00afH are\n\u00afhi,j = hi,j \u0000 gigj\n1 + gn+1\n, i, j = 1 , . . . , n. (4.22)\nThe overall algorithm for the Least Squares Prediction Machine (batch version) is sum-\nmarized as Algorithm2.",
        "ddf44377-4520-4fb6-a592-ed59e8eef2e5": "4.7. Validity of the LSPM in the online mode 85\nAlgorithm 2Least Squares Prediction Machine (batch version)\nRequire: A training sequence(xi, yi) 2 Rp \u0002 R, i = 1 , . . . , n.\nRequire: A test sequencexn+j 2 Rp, j = 1 , . . . , m.\n1: Set X to the data matrix for then training objects.\n2: Set H = ( hi,j) to the hat matrix (4.20).\n3: for j 2 f1, 2, . . . , mg do\n4: Set xn+1 := xn+j.\n5: Define an(n + 1) \u0002 (n + 1) matrix \u00afH = ( \u00afhi,j) by (4.21) and (4.22).\n6: for i 2 f1, 2, . . . , ng do\n7: Define Ai and Bi by (4.17) and (4.16), respectively.\n8: Set Ci := Ai/Bi.\n9: end for\n10: Sort C1, . . . , Cn in the increasing order obtainingC(1) \u0014 \u0001 \u0001 \u0001 \u0014 C(n).\n11: Return the predictive distribution (4.18) for the label ofxn+j.\n12: end for\nThe ordinary LSPM\nA similar calculation demonstrates that the ordinary LSPM has an intuitive and e\ufb00icient\nrepresentation (see, e.g., [12, Appendix A]):\nCi = Ai\nBi\n= \u02c6yn+1 + (yi \u0000 \u02c6yi)1 + gn+1\n1 + gi\n, (4.23)\nTheLeastSquarespredictionsfor yn+1 andyi are \u02c6yn+1 and \u02c6yi,thatarebothcomputedfrom\nthe test objectsxn+1 and xi, respectively and the observationsz1, . . . , zn as the training\nsequence.\nThe predictive distribution is defined by formula (4.18). the fraction1+gn+1\n1+gi\nin (4.23)\nis typically and asymptotically (at least under the assumptions A1\u2013A4 stated in the next\nsection) close to 1, and can usually be ignored. The two other versions of the LSPM also\ntypically have\nCi \u0019 \u02c6yn+1 + (yi \u0000 \u02c6yi). (4.24)\n4.7 Validity of the LSPM in the online mode\nAlgorithm 1) outlined a procedure to compute \u201cfuzzy\u201d distribution functionQn based\non a training sequencezi = ( xi, yi), i = 1 , . . . , n, and a test objectxn+1. We use notation",
        "cfaaba6f-049a-42d5-bcb1-5cf4f3adf102": "86 Chapter 4. Probabilistic regression\nQn(y) todenoteanintervaland Qn(y, \u03c4 ) todenoteapointinsidethatinterval,asexplained\npreviously.\nIn the online prediction mode, the computations proceed as follows:\nProtocol 4.7.1. ONLINE MODE OF PREDICTION\nNature generates an observationz1 = ( x1, y1) from a probability distributionP;\nfor n = 1 , 2, . . . do\nNature independently generates a new observationzn+1 = ( xn+1, yn+1) from P;\nForecaster announcesQn, a predictive distribution based on(z1, . . . , zn) and xn+1;\nset pn := Qn(yn+1, \u03c4n), where\u03c4n \u0018 U\nend for\nThe\u201cgroundtruth\u201ddistribution P andyn+1 arenotavailabletotheForecasterwhencom-\nputing Qn. We adapt condition R2 for the online mode by strengthening it as follows:\nTheorem 1([137], Theorem 8.1). In the online mode of prediction (in which(zi, \u03c4i) \u0018 P \u0002 U\nare IID), the sequence(p1, p2, . . .) is IID and(p1, p2, . . .) \u0018 U\u221e, provided that Forecaster uses\nthe studentized LSPM (or any other conformal transducer).\nThepropertyofvalidityassertedinTheorem 1ismarginal,inthatwedonotassertthat\nthe distribution ofpn is uniform conditionally onxn+1. Conditional validity is attained\nby the LSPM only asymptotically and under additional assumptions, as is demonstrated\nin the the next section.\n4.8 Asymptotic e\ufb00iciency\nIn this section we outline some basic results about the LSPM\u2019s e\ufb00iciency. When standard\nIID assumtpion holds, the nonparametric LSPM has a property of validity. It is natural\nto pose the question about the cost of validity (in terms of potential loss in e\ufb00iciency) in\nsituation when data distribution is parametric or even in ideal situation when Bayesian\nassumptionsalsohold[ 146]. SuchquestionwasaskedindependentlybyEvgenyBurnaev\n[13] and Larry Wasserman [79].\nFor our study of e\ufb00iciency we will use the assumptions that are not in line with the\ngeneral IID model by assuming that strong parametric data generation process for the",
        "373698a0-fa87-4c69-af76-682722bb6863": "4.8. Asymptotic e\ufb00iciency 87\nlabelsyi given the corresponding objectsxi. In addition, we will also remove the assump-\ntion of randomness for the objectsx1, x2, . . . by instead allowing them to be fixed vector.\nAs it turns out, the two main results of this section \u2014 Theorems2and3\u2014 do not depend\non the assumptions of randomness and IID.\nGiven fixed objects,x1, x2, . . ., we assume that the labelsy1, y2, . . . are generated using\nthe linear parametric model, withw being a vector parameter (w 2 Rp) and\u03bei being IID\nnormally distributed (N(0, \u03c32))random variables:\nyi = w\u2032xi + \u03bei, (4.25)\nThis linear parametric model contains two parameters: vectorw and positive number\n\u03c3. We form the training set from the firstn elements of an otherwise infinite sequence of\nobservations (x1, y1), (x2, y2), . . .. To study e\ufb00iciency asymptotically, we then letn ! 1.\nTo summarise all the assumptions required for the e\ufb00iciency results:\nA1 The sequencex1, x2, . . . is bounded:supi kxik < 1.\nA2 The first component of each vectorxi is 1.\nA3 Theempiricalsecond-momentmatrixhasitssmallesteigenvalueeventuallybounded\naway from 0:\nlim inf\nn\u2192\u221e\n\u03bbmin\n \n1\nn\nnX\ni=1\nxix\u2032\ni\n!\n> 0,\nwhere \u03bbmin stands for the smallest eigenvalue.\nA4 The labelsy1, y2, . . . are generated according to the linear model (4.25): yi = w\u2032xi +\u03bei,\nwhere \u03bei are independent Gaussian noise random variables distributed asN(0, \u03c32).\nIn our study of e\ufb00iciency, we consider the three versions of the LSPM and also three\n\u201coracles\u201d. All of the three oracles know the data generation model (4.25). In addition\nOracle III has complete information about both parameters of the data generation model\n(4.25) (bothw and\u03c3), Oracle II knows\u03c3, but does not knoww and finally Oracle I knows\nneither w nor \u03c3. Oracles that don\u2019t know certain parameters have to estimate them from\nthe data.",
        "cc64cc86-1032-440a-8504-a74f7b75c4e0": "88 Chapter 4. Probabilistic regression\nProper Oracle Idoesnotknowbothparametersandhencecomputesboth w and\u03c3 using\nOrdinary Least Squares procedures to obtain the standard predictive distribution for the\nlabelyn+1 of the test objectxn+1 based on the training sequence of the firstn observations\nand xn+1:\n\u02c6yn+1 +\np\n1 + gn+1\u02c6\u03c3ntn\u2212p, (4.26)\nwhere gn+1 is defined in (4.19),\n\u02c6yn+1 := x\u2032\nn+1(X\u2032X)\u22121X\u2032Y,\n\u02c6\u03c3n :=\n\u00c3\n1\nn \u0000 p\nnX\ni=1\n(yi \u0000 \u02c6yi)2, \u02c6yi := x\u2032\ni(X\u2032X)\u22121X\u2032Y,\nX is the data matrix for the training sequence (then \u0002 p matrix whose ith row isx\u2032\ni,\ni = 1 , . . . , n), Y is the vector(y1, . . . , yn)\u2032 of the training labels, andtn\u2212p is Student\u2019st-\ndistribution withn \u0000 p degrees of freedom; see, e.g., [118, Section 5.3.1] or [148, Exam-\nple 3.3].\nThestandardpredictiondistributionobtainedbyOracleI 4.26isslightlydifferentfrom\nits simplified version that is more popular in the literature on empirical processes for\nresiduals wheresimplified Oracle Ioutputs:\nN \u0000\u02c6yn+1, \u02c6\u03c32\nn\n\u0001. (4.27)\nThe difference between the two versions is, however, asymptotically negligible [108], and\nthe results stated below will be applicable to both versions.\nProper Oracle IIthathasaccesstoinformationabout \u03c3,butnot w outputsthepredictive\ndistribution:\nN \u0000\u02c6yn+1, (1 + gn+1)\u03c32\u0001, (4.28)\na simplified version of which (simplified Oracle II) is:\nN \u0000\u02c6yn+1, \u03c32\u0001. (4.29)",
        "73dd6c94-e8d4-46c4-8402-a80f767821e5": "4.8. Asymptotic e\ufb00iciency 89\nSimilar to the case ofProper Oracle II, the difference between the two versions of Oracle II\ncomputations (full and simplified) is again asymptotically negligible under our assump-\ntions 4.8.\nFinally,Oracle IIIoutputs the predictive distribution:\nN \u0000w\u2032xn+1, \u03c32\u0001.\nWe use our notation for the conformal predictive distributionQn (4.3), as before,QI\nn\ndenotes simplified or proper Oracle I\u2019s predictive distribution, (4.27) or (4.26) (Theo-\nrem 2 will hold for both), andQII\nn to denote simplified or proper Oracle II\u2019s predictive\ndistribution, (4.29) or (4.28) (Theorem3 will hold for both). Theorems2 and 3 are ac-\ncordingly applicable to all three versions of the LSPM.\nTheorem 2. The random functionsGn : R ! R defined by\nGn(t) := pn\n\u00c4\nQn(\u02c6yn+1 + \u02c6\u03c3nt, \u03c4) \u0000 QI\nn(\u02c6yn+1 + \u02c6\u03c3nt)\n\u00e4\nweakly converge to a Gaussian processZ with mean zero and covariance function\ncov(Z(s), Z(t)) = \u03a6( s) (1 \u0000 \u03a6(t)) \u0000 \u03d5(s)\u03d5(t) \u0000 1\n2st\u03d5(s)\u03d5(t), s \u0014 t.\nTheorem 3. The random functionsGn : R ! R defined by\nGn(t) := pn\n\u00c4\nQn(\u02c6yn+1 + \u03c3t, \u03c4 ) \u0000 QII\nn (\u02c6yn+1 + \u03c3t)\n\u00e4\nweakly converge to a Gaussian processZ with mean zero and covariance function\ncov(Z(s), Z(t)) = \u03a6( s) (1 \u0000 \u03a6(t)) \u0000 \u03d5(s)\u03d5(t), s \u0014 t.\nIn Theorems2 and 3, we have\u03c4 \u0018 U; alternatively, they will remain true if we fix\u03c4 to\nany value in[0, 1]. For simplified oracles,we haveQI\nn(\u02c6yn+1 + \u02c6\u03c3nt) = \u03a6( t)in Theorem2and\nQII\nn (\u02c6yn+1 + \u03c3t) = \u03a6( t) in Theorem3. For proofs of Theorem2 and Theorem3, see [146].\nApplying Theorems2 and 3 to a fixed argumentt, we obtain (dropping\u03c4 altogether):",
        "ea0e54b3-e29d-4f42-bc40-e5fc7b4e80d1": "90 Chapter 4. Probabilistic regression\nFIGURE 4.2: The asymptotic variances for the Dempster-Hill (DH) proce-\ndure as compared with the truth (Oracle III, red) and for the LSPM and\nDHprocedureascomparedwiththeoracularproceduresforknown \u03c3 (Or-\nacle II, blue) and unknown\u03c3 (Oracle I, black); in black and white, red is\nhighest, blue is intermediate, and black is lowest\nCorollary 1. For a fixedt 2 R,\npn\n\u00c4\nQn(\u02c6yn+1 + \u02c6\u03c3nt) \u0000 QI\nn(\u02c6yn+1 + \u02c6\u03c3nt)\n\u00e4\n) N\n\u00c5\n0, \u03a6(t)(1 \u0000 \u03a6(t)) \u0000 \u03d5(t)2 \u0000 1\n2t2\u03d5(t)2\n\u00e3\nand\npn\n\u00c4\nQn(\u02c6yn+1 + \u03c3t) \u0000 QII\nn (\u02c6yn+1 + \u03c3t)\n\u00e4\n) N \u00000, \u03a6(t)(1 \u0000 \u03a6(t)) \u0000 \u03d5(t)2\u0001.\nFigure4.2presentsplotsfortheasymptoticvariances, giveninCorollary 1, forthetwo\noracular predictive distributions: black for Oracle I (\u03a6(t)(1 \u0000 \u03a6(t)) \u0000 \u03d5(t)2 \u0000 1\n2 t2\u03d5(t)2 vs\nt) and blue for Oracle II (\u03a6(t)(1 \u0000 \u03a6(t)) \u0000 \u03d5(t)2 vs t); the red plot will be discussed later\nin this section. The two asymptotic variances coincide att = 0 , where they attain their\nmaximum of between0.0908 and 0.0909.\nWecan conclude that fordata generatedusing the Gaussian model (4.25) under natu-\nralassumptions,theLSPMisasymptoticallyclosetothepredictivedistributionsgenerated\nby Oracles I and II. The LSPM is therefore approximately conditionally valid and e\ufb00icient\n(i.e., valid and e\ufb00icient givenx1, x2, . . .). On the other hand, Theorem1 guarantees the",
        "ad096bc7-7b40-44ce-aa72-961d76518ae9": "4.8. Asymptotic e\ufb00iciency 91\nmarginal validity of the LSPM under the general IID model, regardless of whether (4.25)\nholds.\nComparison with the Dempster-Hill procedure\nThissubsectionoutlinesaclassicalprocedurewhichwerefertoasthe Dempster-Hill proce-\ndure as it was most clearly articulated in [30, p. 110] and [50, 49]. Both Dempster and Hill\ntrace their ideas to nonparametric version of Fisher\u2019s fiducial method ([38]; [37]). Fisher,\nhowever, was mostly interested in confidence distributions for quantiles, rather than full\npredictive distributions. Hill [49] referred to his procedure as Bayesian nonparametric\npredictive inference, which was later abbreviated to nonparametric predictive inference\n(NPI) by Frank Coolen [3]. An important predecessor of Dempster and Hill was Jeffreys\n[55], who postulated what was later denoted by Hill asA(2) ([70] and [119] contain dis-\ncussions of Jeffreys\u2019s paper and Fisher\u2019s reaction).\nDefinition 4.8.1. The Dempster-Hill procedure is the conformal predictive system deter-\nmined by the conformity measure\nA(z1, . . . , zn+1) = A(y1, . . . , yn+1) = yn+1; (4.30)\nSuch procedure is used when the objectsxi are absent and it can be regarded as the\nspecial case of the LSPM when there are no objects (p = 0 ) \u2014 alternatively one can con-\nsider the situation whenp = 1 but assume that all objects arexi = 0 . In this case, the\npredictions \u02c6y will always be0 and the hat matrices are\u00afH = 0 and H = 0 , this means that\n(4.12), (4.13), and (4.15) all reduce to (4.30).\nIn the absence of ties, such predictive distribution becomes:\nQn(y) :=\n8\n>><\n>>:\n[ i\nn+1 , i+1\nn+1 ] if y 2 (y(i), y(i+1)) for i 2 f0, 1, . . . , ng\n[ i\u22121\nn+1 , i+1\nn+1 ] if y = y(i) for i 2 f1, . . . , ng\n(4.31)\n(cf. (4.18)), wherey(1) \u0014 \u0001 \u0001 \u0001 \u0014 y(n) are theyi sorted in the increasing order,y(0) := \u00001,\nand y(n+1) := 1.",
        "8cca1068-6827-4820-a52a-78fd3315770e": "92 Chapter 4. Probabilistic regression\nThis is essentially Hill\u2019s assumptionA(n) (which he also denotedAn); in his words:\n\u201cAn assertsthatconditionalupontheobservations X1, . . . , Xn,thenextobservation Xn+1\nis equally likely to fall in any of the open intervals between successive order statistics\nof the given sample\u201d [50, Section 1]. The set of all continuous distribution functionsF\ncompatible with Hill\u2019sA(n) coincides with the set of all continuous distribution functions\nF satisfying F(y) 2 Qn(y) for ally 2 R, whereQn is defined by (4.31).\nThe LSPM, as presented in (4.24), is thus a very natural adaptation of Hill\u2019sA(n) to\nthe Least Squares regression. The Dempster-Hill predictive system (4.31) is a conformal\ntransducer under the condition that a point from an interval in (4.31) is chosen random-\nlyfrom the uniform distribution on that interval), resulting in the same guarantees of va-\nlidity as those given above: the distribution of (4.31) is uniform over the interval[0, 1].\nIn terms of e\ufb00iciency, given the most standard case of IID Gaussian observations,\nour predictive distributions for linear regression are as precise as the Dempster-Hill ones\nasymptotically when compared with Oracles I and II.\nLet us consider the Dempster-Hill procedure for the location/scale modelyi = w + \u03bei,\ni = 1 , 2, . . ., where\u03bei \u0018 N(0, \u03c32) are independent. Similar to the case of the LSPM, the\ncomparision if between the Dempster-Hill procedure and the three oracles:\nIn our study of e\ufb00iciency, we consider the three versions of the LSPM and also three\n\u201coracles\u201d. All of the three oracles know the data generation model (4.25). In addition\nOracle III has complete information about both parameters of the data generation model\n(4.25) (bothw and\u03c3), Oracle II knows\u03c3, but does not knoww and finally Oracle I knows\nneither w nor \u03c3. Oracles that don\u2019t know certain parameters have to estimate them from\nthe data. Proper Oracle Idoes not know both parameters, Oracle II knows\u03c3, and Oracle\nIII has complete information as it knows bothw and \u03c3.\nIt is interesting to note that Theorems2 and 3 (and therefore the blue and black plots\nin Figure4.2) are applicable to both the LSPM and Dempster-Hill predictive distributions\n(see, e.g., [107].) The situation with Oracle III is, however, different.\nDonsker\u2019s ([31]) classical result allows to simplify Theorems2 and 3 (QIII denotes\nOracle III\u2019s predictive distribution (independent ofn).",
        "cccba130-7106-49fb-9b0c-3a2cb2dfc893": "4.9. Experimental results 93\nTheorem 4.In the case of the Dempster-Hill procedure, the random functionGn : R ! R defined\nby\nGn(t) := pn\n\u00c4\nQn(w + \u03c3t, \u03c4 ) \u0000 QIII(w + \u03c3t)\n\u00e4\n= pn (Qn(w + \u03c3t, \u03c4 ) \u0000 \u03a6(t)) (4.32)\nweakly converges to a Brownian bridge, i.e., a Gaussian processZ with mean zero and covariance\nfunction\ncov(Z(s), Z(t)) = \u03a6( s) (1 \u0000 \u03a6(t)) , s \u0014 t.\nThe variance\u03a6(t)(1 \u0000 \u03a6(t)) of the Brownian bridge is shown as the red line in Fig-\nure 4.2. However, under assumptions of this section including assumption of fixed ob-\njects, the analogue of the process (4.32) does not converge in general for the LSPM.\n4.9 Experimental results\nIn this section we explore experimentally the validity and e\ufb00iciency of the studentized\nLSPM.\nOnline validity\nFirst, we verify experimental validity of our methods in the online mode of prediction.\nSuch validity is guaranteed at the theoretical level, checking validity via experiments is\na useful opportunity to test the correctness of our code implementation. We provide full\ndetails in order to enable reproducibility of the results.\nWe generate IID observationszn = ( xn, yn), n = 1 , . . . , 1001, the corresponding p-\nvalues pn := Qn(yn+1, \u03c4n), n = 1 , . . . , N , N := 1000 , in the online mode. In our experi-\nments, we generate objectsxn from standard normal distributionxn \u0018 N(0, 1) and labels\nyn from linear model under Gaussian assumption asyn \u0018 2xn + N(0, 1). As usual, we\nconsider \u03c4n to be uniformly distributed\u03c4n \u0018 U and all independent.\nFigure 4.3 plots cumulative p-valuesSn := Pn\ni=1 pi vs n = 1 , . . . , N . As expected,\nit is an approximately straight line with slope0.5. Figure 4.4 additionally presents three\nplots: the cumulative sums S\u03b1\nn := Pn\ni=1 1{pi\u2264\u03b1}, where 1 is the indicator function, vs",
        "2e55b83b-f0a5-4a20-bb6e-90a413068035": "94 Chapter 4. Probabilistic regression\n0 200 400 600 800 1000\n0 100 200 300 400 500\nn\ncumulative sum\nFIGURE 4.3: The cumulative sumsSn of the p-values vsn = 1 , . . . , 1000\nn = 1 , . . . , N , for three values of\u03b1, \u03b1 2 f 0.25, 0.5, 0.75g. For each of the three\u03b1s the\nresult is an approximately straight line with slope\u03b1. Finally, Figure4.5 plots A\u03b1\nN against\n\u03b1 2 [0, 1], whereA\u03b1\nN := 1\nN\nPN\ni=1 1{pi\u2264\u03b1}. The result is, approximately, the main diagonal\nof the square[0, 1]2, as it should be.\nFurther, we explore empirically the e\ufb00iciency of the studentized LSPM. Figure4.1\ncompares the conformal predictive distribution with the true (Oracle III\u2019s) distribution\nfor four randomly generated test objects and a randomly generated training sequence of\nlength 10 with 2 attributes. The first attribute is a dummy all-1 attribute (Theorems2\nand 3 depend on the assumption that one of the attributes is an identical 1 as without it,\nthe plots become qualitatively different: cf. [19, Corollary 2.4.1]).\nThe second attribute is generated from the standard Gaussian distribution, and the\nlabels are generated asyn \u0018 2xn,2 + N(0, 1), xn,2 being the second attribute. Additionally\nWe show (with thinner lines) the output of Oracle I and Oracle II (only for the simplified\nversions, in order not to clutter the plots). Instead, in the left-hand plot of Figure4.6 we\nshow the first plot of Figure4.1 that is normalized by subtracting the true distribution\nfunction; this time, we show the output of both simplified and proper Oracles I and II; the\ndifference is not large but noticeable. The right-hand plot of Figure4.6 is similar except\nthat the training sequence is of length100 and there are 20 attributes generated inde-\npendently from the standard Gaussian distribution except for the first one, which is the",
        "4427f16c-0661-4f79-a4ce-8e041e882fad": "4.10. Conclusion 95\n0 200 400 600 800 1000\n0 200 400 600 800\nn\ncumulative sums\n0.75\n0.5\n0.25\nFIGURE 4.4: The cumulative sums S\u03b1\nn vs n = 1 , . . . , 1000 for \u03b1 2\nf0.25, 0.5.0.75g\ndummy all-1 attribute; the labels are generated as before,yn \u0018 2xn,2 + N(0, 1).\nSince Oracle III is more powerful than Oracles I and II as it knows the true data-\ngenerating distribution) it is more di\ufb00icult to compete with. The black line is farther from\nthe shaded area than the blue and red lines for all four plots in Figure4.1. The estimated\ndistribution functions being to the left of the true distribution functions is a coincidence:\nthe four plots correspond to the values0\u20133 of the seed for the R pseudorandom number\ngenerator, and for other seeds the estimated distribution functions are sometimes to the\nright and sometimes to the left.\n4.10 Conclusion\nThe main contribution of this chapter is the introduction and development of non-\nparametric approach to predictive distribution functions using conformal prediction. In\nstatistics, the theory of predictive distributions (see, e.g., [117], [121]) is based on the as-\nsumption that samples are generated from a parametric model. Our novel contribution\n(publishedpaper[ 146])extendsstatistical\u201cpredictionconfidencedistributions\u201dinterms\nof ability to derive predictive distribution functions for machine learning regression tasks",
        "7efe26af-a562-46c2-9d49-42dc53a15026": "96 Chapter 4. Probabilistic regression\n0.0 0.2 0.4 0.6 0.8 1.0\n0.0 0.2 0.4 0.6 0.8 1.0\n\u03b1\nempirical frequency\nFIGURE 4.5: The calibration curve:A\u03b1\nN vs \u03b1 2 [0, 1] for N = 1000\nusing only limited assumptions customary of machine learning, namely that the observa-\ntions are generated using the IID model. Unlike in the parametric approach of statistical\npredictive distributions, our novel approach is non-parametric approach that does not\nrequire for the data model to be specified.\nThis approach results in predictive distribution functions that have requisite property\nofvalidityforIIDobservationsintermsofguaranteedcoverage. Theadvantageofpredic-\ntive distribution functions over the usual conformal prediction intervals is that conformal\npredictive distributions contains more information \u2014 a conformal predictive distribution\nQn can produce a plethora of prediction intervals corresponding to each confidence level\n1 \u0000 \u03f5.",
        "b60204b7-5a6c-4b26-a5d7-7f09f81c6fa8": "4.10. Conclusion 97\n\u22124 \u22122 0 2 4\n\u22120.2 \u22120.1 0.0 0.1 0.2 0.3\ny\nQ (y)\u2212 Oracle III\noracle I (simplified)\noracle I (proper)\noracle II (simplified)\noracle II (proper)\n\u22124 \u22122 0 2 4\n\u22120.1 0.0 0.1 0.2 0.3\ny\nQ (y)\u2212 Oracle III\noracle I (simplified)\noracle I (proper)\noracle II (simplified)\noracle II (proper)\nFIGURE 4.6: The left-hand plot is the first (upper left) plot of Figure4.1 nor-\nmalized by subtracting the true distribution function (the thick black line\nin Figure4.1, which now coincides with thex-axis) and with the outputs\nof the proper oracles added; the right-hand plot is an analogous plot for\na larger training sequence (of length 100 with 20 attributes, the first one\nbeing the dummy attribute)",
        "299c64b5-a7cb-48de-ac16-8b84625fba49": "98",
        "3af0a95a-1f8e-4dec-b3ce-d2464d541f38": "99\nChapter 5\nKernel probabilistic regression\nThis chapter extends the study of probabilistic regression and combines conformal predictive dis-\ntributions (CPDs) with kernel methods to derive kernelized versions of the algorithms described\nin Chapter4. Kernelized versions of conformal predicitive distributions are studied theoretically to\ndetermine their computational e\ufb00iciency. Experimental study of the predictive e\ufb00iciency demon-\nstrates important advantages of the kernelized versions of CPDs and shows that universal (Lapla-\ncian) kernel works remarkably well in terms of outputting accurate probabilistic predictions for the\ntest objects.\n5.1 Introduction\nInthischapter, wecontinuetostudyprobabilisticregressionproblemthatbeganinChap-\nter 4 . As in Chapter4, we require predictions to satisfy a reasonable property of validity\n(under natural assumptions standard for general machine learning problems). The natu-\nral probabilistic prediction for the labely is that of a complete probability measure onR,\nsuch probability measure can be represented by itsdistribution function(see, e.g., [27, 28,\n40]). Similar to definitions used in Chapter4, this will be referred to aspredictive distribu-\ntion (PD). The necessary property of predictive distributions (same as for any probabilis-\nticpredictors)isthattheyarewell-calibrated. WefollowGneiting et al.[40,Section1.2]to\nformally define calibration as the\u201cstatistical compatibility between the probabilistic fore-\ncasts and the realizations.\u201d The less formal definition of validity is that predictive dis-\ntributions should \u201ctell the truth\u201d (see Section1.2.1 for more details on terminology in\nprobabilistic prediction).",
        "48d9b068-53c4-4ea4-b818-61fe1f00736b": "100 Chapter 5. Kernel probabilistic regression\nIt can happen that the \u201ctruth\u201d can be non-informative (as it should be in highly un-\ncertain situations where predictive distribution should be flat rather than have the peak\nin the wrong place, as would happen with Bayesian prediction with an incorrect prior\n(see, e.g., [88]). The further requirement is that of e\ufb00iciency, often referred to as sharp-\nness [40, Section 2.3]. Our objective is to optimize the e\ufb00iciency subject to the necessary\nassumption of validity [40, Section 1.2].\nThis chapter provides a very selective review of predictive distributions with validity\nguarantees. AfteroutliningthepredictionprobleminSection 5.2,inSection 5.3weturnto\ntheoldestapproachtopredictivedistributions\u2014namelytheBayesianapproach. Undera\nveryrestrictiveassumptionoffullknowledgeofthestochasticdatageneratingmechanism\nsuch an approach gives rise to a perfect solution. Section5.4 proceeds to Fisher\u2019s fiducial\npredictive distributions.\nThe first recent development, as described in Chapter4, was to carry over predic-\ntive distributions to the framework of statistical machine learning as developed by two\ngroups at the Institute of Automation and Remote Control (Aizerman\u2019s laboratory in-\ncluding Braverman and Rozonoer and Lerner\u2019s laboratory including Vapnik and Cher-\nvonenkis). For a brief history of the Institute and research on statistical learning there,\nincluding the role of Emmanuel Markovich Braverman see [21], especially Chapter 5.\nThe first development, described in Chapter4, consisted in adapting predictive distri-\nbutions to the IID model for the simplest linear case (see [145] and Chapter4) for more\ndetails). The second development (published in [139]), is the combination of conformal\npredictive distributions with kernel methods that were invented by the members of Aiz-\nerman\u2019s laboratory, first of all Braverman and Rozonoer [21, p. 48]. This development is\noutlined in Section5.6 where kernelized versions of the main algorithms of Chapter5 are\ndescribed. The experimental section of this chapter (Section5.8) demonstrates important\nadvantages of kernelized versions. The computational e\ufb00iciency of kernelized predic-\ntive distributions is studied theoretically in Section5.6and predictive e\ufb00iciency is studied\nempirically in Section5.8. As demonstrated in Section5.8, a universal (Laplacian) kernel\nworks remarkably well.",
        "99774231-5c6b-44c2-b572-77d4217a8243": "5.2. The problem 101\n5.2 The problem\nProbabilisticregressionrequirespredictingthelabel yn+1 (weuse y andyn+1 interchange-\nably to describe the label of the(n + 1) st observation) based on the training sequence of\nobservations zi = ( xi, yi), i = 1 , . . . , n and a new test objectxn+1 2 Rp. This rather\ngeneral model specification can cover both regression problems with objectsx contain-\ning explanatory variables for labelsy from the setY, as well as time-series problems with\nobjects x containing previous history and other time-series based features of the objectz.\nEach observationzi = ( xi, yi), i = 1 , . . . , n + 1, consists of two components, the objectxi\nfrom a measurable spaceX that we call theobject spaceand the labelyi that belongs to a\nmeasurable spaceY that we call thelabel space. In the probabilistic regression setting the\nobject space is the real line,Y = R. In the probabilistic forecasting setting, our prediction\nis the probability measure on the label spaceY = R, this measure will be represented by\nits cumulative distribution function (CDF).\nThe various approaches to the probabilistic prediction problem include that of\nBayesian methods (see Section5.3 below), statistical parametric approach (discussed in\nSection 5.4), and more recently nonparametric approach based on standard assumptions\nin machine learning (namely the IID model) that is discussed in this chapter. Using the\nterminology in [138], in conformal prediction it is convenient to differentiate between the\ntwo kinds of assumptions: hard and soft. The hard assumption is the standard assump-\ntion of machine learning \u2014 namely that the observations are generated independently\nfrom the same probability distribution (the IID assumption). The validity of our proba-\nbilistic forecasts will depend only on the model with hard assumption. The soft assump-\ntion is the assumption that the labely of an objectx depends onx in an approximately\nlinear manner \u2014 this soft assumption will be used to optimize e\ufb00iciency.\nSimilartostandardmachinelearningnotation,wewilluseafixedparameter a > 0 that\ndetermines the degree of regularization applied to the solution, regularization becomes\nindispensable when kernel methods are used.",
        "6b64d7a4-695f-4961-bb46-27dcc3f33c31": "102 Chapter 5. Kernel probabilistic regression\n5.3 Bayesian solution\nBayesian approach provides a very satisfactory solution to our probabilistic prediction\nproblem. This approach has dominated statistical inference for more than 150 years (cov-\neringperiodoftimeroughlyfrom1770to1930)\u2014ithasstartedwiththeworksofThomas\nBayes and Pierre-Simon Laplace and was continued by Karl Pearson. Bayesian approach,\nhowever, has limitations as it is based on very strong and restrictive assumptions.\nWe assume linear statistical model in a feature space and that the noise follows Gaus-\nsian distribution. Our model is linear in terms of feature mapping of a deterministic se-\nquence of objectsx1, . . . , xn+1:\nyi = w \u0001 F(xi) + \u03bei, i = 1 , . . . , n + 1, (5.1)\nwhere F : X ! H is a mapping from the object space to a Hilbert spaceH, \u201c\u0001\u201d is the\ndot product in Hilbert spaceH and w is a random vector distributed asN(0, (\u03c32/a)I) (I\nbeingtheidentityoperatoron H),and \u03bei arerandomvariablesdistributedas N(0, \u03c32) and\nindependentof w andbetweenthemselves. Wehaveintroduced a attheendofSection 5.2\nas the regularization constant, whilst\u03c3 > 0 is another parameter, the standard deviation\nof the noise variables\u03bei.\nBy applying expectation operatorE to the model (5.1), it is easy to check that\nE yi = 0 , i = 1 , . . . , n,\ncov(yi, yj) = \u03c32\na K(xi, xj) + \u03c321{i=j}, i, j = 1 , . . . , n,\n(5.2)\nwhereK(x, x\u2032) := F(x)\u0001F(x\u2032). By the theorem on normal correlation (see, e.g., [122, The-\norem II.13.2]), the Bayesian predictive distribution foryn+1 given xn+1 and the training\nsequence is\nN\n\u00c5\nk\u2032(K + aI)\u22121Y, \u03c32\na \u03ba + \u03c32 \u0000 \u03c32\na k\u2032(K + aI)\u22121k\n\u00e3\n, (5.3)\nwhere k is the n-vector ki := K(xi, xn+1), i = 1 , . . . , n; K is the kernel matrix for the\nfirst n observations (the training observations only);Ki,j := K(xi, xj), i, j = 1 , . . . , n;\nI = In is then \u0002 n unit matrix;Y := ( y1, . . . , yn)\u2032 is the vector of then training labels; and",
        "7cec150e-d061-4ff3-b0b4-a81d4e49a8be": "5.4. Fiducial predictive distributions 103\n\u03ba := K(xn+1, xn+1).\nTheweaknessofthemodel( 5.1)(used,e.g.,in[ 137,Section10.3])isthattheGaussian\nmeasure N(0, (\u03c32/a)I) exists only whenH is finite-dimensional, but we can circumvent\nthisdi\ufb00icultybyusing( 5.2)directlyasourBayesianmodel,foragivensymmetricpositive\nsemidefinite K. The mappingF in not part of the picture any longer. This is the standard\napproach in Gaussian process regression in machine learning.\nIn the Bayesian solution, there is no difference between the hard and soft model; in\nparticular, (5.2) is required for the validity of the predictive distribution (5.3).\n5.4 Fiducial predictive distributions\nAfter its sesquicentennial rule, Bayesian statistics was challenged by Fisher and Neyman,\nwho had little sympathy with each other\u2019s views apart from their common disdain for\nBayesianmethods[ 139]. Fisher\u2019sapproachwasmoreambitious,andhisgoalwastocom-\npute a full probability distribution for a future value (test label in our context) or for the\nvalue of a parameter. Neyman and his followers were content with computing intervals\nfor future values (prediction intervals) and values of a parameter (confidence intervals).\nFisher and Neyman relaxed the assumptions of Bayesian statistics by allowing uncer-\ntainty, in Knight\u2019s [61] terminology. In Bayesian statistics overall probability measure is\nknown, i.e., there is a situation of risk without any uncertainty. Fisher and Neyman used\nthe framework of parametric statistics, in this framework the value of the parameter is a\nnumber or an element of a Euclidean space (there is no stochastic model for the value of\ntheparameter. Inthenextsection 5.5weallowforevengreateramountofuncertainty: our\nstatistical model will be the nonparametric IID model (as standard in machine learning).\nThe available properties of validity naturally become weaker as we relax our assump-\ntions. For predicting future values, conformal prediction ensures calibration in probabil-\nity, in the terminology of [40, Definition 1]. As to Bayesian prediction, it can be shown\nthat it satisfies a stronger conditional version of this property: Bayesian predictive distri-\nbutionsarecalibratedinprobabilityconditionallyonthetrainingsequenceandtestobject\n(more generally, on the past).",
        "1a6c05e6-10de-406e-ad43-dafceeaf4ece": "104 Chapter 5. Kernel probabilistic regression\nThepropertyofbeingcalibratedinprobabilityforconformalpredictionis,ontheother\nhand, unconditional; or, in other words, it is conditional on the trivial\u03c3-algebra. Fisher\u2019s\nfiducial predictive distributions satisfy an intermediate property of validity: they are cal-\nibrated in probability conditionally on what was called the\u03c3-algebra of invariant events\nin [87], which is greater than the trivial\u03c3-algebra but smaller than the\u03c3-algebra repre-\nsenting the full knowledge of the past.\nFisher did not formalize his fiducial inference (this is often regarded as his \u201cbiggest\nblunder\u201d [35]). By replacing probability distributions by intervals, Neyman\u2019s simplifica-\ntion allowed for him to state suitable notions of validity more easily, as a result of which\nhis approach to statistics became mainstream until the Bayesian approach started to re-\nassert itself towards the end of the 20th century [139]. However, there has been a recent\nrevivalofinterestinfiducialinference: cf.theBFF(Bayesian, frequentist, andfiducial)se-\nries of conferences that began in 2014. Fiducial inference is a key topic of the series, both\nin the form of confidence distributions (the term introduced by David Cox [24] for distri-\nbutionsforparameters)andpredictivedistributions(whichbydefinition[ 121,Definition\n1] must be calibrated in probability).\nSince fiducial inference was developed in the framework of parametric statistics, it\nhas two versions, one targeting computing confidence distributions and the other predic-\ntive distributions. Under nonparametric assumptions, such as our IID model, we are not\ninterested in confidence distributions as the set of all probability measures on the obser-\nvation spaceX \u0002 R is just too big), we therefore concentrate on predictive distributions.\nThe standard notion of validity for predictive distributions, introduced independently\nby Schweder and Hjort [117, Chapter 12] and Shen, Liu, and Xie [121], is calibration in\nprobability going back to Philip Dawid\u2019s work (see, e.g., [27, Section 5.3] and [28]).\n5.5 Conformal predictive distributions\nValidpredictivedistributionsundertheIIDmodelcanbeobtainedbyslightlyrelaxingthe\nnotion of a predictive distribution as given in [121]. We follow [145] and [136] to define\npredictive distributions (see4.4for definition of conformal predictive distribution (CPD)\nand related definitions).",
        "bc672b0a-d96e-432a-a2d1-e3bb0548c8c9": "5.6. Kernel Ridge Regression Prediction Machine 105\n5.6 Kernel Ridge Regression Prediction Machine\nIn this section the Kernel Ridge Regression Prediction Machine (KRRPM) is introduced;\nitwillbetheconformaltransducerdeterminedbyaconformitymeasureoftheform 4.6.1,\nwhere \u02c6yn+1 is computed using kernel ridge regression.\nWe use three natural versions of the definition, all three versions are based on (4.6) as\nsoft model (with the IID model being the hard model).\nGiven a training sequence(z1, . . . , zn) 2 Zn and a test objectxn+1 2 X, thekernel ridge\nregression predicts\n\u02c6yn+1 := k\u2032(K + aI)\u22121Y\nfor the labelyn+1 ofxn+1. This is just the mean in (5.3), and the variance is ignored. Plug-\nging this definition into4.6, we obtain thedeleted KRRPM. Alternatively, we can replace\nthe conformity measure (4.6) by\nA(z1, . . . , zn+1) := yn+1 \u0000 \u00cayn+1, (5.4)\nwhere\n\u00cayn+1 := \u00afk\u2032( \u00afK + aI)\u22121 \u00afY (5.5)\nis the prediction for the labelyn+1 of xn+1 computed usingz1, . . . , zn+1 as the training\nsequence. The notation used in (5.5) is:\n\u2022 \u00afk is the(n + 1)-vectorki := K(xi, xn+1), i = 1 , . . . , n + 1\n\u2022 \u00afK is the kernel matrix for alln + 1 observations\n\u2022 \u00afKi,j := K(xi, xj), i, j = 1 , . . . , n + 1\n\u2022 I = In+1 is the(n + 1) \u0002 (n + 1) unit matrix\n\u2022 \u00afY := ( y1, . . . , yn+1)\u2032 is the vector of alln + 1 labels",
        "244cb2f9-9243-4f06-a342-fed86f817154": "106 Chapter 5. Kernel probabilistic regression\nIn this context, K is any givenkernel, i.e., symmetric positive semidefinite function\nK : X2 ! R. The corresponding conformal transducer is theordinary KRRPM. The dis-\nadvantage of the deleted and ordinary KRRPM is that they are not RPSs (they can fail to\nproduce a function increasing iny in the presence of extremely high-leverage objects).\nLet us set\n\u00afH := ( \u00afK + aI)\u22121 \u00afK = \u00afK( \u00afK + aI)\u22121. (5.6)\nThis hat matrix\u201cputs hats on theys\u201d: according to (5.5), \u00afH \u00afY is the vector(\u00cay1, . . . , \u00cayn+1)\u2032,\nwhere\u00cayi,i = 1 , . . . , n +1,isthepredictionforthelabel yi ofxi computedusing z1, . . . , zn+1\nas the training sequence. We will refer to the entries of the matrix\u00afH as \u00afhi,j (wherei is the\nrow andj is the column of the entry), abbreviating\u00afhi,i to \u00afhi. The usual relation between\nthe residuals in (4.6) and (5.4) is\nyn+1 \u0000 \u02c6yn+1 = yn+1 \u0000 \u00cayn+1\n1 \u0000 \u00afhn+1\n.\nThe diagonal elements\u00afhi of the hat matrix are always in the semi-open interval[0, 1) and\nso the numerator is non-zero). Similar to Chapter4, we will be usingstudentized residuals\n(yn+1 \u0000 \u00cayn+1)(1 \u0000 \u00afhn+1)\u22121/2, which are half-way between the deleted residuals in4.6.2\nand the ordinary residuals in4.6.1.\nTheconformaltransducerwiththecorrespondingconformitymeasureisthe(studen-\ntized) KRRPM [139].\nA(z1, . . . , zn+1) := yn+1 \u0000 \u00cayn+1p\n1 \u0000 \u00afhn+1\n(5.7)\nLater in this section we will see that the KRRPM is an RPS. This version of the confor-\nmaltransducerwillbetheprimaryversionconsideredinthischapter, with\u201cstudentized\u201d\nusually omitted.\nAn explicit form of the KRRPM\nAccordingto( 4.8),inordertocomputethestudentizedversionofpredictivedistributions\nproduced by the KRRPM, the following equation need to be solved:\n\u03b1y\ni = \u03b1y\nn+1",
        "96c96216-d619-4b27-82f7-150116ff4ca3": "5.6. Kernel Ridge Regression Prediction Machine 107\nAlgorithm 3Kernel Ridge Regression Prediction Machine\nRequire: A training sequence(xi, yi) 2 X \u0002 R, i = 1 , . . . , n.\nRequire: A test objectxn+1 2 X.\n1: Define the hat matrix\u00afH by (5.6), \u00afK being the(n + 1) \u0002 (n + 1) kernel matrix.\n2: for i 2 f1, 2, . . . , ng do\n3: Define Ai and Bi by (5.8) and (5.9), respectively.\n4: Set Ci := Ai/Bi.\n5: end for\n6: Sort C1, . . . , Cn in the increasing order obtainingC(1) \u0014 \u0001 \u0001 \u0001 \u0014 C(n).\n7: Return the following predictive distribution foryn+1:\nQn(y, \u03c4 ) :=\n\u00aei+\u03c4\nn+1 if y 2 (C(i), C(i+1)) for i 2 f0, 1, . . . , ng\ni\u2032\u22121+\u03c4(i\u2032\u2032\u2212i\u2032+2)\nn+1 if y = C(i) for i 2 f1, . . . , ng. (5.10)\n(together with the corresponding inequality\u03b1y\ni < \u03b1y\nn+1) for i = 1 , . . . , n + 1.\nCombining the definition (4.7) of the conformity scores\u03b1y\ni with the definition (5.7) of\nthestudentizedversionoftheconformitymeasure,andthefactthatthepredictions \u00cayi can\nbe obtained from\u00afY by applying the hat matrix\u00afH (cf. (5.6)), we can rewrite\u03b1y\ni = \u03b1y\nn+1 as\nyi \u0000 Pn\nj=1 \u00afhijyj \u0000 \u00afhi,n+1yp\n1 \u0000 \u00afhi\n=\ny \u0000 Pn\nj=1 \u00afhn+1,jyj \u0000 \u00afhn+1yp\n1 \u0000 \u00afhn+1\n.\nThis is a linear equation,Ai = Biy, and by solving it we obtainy = Ci := Ai/Bi, where\nAi :=\nPn\nj=1 \u00afhn+1,jyj\np\n1 \u0000 \u00afhn+1\n+\nyi \u0000 Pn\nj=1 \u00afhijyj\np\n1 \u0000 \u00afhi\n, (5.8)\nBi :=\n\u00bb\n1 \u0000 \u00afhn+1 +\n\u00afhi,n+1p\n1 \u0000 \u00afhi\n. (5.9)\nThe following Algorithm3 for computing the conformal predictive distribution (4.3), al-\nlows us to compute (4.8) easily.\nThenotation i\u2032 andi\u2032\u2032 usedinline 7oftheAlgorithm 3isdefinedas i\u2032 := min fj j C(j) =\nC(i)g and i\u2032\u2032 := max fj j C(j) = C(i)g, to ensure thatQn(y, 0) = Qn(y\u0000, 0) and Qn(y, 1) =\nQn(y+, 1) at y = C(i); C(0) and C(n+1) are understood to be\u00001 and 1, respectively.\nAlgorithm 3 is not computationally e\ufb00icient for a large test set, since the hat matrix\n\u00afH (cf. (5.6)) needs to be computed from scratch for each test object. To obtain a more\ne\ufb00icient version, we use a standard formula for inverting partitioned matrices (see, e.g.,",
        "94a24b34-78b5-43a4-88ff-034017dbfda5": "108 Chapter 5. Kernel probabilistic regression\n[46, (8)] or [137, (2.44)]) to obtain\n\u00afH = ( \u00afK + aI)\u22121 \u00afK =\n\u00d6\nK + aI k\nk\u2032 \u03ba + a\n\u00e8\u22121 \u00d6\nK k\nk\u2032 \u03ba\n\u00e8\n=\n\u00d6\n(K + aI)\u22121 + d(K + aI)\u22121kk\u2032(K + aI)\u22121 \u0000d(K + aI)\u22121k\n\u0000dk\u2032(K + aI)\u22121 d\n\u00e8\u00d6\nK k\nk\u2032 \u03ba\n\u00e8\n=\n\u00d6\nH + d(K + aI)\u22121kk\u2032H \u0000 d(K + aI)\u22121kk\u2032\n\u0000dk\u2032H + dk\u2032\n(5.11)\n(K + aI)\u22121k + d(K + aI)\u22121kk\u2032(K + aI)\u22121k \u0000 d\u03ba(K + aI)\u22121k\n\u0000dk\u2032(K + aI)\u22121k + d\u03ba\n\u00e8\n(5.12)\n=\n\u00d6\nH + d(K + aI)\u22121kk\u2032(H \u0000 I) d(I \u0000 H)k\ndk\u2032(I \u0000 H) \u0000dk\u2032(K + aI)\u22121k + d\u03ba\n\u00e8\n(5.13)\n=\n\u00d6\nH \u0000 ad(K + aI)\u22121kk\u2032(K + aI)\u22121 ad(K + aI)\u22121k\nadk\u2032(K + aI)\u22121 d\u03ba \u0000 dk\u2032(K + aI)\u22121k\n\u00e8\n, (5.14)\nwhere\nd := 1\n\u03ba + a \u0000 k\u2032(K + aI)\u22121k (5.15)\n(the denominator is positive by the theorem on normal correlation, already used in Sec-\ntion 5.3), the equality in line (5.13) follows from\u00afH being symmetric (which allows us to\nignore the upper right block of the matrix (5.11)\u2013(5.12)), and the equality in line (5.14)\nfollows from\nI \u0000 H = ( K + aI)\u22121(K + aI) \u0000 (K + aI)\u22121K = a(K + aI)\u22121.\nWe have been using the notationH for the training hat matrix\nH = ( K + aI)\u22121K = K(K + aI)\u22121. (5.16)",
        "0b1abfc3-fe45-4fca-b5fe-5f7962a915f7": "5.6. Kernel Ridge Regression Prediction Machine 109\nNotice that the constantad occurring in several places in (5.14) is between 0 and 1:\nad = a\na + \u03ba \u0000 k\u2032(K + aI)\u22121k 2 (0, 1] (5.17)\n(the fact that\u03ba \u0000k\u2032(K + aI)\u22121k is nonnegative follows from the lower right entry\u00afhn+1 of\nthe hat matrix (5.14) being nonnegative.\nThe important components in the expressions forAi and Bi (cf. (5.8) and (5.9)) are,\naccording to (5.14),\n1 \u0000 \u00afhn+1 = 1 + dk\u2032(K + aI)\u22121k \u0000 d\u03ba = 1 + k\u2032(K + aI)\u22121k \u0000 \u03ba\n\u03ba + a \u0000 k\u2032(K + aI)\u22121k\n= a\n\u03ba + a \u0000 k\u2032(K + aI)\u22121k = ad, (5.18)\n1 \u0000 \u00afhi = 1 \u0000 hi + ade\u2032\ni(K + aI)\u22121kk\u2032(K + aI)ei\n= 1 \u0000 hi + ad(e\u2032\ni(K + aI)\u22121k)2, (5.19)\nwhere hi = hi,i is theith diagonal entry of the hat matrix (5.16) for then training objects\nandei isthe ithvectorinthestandardbasisof Rn (sothatthe jthcomponentof ei is1{i=j}\nfor j = 1 , . . . , n). Let \u02c6yi := e\u2032\niHY be the prediction foryi computed from the training\nsequence z1, . . . , zn and the test objectxi. Using (5.18) (but not using (5.19) for now), we\ncan transform (5.8) and (5.9) as\nAi :=\nPn\nj=1 \u00afhn+1,jyj\np\n1 \u0000 \u00afhn+1\n+\nyi \u0000 Pn\nj=1 \u00afhijyj\np\n1 \u0000 \u00afhi\n= ( ad)\u22121/2\nnX\nj=1\nadyjk\u2032(K + aI)\u22121ej\n+\nyi \u0000 Pn\nj=1 hijyj + Pn\nj=1 adyje\u2032\ni(K + aI)\u22121kk\u2032(K + aI)\u22121ej\np\n1 \u0000 \u00afhi\n= ( ad)1/2k\u2032(K + aI)\u22121Y + yi \u0000 \u02c6yi + ade\u2032\ni(K + aI)\u22121kk\u2032(K + aI)\u22121Yp\n1 \u0000 \u00afhi\n,\n=\np\nad\u02c6yn+1 + yi \u0000 \u02c6yi + ad\u02c6yn+1e\u2032\ni(K + aI)\u22121kp\n1 \u0000 \u00afhi\n, (5.20)",
        "62c64e7b-c7e8-4f4a-9280-41db6bc0d36e": "110 Chapter 5. Kernel probabilistic regression\nwhere \u02c6yn+1 is the Bayesian prediction foryn+1 (cf. the expected value in (5.3)), and\nBi :=\n\u00bb\n1 \u0000 \u00afhn+1 +\n\u00afhi,n+1p\n1 \u0000 \u00afhi\n=\np\nad + adk\u2032(K + aI)\u22121eip\n1 \u0000 \u00afhi\n. (5.21)\nTherefore, we can implement Algorithm3 as follows. Preprocessing the training se-\nquence takes timeO(n3) (or faster if using, say, the Coppersmith\u2013Winograd algorithm\nand its versions; we assume that the kernelK can be computed in timeO(1)):\n1. The n \u0002 n kernel matrixK can be computed in timeO(n2).\n2. The matrix(K + aI)\u22121 can be computed in timeO(n3).\n3. The diagonal of the training hat matrixH := ( K + aI)\u22121K can be computed in time\nO(n2).\n4. All \u02c6yi, i = 1 , . . . , n, can be computed by\u02c6y := HY = ( K + aI)\u22121(KY ) in timeO(n2)\n(even without knowingH).\nProcessing each test objectxn+1 takes timeO(n2):\n1. Vectork and number\u03ba (as defined after (5.3)) can be computed in timeO(n) and\nO(1), respectively.\n2. Vector(K + aI)\u22121k can be computed in timeO(n2).\n3. Numberk\u2032(K + aI)\u22121k can now be computed in timeO(n).\n4. Numberd defined by (5.15) can be computed in timeO(1).\n5. For alli = 1 , . . . , n, compute1 \u0000 \u00afhi as (5.19), in timeO(n) overall (given the vector\ncomputed in2).\n6. Compute the number\u02c6yn+1 := k\u2032(K + aI)\u22121Y in timeO(n) (given the vector com-\nputed in2).\n7. Finally, computeAi and Bi for alli = 1 , . . . , n as per (5.20) and (5.21), setCi :=\nAi/Bi, and output the predictive distribution (5.10). This takes timeO(n) except\nfor sorting theCi, which takes timeO(n log n).",
        "651232e1-7d1f-4b3f-b804-2ed00f120e98": "5.7. Limitation of the KRRPM 111\n5.7 Limitation of the KRRPM\nThe KRRPM makes a significant step forward as compared to the LSPM described in\nChapter4: oursoftmodel( 5.1)isnolongerlinearin xi. Infact,byusingauniversalkernel\n(such as Laplacian kernel in Section5.8) allows the functionx 2 X 7! w \u0001F(x) to approx-\nimate any continuous function arbitrarily well within any compact set inX. However,\nsince we are interested in predictive distributions rather than point predictions, using the\nsoft model (5.1) still results in the KRRPM being restricted. In this section we discuss the\nnature of the restriction, using the ordinary KRRPM as a technical tool.\nThe Bayesian predictive distribution (5.3) is Gaussian and (as clear from (5.1) and\nfromthebottomrightentryof( 5.14)beingnonnegative)itsvarianceisatleast \u03c32. Wewill\nsee that the situation with the conformal distribution is not as bad, despite the remaining\nrestriction. To understand the nature of the restriction it will be convenient to ignore the\ndenominator in (5.7), i.e., to consider the ordinary KRRPM; the difference between the\n(studentized)KRRPMandordinaryKRRPMwillbesmallintheabsenceofhigh-leverage\nobjects (an example will be given in the next section). For the ordinary KRRPM we have,\nin place of (5.8) and (5.9),\nAi :=\nnX\nj=1\n\u00afhn+1,jyj + yi \u0000\nnX\nj=1\n\u00afhi,jyj,\nBi := 1 \u0000 \u00afhn+1 + \u00afhi,n+1.\nTherefore, (5.20) and (5.21) become\nAi = ad\u02c6yn+1 + yi \u0000 \u02c6yi + ad\u02c6yn+1e\u2032\ni(K + aI)\u22121k\nand\nBi = ad + ade\u2032\ni(K + aI)\u22121k,",
        "4bc7a031-4f5e-4f3a-b640-1b420d66d5a7": "112 Chapter 5. Kernel probabilistic regression\nrespectively. ForCi := Ai/Bi we now obtain\nCi = \u02c6yn+1 + yi \u0000 \u02c6yi\nad + ade\u2032\ni(K + aI)\u22121k\n= \u02c6yn+1 +\n\u03c32\nBayes/\u03c32\n1 + e\u2032\ni(K + aI)\u22121k(yi \u0000 \u02c6yi), (5.22)\nwhere \u02c6yn+1 is, as before, the Bayesian prediction foryn+1, and\u03c32\nBayes is the variance of the\nBayesian predictive distribution (5.3) (cf. (5.17)).\nThe second addende\u2032\ni(K + aI)\u22121k in the denominator of (5.22) is the prediction for\nthe label of the test objectxn+1 in the situation where all training labels are0 apart from\nthe ith, which is1. For a long training sequence we can expect it to be close to 0 (unless\nxi or xn+1 are highly influential); therefore, we can expect the shape of the predictive\ndistribution output by the ordinary KRRPM to be similar to the shape of the empirical\ndistribution function of the residualsyi \u0000 \u02c6yi. In particular, this shape does not depend\n(or depends weakly) on the test objectxn+1. This lack of sensitivity of the predictive dis-\ntribution on the test object prevents the conformal predictive distributions output by the\nKRRPM from being universally consistent in the sense of [136]. The predictive distribu-\ntion is not necessarily Gaussian (as in (5.3)), and the distribution is fitted to all training\nresiduals (and not just the residuals for objects similar to the test object). One possible\nway to get universally consistent conformal predictive distributions would be to replace\nthe right-hand side of (4.10) by \u02c6Fn+1(yn+1), where \u02c6Fn+1 is the Bayesian predictive dis-\ntribution foryn+1 computed fromxn+1 and z1, . . . , zn+1 as training sequence for a su\ufb00i-\nciently flexible Bayesian model (in any case, more flexible than our homoscedastic model\n(5.1)). This idea was referred to as de-Bayesing in [137, Section 4.2] and frequentizing in\n[149, Section 3]. However, modelling input-dependent (heteroscedastic) noise e\ufb00iciently\nis a well-known di\ufb00icult problem in Bayesian regression, including Gaussian process re-\ngression (see, e.g., [42, 74, 123]).",
        "9f21fe6a-4773-4e47-9fa4-a7829f4a4595": "5.8. Experimental results 113\n5.8 Experimental results\nIn the first part of this section we illustrate the main advantage of the KRRPM over the\nLSPM introduced in Chapter4, namely its flexibility \u2014 for a suitable kernel, it gets the\nlocation of the predictive distribution right. The second part of this section illustrates the\nlimitation of the KRRPM discussed in the previous section: while the KRRPM adapts to\nthe shape of the distribution of labels, the adaptation is not conditional on the test object.\nBoth points will be demonstrated using artificial data sets.\nIn the experiment we generate a training sequence of length 1000 from the model\nyi = w1 cos xi,1 + w2 cos xi,2 + w3 sin xi,1 + w4 sin xi,2 + \u03bei, (5.23)\nwhere (w1, w2, w3, w4) \u0018 N(0, I4) (I4 being the unit4 \u0002 4 matrix), (xi,1, xi,2) \u0018 U[\u00001, 1]2\n(U[\u00001, 1] being the uniform probability distribution on[\u00001, 1]), and\u03bei \u0018 N(0, 1), all in-\ndependent. ThiscorrespondstotheBayesianridgeregressionmodelwith a = \u03c3 = 1 . The\ntrue kernel is\nK((x1, x2), (x\u2032\n1, x\u2032\n2))\n= (cos x1, cos x2, sin x1, sin x2) \u0001 (cos x\u2032\n1, cos x\u2032\n2, sin x\u2032\n1, sin x\u2032\n2)\n= cos( x1 \u0000 x\u2032\n1) + cos( x2 \u0000 x\u2032\n2). (5.24)\nBy definition (see, e.g., [125]) a kernel isuniversal if any continuous function can be\nuniformly approximated (over each compact set) by functions in the corresponding re-\nproducing kernel Hilbert space. An example of a universal kernel is theLaplacian kernel\nK(x, x\u2032) := exp \u0000\u0000\n\r\rx \u0000 x\u2032\r\r\u0001.\nLaplacian kernels were introduced and studied in [128]; the corresponding reproducing\nkernel Hilbert space has the Sobolev norm\nkuk2 = 2\nZ \u221e\n\u2212\u221e\nu(t)2dt + 2\nZ \u221e\n\u2212\u221e\nu\u2032(t)2dt",
        "cdcc76f5-65f6-49f0-a91e-a2eaa2fb1815": "114 Chapter 5. Kernel probabilistic regression\nFIGURE 5.1: The predictive distribution for the label of the test object(1, 1)\nbased on a training sequence of length 1000 (all generated from the model\n(5.23)). The red line in each panel is the Bayesian predictive distribution\nbased on the true kernel (5.24), and the blue line is the conformal predic-\ntive distribution based on: the true kernel (5.24) in the left-most panel; the\nLaplacian kernel in the middle panel; the linear kernel in the right-most\npanel.\nFIGURE5.2: TheanalogueofFigure 5.1foratrainingsequenceoflength100.\n(see [128, Corollary 1]). This expression shows that Laplacian kernels are indeed uni-\nversal. On the other hand, thelinear kernelK(x, x\u2032) := x \u0001 x\u2032 is far from being universal;\nremember that the LSPM [145] corresponds to this kernel anda = 0 .\nFigure 5.1 illustrates that for this data set, universal kernels lead to better results. The\nparameter a in Figure5.1 is the true one,a = 1 . In the case of the Bayesian predictive\ndistribution, the parameter\u03c3 = 1 is also the true one \u2014 unlike for Bayesian predictive\ndistributions predictive distributions do not require\u03c3. The right-most panel shows that,\ntheconformalpredictivedistributionbasedonthelinearkernelcangetthepredictivedis-\ntribution wrong. The other two panels show that the true kernel and, more importantly,\ntheLaplaciankernel(chosenindependentlyofthemodel( 5.23))aremuchmoreaccurate.\nFigure5.1showspredictivedistributionsforaspecifictestobject, (1, 1),butthisbehaviour\nis typical. The effect of using a universal kernel becomes much less pronounced (or even\ndisappears completely) for smaller lengths of the training sequence: see Figure5.2 using\n100 training observations (whereas Figure5.1 uses 1000).",
        "56e04b52-895b-4553-b941-9912f410bba9": "5.8. Experimental results 115\nFIGURE 5.3: Left panel: predictions of the KRRPM for a training sequence of\nlength 1000 and x1001 = 0 . Right panel: predictions forx1001 = 1 . The data\nare described in the text.\nWe now illustrate the limitation of the KRRPM that we discussed in the previous sec-\ntion. An artificial data set is generated as follows:xi 2 [0, 1], i = 1 , . . . , n, are chosen in-\ndependently from the uniform distributionU on [0, 1], andyi 2 [\u0000xi, xi] are then chosen\nindependently, again from the uniform distributionsU[\u0000xi, xi] on their intervals. Fig-\nure 5.3 shows the prediction forxn+1 = 0 on the left and forxn+1 = 1 on the right for\nn = 1000 ; there is no discernible difference between the studentized and ordinary ver-\nsions of the KRRPM. The difference between the predictions forxn+1 = 0 and xn+1 = 1\nis slight, whereas ideally we would like the former prediction to be concentrated at 0\nwhereas the latter should be close to the uniform distribution on[\u00001, 1] [139].\nFinedetails can be seen in Figure5.4, which is analogous toFigure5.3but uses a train-\ning sequence of lengthn = 10 . It shows the plots of the functionsQn(y, 0) and Qn(y, 1) of\ny,inthenotationof( 4.3). Thesefunctionscarryallinformationabout Qn(y, \u03c4 ) asfunction\nofy and\u03c4 sinceQn(y, \u03c4 ) canbecomputedastheconvexmixture (1\u0000\u03c4)Qn(y, 0)+\u03c4Qn(y, 1)\nof Qn(y, 0) and Qn(y, 1) [139].\nIn our future experiments we can use three kinds of kernels:\n\u2022 the polynomial kernel\nK(x, x\u2032) := (1 + x \u0001 x\u2032)k\nparameterized byk = 1 , 2, . . . and sometimes abbreviated to \u201cpolyk\u201d;\n\u2022 the Gaussian kernel\nK(x, x\u2032) := exp\n\u00c4\n\u0000c\n\r\rx \u0000 x\u2032\r\r2\u00e4",
        "6b125747-0dc0-49af-b1a1-eb47676158eb": "116 Chapter 5. Kernel probabilistic regression\nFIGURE 5.4: Upper left panel: predictions of the (studentized) KRRPM for a\ntraining sequence of length10 and x11 = 0 . Upper right panel: analogous\npredictions forx11 = 1 . Lower left panel: predictions of the ordinary KR-\nRPM for a training sequence of length10 and x11 = 0 . Lower right panel:\nanalogous predictions forx11 = 1 .\nparameterized byc > 0 and sometimes abbreviated to \u201cGaussc\u201d;\n\u2022 the Laplacian kernel\nK(x, x\u2032) := exp \u0000\u0000c\n\r\rx \u0000 x\u2032\r\r\u0001\nparameterized byc > 0 and sometimes abbreviated to \u201cLaplacec\u201d.\nAccording to [110] (see also [47, Example 2.22]), polynomial kernels are not universal\nandthecorrespondingreproducingkernelHilbertspaceconsistsofdegree k polynomials.\nOntheotherhand, accordingto[ 126], Gaussiankernelsareuniversal(and[ 126]givesan\nexplicit description of the corresponding reproducing kernel Hilbert spaces). Laplacian\nkernels are also universal, as mentioned earlier.\nFurther experiments with artificial data\nWe follow the experiments in Section 10.3 of [137] but in the kernelized setting. In this\nchapter we use pictures of two kinds. First, those for the correct kernel (such as (5.24)):",
        "4fe059eb-456d-4cf8-aea2-b135bae655f5": "5.8. Experimental results 117\n1. The validity pictures like those in Section 10.3 of [137], Figures 10.1\u201310.4. The cal-\nibration curve would be a plot of the empirical frequency ofQn \u0014 \u03f5 vs \u03f5. The ef-\nficiency would be measured by one number (no need to show it on the plot): the\ncumulative loss function (see below) over the test set.\n2. E\ufb00iciency figures similar to those in the next subsection. Now we have the shaded\nareas for the CPDs and the lines for Oracles II and III.\nThose for the standard kernels (different from the true one (5.24)), such as polynomial,\nGaussian, and Laplacian: the only kind of pictures and figures would be those described\nin item 1.\nExperiments with a benchmark data set\nIn our experiments we can use continuous ranked probability score, which is the most\nstandard proper loss function for regression [48, 41]. To make it applicable to our confor-\nmal predictive distributions, we modify them slightly by replacing (5.10) by\nQn(y, \u03c4 ) :=\n8\n>><\n>>:\ni\nn if y 2 (C(i), C(i+1)) for i 2 f0, 1, . . . , ng\ni\nn if y = C(i) for i 2 f1, . . . , ng.\nSince Q is now a bona fide distribution function, we can use the usual expression\nCRPS(Q, y) :=\nZ y\n\u2212\u221e\nQ(u, 0)2du +\nZ \u221e\ny\n(1 \u0000 Q(u, 1))2du\n(as in [41, (20)]). This loss function can be used for comparing conformal predictive\ndistributions among themselves and with other predictive distributions.\nFirst we use the standard Boston Housing data set available from the UCI repository\n[33]. It consists of 506 observations each with 14 attributes. We split it into a training\nset of size 406 and test set of size 100 (using the original order of the observations). All\nattributes were standardized making the mean of each attribute zero and its standard\ndeviation 1 (based only on the training set).",
        "db77f27c-bfaf-452d-b83f-13df63e8ed30": "118 Chapter 5. Kernel probabilistic regression\nKernel a = 0 .001 a = 0 .01 a = 0 .1 a = 1 a = 10\npoly 1 3.45 3.45 3 .45 3 .44 3 .42\npoly 2 2.71 2.63 2 .58 2 .56 2 .70\npoly 3 59.90 20 .40 8 .62 4 .43 6 .86\nGauss 0.01 2.30 2.29 2 .38 3 .23 5 .04\nGauss 0.05 2.42 2.24 2 .40 2 .99 5 .33\nGauss 0.1 2.74 2.41 2 .60 3 .50 6 .29\nTABLE 5.1: The cumulative losses for polynomial kernels and Gaussian ker-\nnels\nFIGURE5.5: Thepredictivedistributionforthelabelofthe100thobservation\n(based on the previous 99 observations) and for the label of the 500th ob-\nservation(basedontheprevious499observations)fortheBostonHousing\ndata set\nOur results are given in Table5.1. For polynomial kernels, the best result is for degree\n2and a = 1 : thecumulativelossis2.56. ThebestRBFresultisslightlybetter: itsparameter\nc = 0 .05 and a = 0 .01, the loss is 2.24, but this was more sensitive to parameter tuning.\nWe do not give validity results like those in [145] (they are useless since validity is\nguaranteed by our theoretical results). Two sample predictive distribution functions are\nshowninFigure 5.5. ThekernelisGaussian, andtheparametersare a = 0 .01 andc = 0 .05\n(giving the best values in Table5.1).\n5.9 Conclusion\nThe main contribution of this chapter is extension the study of probabilistic regression\nby combining conformal predictive distributions with kernel methods (published paper\nversion [139]). Experimental study of the predictive e\ufb00iciency demonstrates important",
        "d3e64b6c-3a41-430b-9c54-763f550963b2": "5.9. Conclusion 119\nadvantages of the kernelized versions of CPDs and shows that universal (Laplacian) ker-\nnelperformsremarkablywellintermsofoutputtingaccurateprobabilisticpredictionsfor\nthe test objects.",
        "6d5c45b9-676b-4011-ad31-ab0b03144c4c": "120",
        "40867467-34c7-4e3c-a036-09dafde355c2": "121\nChapter 6\nComputationally e\ufb00icient\nprobabilistic regression\nConformal predictive distributions (CPD) output probability distributions of the label value in\nmachine learning regression problems. This chapter extends the study of probabilistic regression\nto address the relative computational ine\ufb00iciency of classical conformal predictors. Two novel com-\nputationally e\ufb00icient conformal predictive systems are introduced \u2014 split conformal predictive\nsystems (SPC) and cross-conformal predictive system (CCPS). Split conformal predictive sys-\ntems provide guaranteed validity, whilst the main advantage of cross-conformal predictive systems\nis greater predictive e\ufb00iciency. For cross-conformal predictive systems validity only holds empiri-\ncally and in the absence of excessive randomization. The main aim of this chapter is to define and\nstudy computationally e\ufb00icient versions of CPS without any restrictions related to the underlying\nalgorithm.\n6.1 Introduction\nIn this chapter, we continue the study of probabilistic regression problem introduced in\nChapter 4 and continued in Chapter5 with the study of kernel probabilistic regression.\nChapter 4 described extension of parametric statistical \u201cprediction confidence distribu-\ntions\u201d to machine learning regression tasks using only limited assumptions customary of\nmachine learning, namely that the observations are generated using the IID model. Un-\nlike the parametric approach of statistical predictive distributions, conformal predictive",
        "ff2bbf1e-3728-4ba4-8d8d-c1940338847d": "122 Chapter 6. Computationally e\ufb00icient probabilistic regression\ndistributions do not require specification of the data model and thus can be applied to a\nwide range of prediction and decision-making problems.\nThedisadvantageofCPSisthatformanyunderlyingalgorithmsCPSarecomputation-\nally ine\ufb00icient as they require re-training the underlying machine learning or statistical\nalgorithm for each test object and each potential label for this object. In practice this can\nbe done e\ufb00iciently only for a narrow class of underlying algorithms, including for Least\nSquares and Kernel Ridge Regression as described in Chapter4 and Chapter4 accord-\ningly.\nAveryrecentdevelopmentinVennpredictionhasbeentheintroductionofsplitVenn-\nAbers predictive systems in [101]. Venn-Abers predictive systems are another way to\nproduce predictive distributions. In this chapter we explore several versions of Venn-\nAbers predictive systems and compare them with conformal predictive systems.\nWefollowthedefinitionofrandomizedpredictivesystems(RPS)introducedinChap-\nter 4 and in section6.2 define a special case of RPS \u2014 split conformal predictive systems\n(SCPS).Splitconformalpredictivesystems(SCPS)arecomputationallye\ufb00icient,butmay\nsuffer loss in predictive e\ufb00iciency when compared with CPS, as CPS uses data more ef-\nficiently. A very important advantage of SCPS is their validity (similar to CPS) \u2014 we\ndemonstrate validity of SCPS in Section6.2.\nSection 6.3 introduces cross-conformal predictive systems (CCPS), whilst CCPS use\ndata more e\ufb00iciently, they can lose their validity in principle as they are formally are no\nlongerRPS\u2014inpracticeCCPSusuallysatisfytherequirementofvalidityasdemonstrated\nin Section6.5.\nSection 6.5 compares the predictive e\ufb00iciency of SCPS and CCPS and explores their\nempirical validity. The predictive e\ufb00iciency of predictive distributions is measured using\na loss function called continuous ranked probability score (CRPS, see1.2.5).\n6.2 Split conformal predictive systems\nIn this section we will modify the definition4.4.6 of conformal predictive systems intro-\nduced in Chapter4 along the lines of [6, Section 2.3] by removing an unnecessary as-\nsumption in [137, Section 4.1]. The definitions below follow [139].",
        "ffcbd04d-6567-4afd-b8a2-0f2cf415b9ab": "6.2. Split conformal predictive systems 123\nDefinition 6.2.1(Split Conformity Measure). A split conformity measure is a family of\nmeasurable functionsAm : Zm+1 ! R [ f\u00001, 1g, m = 1 , 2, . . . . The intention is that\nAm(z1, . . . , zm+1) measures how large the labelym+1 in zm+1 is, as compared with the\nlabels inz1, . . . , zm.\nConsider the training sequencez1, . . . , zn that is split into two parts: the proper train-\ning sequencez1, . . . , zm and the calibration sequencezm+1, . . . , zn. Based on the objectx\nfrom the test set, the task of probabilistic regression is to predict probability distribution\nfory (similarnotationuseinChapter 4wewilluse y andyn+1 interchangeablytodescribe\nthe label of the test object that requires prediction.\nDefinition 6.2.2(Split Conformal Transducer). The output of the split conformal trans-\nducer determined by the split conformity measureA is defined as\nQ(z1, . . . , zn, (x, y), \u03c4) := 1\nn \u0000 m + 1 jfi = m + 1, . . . , n j \u03b1i < \u03b1ygj\n+ \u03c4\nn \u0000 m + 1 jfi = m + 1, . . . , n j \u03b1i = \u03b1ygj + \u03c4\nn \u0000 m + 1 , (6.1)\nDefinition 6.2.3(Conformity Scores). The conformity scores\u03b1i, i = m+ 1, . . . , n, and\u03b1y,\ny 2 R, are defined by\n\u03b1i := A(z1, . . . , zm, (xi, yi)), i = m + 1, . . . , n,\n\u03b1y := A(z1, . . . , zm, (x, y)).\nDefinition 6.2.4(SplitConformalTransducer) . Afunctionisasplitconformaltransducer\nif it is the split conformal transducer determined by some split conformity measure.\nDefinition 6.2.5(Split Conformal Predictive System). A split conformal predictive sys-\ntem (SCPS) is a function which is both a split conformal transducer and a randomized\npredictive system.\nThe standard property of validity for split conformal transducers (that is satisfied\nautomatically) is that the valuesQ(z1, . . . , zn, z, \u03c4 ) have uniform distribution on[0, 1] if\nz1, . . . , zn, z are IID and\u03c4 is generated independently ofz1, . . . , zn, z from the uniform\nprobability distributionU on [0, 1] (see, e.g., [137, Proposition 4.1]).",
        "bb364e96-ac17-4c1b-b5da-252ec717a581": "124 Chapter 6. Computationally e\ufb00icient probabilistic regression\nItismucheasiertogetanRPSusingsplitconformaltransducersthanusingconformal\ntransducers.\nDefinition 6.2.6 (Isotonic Split Conformity Measure). A split conformity measureA is\nisotonic if, for allm, z1, . . . , zm, andx, A(z1, . . . , zm, (x, y)) is isotonic iny, i.e.,\ny \u0014 y\u2032 =) A(z1, . . . , zm, (x, y)) \u0014 A(z1, . . . , zm, (x, y\u2032)) (6.2)\nDefinition 6.2.7(BalancedSplitConformityMeasure) . Anisotonicsplitconformitymea-\nsure A is balanced if, for anym and z1, . . . , zm, the set\nconv A(z1, . . . , zm, (x, R)) := conv fA(z1, . . . , zm, (x, y)) j y 2 Rg (6.3)\ndoes not depend onx, whereconv stands for the convex closure inR.\nThe set (6.3) then coincides with conv A(z1, . . . , zm, Z) and has one of four forms:\n(a, b),[a, b),(a, b],or [a, b],where a < b areelementsoftheextendedrealline R[f\u00001, 1g;\nwe will be mainly interested in the caseconv A(z1, . . . , zm, Z) = ( \u00001, 1).\nProposition 6.The split conformal transducer(6.1)based on a balanced isotonic split conformity\nmeasure is an RPS.\nProof. Since property R2 in the definition of RPS4.4.1 is automatic, we only need to check\nR1. It is clear that (6.1) is increasing in\u03c4 (and linear).\nTo show that it is increasing iny, split, in the context of (6.1), alli 2 f m + 1, . . . , ng\ninto three groups: thei in group 1 satisfy\u03b1i < \u03b1 y, thei in group 2 satisfy\u03b1i = \u03b1y, and\nthe i in group 3 satisfy\u03b1i > \u03b1 y. Then (6.1) is the total weight of alli where the weights\nare 1, \u03c4 2 [0, 1], and0 for i in groups 1, 2, and 3, respectively. Asy increases, \u03b1y increases\nas well, and therefore, eachi can only move to a lower-numbered group thus increasing\n(6.1).\nOut of the remaining two conditions, let us check, e.g., (4.2). It su\ufb00ices to notice that,\nsince A is balanced, we have\u03b1y \u0015 maxi\u2208{m+1,...,n} \u03b1i from somey on, for anyz1, . . . , zn\nand x.",
        "49cdee3a-0c20-4eec-81b4-e63b9370f335": "6.2. Split conformal predictive systems 125\nThe next proposition shows that a split conformity measure being isotonic and bal-\nanced is not only a su\ufb00icient but also a necessary condition for the corresponding split\nconformal transducer to be an RPS.\nProposition 7.If the split conformal transducer based on a split conformity measureA is an RPS,\nA is isotonic and balanced.\nProof. SupposeA isnotisotonic. Fix m,z1, . . . , zm,x,y,and y\u2032 suchthat y < y \u2032 butthecon-\nsequent of (6.2) is violated. Then the predictive distributionQ(z1, . . . , zm, (x, y), (x, \u0001), 1),\ncorresponding to the training sequence properz1, . . . , zm, calibration sequence(x, y), test\nobject x, and\u03c4 = 1 , will not be increasing: its value aty (which is1) will be greater than\nits value aty\u2032 (which is0.5).\nNow supposeA is not balanced. Fixm, z1, . . . , zm, andx, x\u2032 2 X such that\nconv A(z1, . . . , zm, (x, R)) 6= conv A(z1, . . . , zm, (x\u2032, R))\n(cf. (6.3)). Suppose, for concreteness, that there isy 2 R such that\nconv A(z1, . . . , zm, (x, R)) 3 y < conv A(z1, . . . , zm, (x\u2032, R)),\nwhere y < S means 8s 2 S : y < s when S \u0012 R. (The other three possible cases can be\nanalyzed in the same way.) Let the training sequence proper bez1, . . . , zm, the calibration\nsequence be(x, y), the test object bex\u2032, and the random number be\u03c4 = 0 . Then we will\nhave\nlim\ny\u2032\u2192\u2212\u221e\nQ(z1, . . . , zm, (x, y), (x\u2032, y\u2032), 0) > 0,\nwhich contradicts R1 (cf. (4.2)).\nLet us say that a split conformity measureA is strictly isotonicif (6.2) holds with both\n\u201c\u0014\u201d replaced by \u201c<\u201d. A possible implementationof the SCPS based on a balanced strictly",
        "8ca8528c-b8a4-4eb3-841c-904e7da44927": "126 Chapter 6. Computationally e\ufb00icient probabilistic regression\nAlgorithm 4Split Conformal Predictive System\nRequire: A training sequence(xi, yi) 2 Z, i = 1 , . . . , n.\nRequire: A test objectx 2 X.\nfor i 2 f1, . . . , n \u0000 mg do\nDefine Ci by the conditionA(z1, . . . , zm, zm+i) = A(z1, . . . , zm, (x, Ci)).\nend for\nSort C1, . . . , Cn\u2212m in the increasing order obtainingC(1) \u0014 \u0001 \u0001 \u0001 \u0014 C(n\u2212m).\nSet C(0) := \u00001 and C(n\u2212m+1) := 1.\nReturn the predictive distribution (6.4) for the labely of x.\nisotonic split conformity measure is shown as Algorithm4, where the predictive distribu-\ntion is defined by\nQ(z1, . . . , zn, (x, y), \u03c4) :=\n8\n>>><\n>>>:\ni+\u03c4\nn\u2212m+1 if y 2 (C(i), C(i+1)) for i 2 f0, 1, . . . , n \u0000 mg\ni\u2032\u22121+(i\u2032\u2032\u2212i\u2032+2)\u03c4\nn\u2212m+1 if y = C(i) for i 2 f1, . . . , n \u0000 mg,\n(6.4)\nwhere i\u2032 := min fj j C(j) = C(i)g and i\u2032\u2032 := max fj j C(j) = C(i)g. To use the terminology\nof Chapter4, the thickness of this predictive distribution is1\nn\u2212m+1 with the exception size\nat mostn \u0000 m.\nComputational complexity of the Algorithm4 depends on how di\ufb00icult it is to solve\nthe equation definingCi. For a standard choice of split conformity measure:\nA(z1, . . . , zm, (x, y)) := y \u0000 \u02c6y\n\u02c6\u03c3 , (6.5)\nwith \u02c6y being prediction for the labely that is computed based on the training sequence\nz1, . . . , zm and test objectx,and \u02c6\u03c3 is an estimate of the quality of\u02c6y computed from the\nsame data. In this case the equation definingCi\nA(z1, . . . , zm, zm+i) = A(z1, . . . , zm, (x, Ci)) (6.6)\nbecomes\nym+i \u0000 \u02c6ym+i\n\u02c6\u03c3m+i\n= Ci \u0000 \u02c6y\n\u02c6\u03c3 ,",
        "5258f2c0-1997-4690-bed4-d25f86fc63cb": "6.3. Cross-conformal predictive distributions 127\nwhere \u02c6ym+i (resp. \u02c6y) is the prediction forym+i (resp. y) computed fromxm+i (resp. x) as\ntest object andz1, . . . , zm as training sequence, and\u02c6\u03c3m+i (resp. \u02c6\u03c3) is the estimate of the\nqualityof \u02c6ym+i (resp. \u02c6y)computedfromthesamedata. Thelastequationcanberewritten\nto define expression forCi as:\nCi := \u02c6y + \u02c6\u03c3\n\u02c6\u03c3m+i\n(ym+i \u0000 \u02c6ym+i) .\nFor more complicated split conformity measuresA, it might be more e\ufb00icient to use the\nexpression (6.1) directly for a grid of values ofy.\n6.3 Cross-conformal predictive distributions\nRemember that amultiset (or bag) is different from a set in that it can contain several\ncopies of the same element. A split conformity measureA is across-conformity measureif\nA(z1, . . . , zm, z) does not depend on the order of its firstm arguments; in other words, if\nA(z1, . . . , zm, z) only depends on the multiset*z1, . . . , zm+ and z (where * \u0001 \u0001 \u0001+ is used as\nthe analogue off\u0001 \u0001 \u0001 g for multisets).\nForabalancedisotoniccross-conformitymeasure A,thecorresponding cross-conformal\npredictive system(CCPS) is defined as follows.\nDefinition 6.3.1(Cross-Conformal Predictive System (CCPS)).\npy = Q(z1, . . . , zn, (x, y), \u03c4) := 1\nn + 1\nKX\nk=1\n\f\f\bi 2 Sk j \u03b1i,k < \u03b1y\nk\n\t\f\f\n+ \u03c4\nn + 1\nKX\nk=1\n\f\f\bi 2 Sk j \u03b1i,k = \u03b1y\nk\n\t\f\f+ \u03c4\nn + 1 . (6.7)\nWhere (S1, . . . , SK) is a partition of the index setf1, . . . , ng, andzSk consists of allzi,\ni 2 Sk for random split of the training sequencez1, . . . , zn into K non-empty multisets\n(folds) zSk , k = 1 , . . . , K , of equal (or as equal as possible) sizes (whereK 2 f2, 3, . . .g is\na parameter of the algorithm).",
        "7d5b1560-dfa9-4ce8-bc48-23dfd3faafd3": "128 Chapter 6. Computationally e\ufb00icient probabilistic regression\nAlgorithm 5Cross-Conformal Predictive System\nRequire: A training sequence(xi, yi) 2 Z, i = 1 , . . . , n.\nRequire: A test objectx 2 X.\nSplit z1, . . . , zn into K folds zSk as described in text.\nSet C := ;, whereC is a multiset.\nfor k 2 f1, . . . , K g do\nfor i 2 Sk do\nDefine Ci,k by the conditionA(zS\u2212k , zi) = A(zS\u2212k , (x, Ci,k)).\nPut Ci,k in C.\nend for\nend for\nSort C in the increasing order obtainingC(1) \u0014 \u0001 \u0001 \u0001 \u0014 C(n).\nSet C(0) := \u00001 and C(n+1) := 1.\nReturn the predictive distribution (6.8) for the labely of x.\nFor eachk 2 f 1, . . . , K g and each potential labely 2 R of the test objectx, find the\nconformity scores of the observations inzSk and of(x, y) by\n\u03b1i,k := A(zS\u2212k , zi), i 2 Sk, \u03b1 y\nk := A(zS\u2212k , (x, y)),\nwhere S\u2212k := [j\u0338=kSj = f1, . . . , ng n Sk. The corresponding p-values and CCPS are de-\nfined by equation6.7.\nThe intuition behind (6.7) is that it becomes an SCPS when the training multisetszS\u2212k\nare replaced by a single hold-out training sequence (one disjoint from and independent\nof z1, . . . , zn).\nAlgorithm 5 describes an implementation of the CCPS based on a balanced strictly\nisotonic cross-conformity measure, where the predictive distribution is now defined by\nQ(z1, . . . , zn, (x, y), \u03c4) :=\n8\n>>><\n>>>:\ni+\u03c4\nn+1 if y 2 (C(i), C(i+1)) for i 2 f0, 1, . . . , ng\ni\u2032\u22121+(i\u2032\u2032\u2212i\u2032+2)\u03c4\nn+1 if y = C(i) for i 2 f1, . . . , ng,\n(6.8)\nwhere, as before,i\u2032 := min fj j C(j) = C(i)g and i\u2032\u2032 := max fj j C(j) = C(i)g; the only\ndifference from (6.4) is that we usen in place ofn \u0000 m (now all training observations\nare used for calibration). The thickness of this predictive distribution is 1\nn+1 with the",
        "ca57d1cc-a457-4aa5-9fc9-cde9414ed10d": "6.3. Cross-conformal predictive distributions 129\nexception size at mostn. The size of the multisetC in Algorithm5 grows from0 to n as\nthe algorithm runs. As in the case of SCPS, it might be easier to use (6.7) directly if the\nequations definingCi,k are di\ufb00icult to solve. (Alternatively, one could use (6.10) below\ninstead of (6.7).)\nDefinition 6.3.2(Fold p-value). Define afold p-valueas separate p-value for each fold\npy\nk := 1\njSkj + 1\n\f\f\bi 2 Sk j \u03b1i,k < \u03b1y\nk\n\t\f\f+ \u03c4\njSkj + 1\n\f\f\bi 2 Sk j \u03b1i,k = \u03b1y\nk\n\t\f\f+ \u03c4\njSkj + 1 (6.9)\nfor each fold (cf. (6.1)); let us check thatpy is close to being an average ofpy\nk. Com-\nparing (6.7) and (6.9), we can see that\n(n + 1)py \u0000 \u03c4 =\nKX\nk=1\n(jSkj + 1) py\nk \u0000 K\u03c4,\nwhich implies\npy =\nKX\nk=1\njSkj + 1\nn + 1 py\nk \u0000 K \u0000 1\nn + 1 \u03c4. (6.10)\nThe sumPK\nk=1 . . . is not quite a weighted average ofpy\nk since the sum of the weights is\nslightly above 1 (\u201cslightly\u201d assumesK \u001c n), but this is partially compensated by the\nsubtrahendin (6.10); overall, theright-hand sideof (6.10)is aweightedaverageof py\nk and\n\u03c4, with the weight in front of\u03c4 being negative.\nAccording to the intuition behind cross-conformal predictive distributions described\nearlier, we will get perfect validity for CCPS if we replace theK training multisets (the\ncomplements to theK folds) by one hold-out training sequence. But whereas SCPS are\nprovably valid, in the sense of being RPS, real CCPS are not RPS: see the example in [135,\nAppendix A]. This has been demonstrated in experimental studies, e.g., Linussonet al.\n[81]hasshownthedangerofrandomizedandextremelyunstableunderlyingalgorithms.\n(Perhapssuchunstablealgorithmsmightbestabilized,tosomedegree,byusingthesame\nseed of the random numbers generator for each fold, or by averaging conformity scores\nover several seeds, or both.)\nA useful intuition in [81] is that \u201cthe random fold p-value and then essentially aver-\naged by cross-conformal predictors are to some degree independent. The distribution of",
        "8e97489d-cfdd-4e31-99d6-203053e74184": "130 Chapter 6. Computationally e\ufb00icient probabilistic regression\ncross-conformalp-valuesisintermediatebetweentheuniformandtheBatesdistributions,\ncross-conformal p-values are therefore conservative (for small significance levels) when\nnot exact.\u201d According to the result in [147] (see, e.g., Table 1 forr := 1 ), one can obtain\nprovably valid (but perhaps conservative) p-values, when p-values output by a cross-\nconformal transducer are multipled by 2. Linussonet al. [81] observed empirically that\n\u201cfor randomized and unstable underlying algorithms even unadjusted p-values output\nby a cross-conformal transducer are valid but perhaps overly conservative for interesting\n(not exceeding0.5) significance levels\u201d.\nA more general procedure than the cross-conformal predictor was proposed in [15]\nunder the name of \u201caggregated conformal predictor\u201d. Similar methods might be applica-\nble for producing conformal predictive distributions.\n6.4 Continuous ranked probability score\nThe continuous ranked probability score was previously defined in Chapter1 (see 1.2.5)\nas\nCRPS(F, yi) :=\nZ \u221e\n\u2212\u221e\n\u0000F(y) \u0000 1{y\u2265yi}\n\u00012 dy, (6.11)\nCRPS attains the lowest possible value of 0 when distribution functionF is concen-\ntrated atyi, and in all other casesCRPS(F, yi) will be positive. (See, e.g., [40] for further\ndetails and references.)\nEquation(6.11)isnotdirectlyapplicabletosplitandcross-conformalpredictivedistri-\nbutions,howeverinpracticethefuzzinesscanusuallybeignored,evenforrelativelysmall\ndatasets: see, e.g., Figure6.1 and [136]. However, conceptually we do need to change the\ndefinitions of split and cross-conformal predictive distributions slightly to remove their\nfuzziness [140].\nInstead of (6.4) and (6.8) we use their crisp modifications\nQ(z1, . . . , zn, (x, y)) :=\n8\n>><\n>>:\ni\nn\u2212m if y 2 (C(i), C(i+1)) for i 2 f0, 1, . . . , n \u0000 mg\ni\nn\u2212m if y = C(i) and y 6= C(i+1) for i 2 f1, . . . , n \u0000 mg\n(6.12)",
        "3631b896-ffcb-44a8-b5b0-573c56d84380": "6.4. Continuous ranked probability score 131\n0 5 10 15 20 25 30 35 40\ny\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0SCPD\nFIGURE 6.1: The split conformal predictive distribution for a random test\nobject in theBoston Housing dataset (described in Section6.5), the Least\nSquaresunderlyingalgorithm,andarandom 50% : 50% splitofthetraining\nsequenceintopropertrainingandcalibrationsequences. Thebluesolidline\ncorresponds to\u03c4 = 0 and the red dashed line to\u03c4 = 1 .\nand\nQ(z1, . . . , zn, (x, y)) :=\n8\n>><\n>>:\ni\nn if y 2 (C(i), C(i+1)) for i 2 f0, 1, . . . , ng\ni\nn if y = C(i) and y 6= C(i+1) for i 2 f1, . . . , ng,\n(6.13)\nrespectively; these modifications no longer depend on\u03c4, and the convention fory = C(i)\ndoesnotaffectthevalueofCRPS.Incaseswheretheequation( 6.6)oritsanalogueforthe\nCCPS are di\ufb00icult to solve, we can instead use the following crisp modifications of (6.1)\nand (6.7), respectively:\nQ(z1, . . . , zn, (x, y)) := 1\nn \u0000 m jfi = m + 1, . . . , n j \u03b1i \u0014 \u03b1ygj,\nQ(z1, . . . , zn, (x, y)) := 1\nn\nKX\nk=1\n\f\f\bi 2 Sk j \u03b1i,k \u0014 \u03b1y\nk\n\t\f\f.",
        "6fc47d6e-c8ce-4233-ab60-519c2fe80757": "132 Chapter 6. Computationally e\ufb00icient probabilistic regression\nThe last equation, defining a crisp CCPS, can be rewritten as\nQ(z1, . . . , zn, (x, y)) =\nKX\nk=1\njSkj\nn py\nk\n(cf. (6.10)), where the separate \u201cp-values\u201d for each fold are now defined as\npy\nk := 1\njSkj\n\f\f\bi 2 Sk j \u03b1i,k \u0014 \u03b1y\nk\n\t\f\f\n(they, however, do not satisfy any validity properties).\n6.5 Experiments\nThe purpose of this section is to compare the predictive performance of SCPS and CCPS\nand to recommend the choice of the parameterK for CCPS.\nIn our experiments we use five well-known benchmark datasets, namely Boston\nHousing, Diabetes, Yacht Hydrodynamics , Wine Quality , and Condition Based\nMaintenance of Naval Propulsion Plants (abbreviated toNaval Propulsion ) avail-\nable at http://scikit-learn.org/stable/datasets/ (the first two) and the UCI Machine\nLearning repository [33] (the other ones). The first three datasets are small: Boston\nHousing consists of 506 observations, Diabetes of 442 observations, and Yacht\nHydrodynamics of 308 observations; for them we use test sequences of lengthl := 100 .\nThe Wine Quality dataset consists of 6497 observations, and we use test sequences of\nlength l := 1000 . Finally, theNaval Propulsion dataset consists of 11,934 observations,\nand we use test sequences of lengthl := 4000 .\nGiven a training sequence(z1, . . . , zn) (where n 2 f 406, 342, 208, 5497, 7934g) and a\ntest sequence(zn+1, . . . , zn+l), the quality of prediction is represented by the distribution\nof CRPS(Fi, yi), i = n + 1, . . . , n + l, whereFi is the predictive distribution for the labelyi\nof the test objectxi. As already mentioned, the lengthl of the test sequence is 100, 1000,\nor 4000 in our experiments.\nIn order to obtain boxplots less affected by the split of each dataset into a training and\ntest sequence and by the random split of each training sequence into a training sequence",
        "b8234e47-2009-4b92-89ec-9c9ac936841f": "6.5. Experiments 133\nproper and a calibration sequence (in the case of SCPS) orK folds (in the case of CCPS),\nwe use the procedure describe below (see experiments section in [143] for more details):\n\u2022 Each dataset is randomly permuted 10 times.\n\u2022 The last l observations of each permutation are used for testing and the rest for\ntraining.\n\u2022 The first m observations in the training sequence are used as training sequence\nproper in the case of SCPS and consecutive blocks of the training sequence are used\nas theK folds in the case of CCPS (using thescikit-learn KFold procedure with\nno randomization).\n\u2022 Theboxplotsinallfiguresgivenbelowareindexedbythefractions m/n ofthetrain-\ning sequence used as the training sequence proper (in the case of SCPS) or by the\nnumbers K of folds (in the case of CCPS).\n\u2022 Foreachsplitandeachboxplotwefindthe l valuesCRPS(Fi, yi) foralltestobserva-\ntions (the same test sequence is used for each split); the resulting boxplot is based\non all10 l numbers.\nIn all cases the SCPS and CCPS use the cross-conformity measure (a special case of\n(6.5))\nA(z1, . . . , zm, (x, y)) := y \u0000 \u02c6y, (6.14)\nwhere \u02c6y is the prediction computed using the underlying algorithmU for the label ofx\nbasedon z1, . . . , zm astrainingsequence. (Rememberthateachcross-conformitymeasure\nis also a split conformity measure.) Similarly to the CPS based on Least Squares and Ker-\nnel Ridge Regression (as discussed in previous chapters \u2014 Chapters5, 4), this procedure\nis far from universal and can be expected to be e\ufb00icient only for data that is not too far\nfrom being homoscedastic.\nNotice that, the SCPS is no longer provably calibrate because parameter tuning de-\npends on the full training sequence), we therefore also check its validity in our experi-\nments. To check the validity of both SCPS and CCPS, we run Algorithm6.5 replacing",
        "22946832-c9af-47d8-888a-e1f828091680": "134 Chapter 6. Computationally e\ufb00icient probabilistic regression\n1.0\n10.0CRPS (LS)\nSCPS boston CCPS boston\n1.0\n10.0CRPS (RF)\n0.010.050.10.20.30.40.50.60.70.80.90.950.99\nm / n\n1.0\n10.0CRPS (NN)\n2 3 4 5 7 9 10 20 30 40 60 80 100\nK\nFIGURE 6.2: The performance of the SCPS (left panel) and CCPS (right\npanel) on theBoston Housing dataset using Least Squares (LS), Random\nForest (RF), and Neural Networks (NN) as the underlying algorithms, as\nindicatedontheleft. TheverticalaxisusesthelogscaleandgivestheCRPS.\nLeft panel: the numbers on the horizontal axis are the fractionsm/n of the\ntraining sequence used as the training sequence proper. Right panel: the\nnumbers on the horizontal axis are the numbersK of folds.\nCRPS(Fi, yi) with Fi(yi) and replacing boxplots with plots, such as those in Figure6.7\n(described in detail at the end of this section).\nThe Boston Housing dataset consists of 506 observations each with 14 attributes (de-\nscribing an area of Boston) and a real-valued label (median house price in that area).\nFigure 6.2 shows the performance of the SCPS and CCPS.\nThehorizontalaxisintheleftpanelislabelledby \u03b1 \u0019 m/n; thevaluesof \u03b1 usedinour\nexperimentsarebetween 0.1 and0.9,plusafewmoreextremevalues. Foragivenvalueof\n\u03b1 we setm := b\u03b1nc. We follow the approach in [141]. The CRPS loss is computed for the\n(crisp) SCPS based on (6.14) and the three underlying algorithms on each observation in\nthe test sequence; as described above, we then represent the resulting 1000 CRPS losses\nas a boxplot. We can see a characteristic U-shape (especially pronounced on the left);",
        "6d469f2d-55c2-4f02-b2ed-2b5dc47a69ca": "6.5. Experiments 135\n10\n100CRPS (LS)\nSCPS diabetes CCPS diabetes\n10\n100CRPS (RF)\n0.010.050.10.20.30.40.50.60.70.80.90.950.99\nm / n\n10\n100CRPS (NN)\n2 3 4 5 7 9 10 20 30 40 60 80 100\nK\nFIGURE 6.3: The analogue of Figure6.2 for theDiabetes dataset.\nsmall m/n lead to a significant increase in the CRPS loss, and largem/n lead to a slight\nincrease in the CRPS loss but a significant increase in its variability (the rightmost box\nand its whiskers tend to be longer).\nThe right panel of Figure6.2 is similar to the left panel, but now we use the CCPS and\nlabel the horizontal axis by the numberK of folds. The usual advice in cross validation\nis to useK 2 f5, 10g, and these two values produce reasonable results. In fact, the results\nare remarkably stable and barely depend onK [140].\nThe Diabetes dataset consists of 10 physiological measures on 442 patients, and the\nlabel indicates disease progression after one year. Figure6.3 is the analogue of Figure6.2\nfor this dataset. We can see the same tendencies, withK 2 f5, 10g still being reasonable\nnumbers of folds for CCPS [140].\nThe Yacht Hydrodynamics is the smallest of our datasets. It consists of 7 attributes\nincluding the basic hull dimensions and the boat velocity for 308 experiments, and the\ntask is to predict the residuary resistance of sailing yachts. Figure6.4 suggests that the",
        "55a2ce90-bcd4-4099-adfe-a6e09fe2e563": "136 Chapter 6. Computationally e\ufb00icient probabilistic regression\n1.0\n10.0CRPS (LS)\nSCPS yacht CCPS yacht\n0.0\n0.1\n1.0\n10.0CRPS (RF)\n0.010.050.10.20.30.40.50.60.70.80.90.950.99\nm / n\n0.1\n1.0\n10.0CRPS (NN)\n2 3 4 5 7 9 10 20 30 40 60 80 100\nK\nFIGURE 6.4: The analogue of Figure 6.2 for the Yacht Hydrodynamics\ndataset.\nbehavior shown in Figures6.2 and 6.3 is in fact typical of small datasets.\nThe Wine Quality dataset has information about 1599 red wines and 4898 white\nwines. We merge these two groups creating another attribute taking two values, 0 for\nwhite and 1 for red. The label is the quality of wine expressed as a score between 0 and\n10. (The most common labels are 5 and 6, labels 3 and 9 are very uncommon, and labels\n0 and 1 are absent [140]).\nFigure6.5isqualitativelysimilartoFigures 6.2and6.3. TheshapeoftheplotsforSCPS\nsuggests that we need a reasonable lengthn \u0000 m of the calibration sequence, such as 100\nor 200, since it determines the granularity of the predictive distributions (see, e.g., [140]):\nas we have already mentioned in connection with (6.4), the thickness of the predictive\ndistribution is 1\nn\u2212m+1 . Increasing the length of the calibration sequence further does not\nimprove the predictive performance significantly, and starts hurting it when the training\nsequence proper becomes too short.",
        "5d5068b9-5e1a-4c20-bf86-ae1d00c1eda8": "6.5. Experiments 137\n1.00CRPS (LS)\nSCPS wine CCPS wine\n0.10\n1.00CRPS (RF)\n0.010.050.10.20.30.40.50.60.70.80.90.950.99\nm / n\n1.00CRPS (NN)\n2 3 4 5 7 9 10 20 30 40 60 80 100\nK\nFIGURE 6.5: The analogue of Figure6.2 for theWine Quality dataset.\nFigure6.6reports the results for the largest dataset that we use,Naval Propulsion . It\ncontainsinformationabout11,934simulatedexperiments,eachdescribedby16attributes,\nand the task is to predict the Gas Turbine Compressor decay state coe\ufb00icient for a propul-\nsion plant. Here we observe the same general behavior.\nThebestresultspresentedinFigures 6.2\u20136.6aresummarizedinTable 6.1. Namely, the\ntablereportsthemedianCRPSlossesshowninFigures 6.2\u20136.5obtainedbyoptimizingthe\nparameters m/n in the case of SCPS andK in the case of CCPS. In the majority of cases\nCCPS perform better than SCPS. But what is even more important, CCPS are much less\nsensitive to choosing their parameterK, and so the best results given in Table6.1 are in\nfact typical for them. In all our experiments, it is safe to choose any of the standard values\nfor the numberK of folds in the range from 5 to 10.\nA natural question is whether the CCPS satisfy the property of validity4.4.1 at least\napproximately;rememberthattherearenotheoreticalvalidityresultsforcross-conformal",
        "a1f52fda-2b61-4b7d-8ffa-924a20c1887b": "138 Chapter 6. Computationally e\ufb00icient probabilistic regression\n0.00100CRPS (LS)\nSCPS naval CCPS naval\n0.0001\n0.0010\n0.0100CRPS (RF)\n0.010.050.10.20.30.40.50.60.70.80.90.950.99\nm / n\n0.010\n0.100CRPS (NN)\n2 3 4 5 7 9 10 20 30 40 60 80 100\nK\nFIGURE 6.6: The analogue of Figure6.2 for theNaval Propulsion dataset.\npredictors,andithasbeendemonstratedtheoretically[ 135,AppendixA]andexperimen-\ntally [81] that a loss of validity is possible. Figure6.7 (right panel) shows the distribu-\ntion of the values (6.13) forBoston Housing and K = 5 , where z1, . . . , zn is the train-\ning sequence, and(x, y) range over the elements of the test sequence. Figure6.7 (right\npanel) provides thecalibration curves, which are the sets of points(\u03b1, F (\u03b1)), \u03b1 2 (0, 1)\nranging over the possible significance levels andF(\u03b1) being the percentage of the val-\nues Q(z1, . . . , zn, (x, y)) for (x, y) in the test sequence that do not exceed\u03b1 [140]. The\nright panels of Figures6.8, 6.9, 6.10, and6.11 are the analogues for theDiabetes, Yacht\nHydrodynamics, Wine Quality , and Naval Propulsion datasets, respectively. Under\nperfect validity R2 in 4.4.1 and an infinitely long test sequence, the calibration curves\nshould be the diagonals shown as dashed lines on both panels of Figures6.7\u20136.11; the\nactual calibration curves are fairly close. The calibration curves for otherK are roughly\nsimilar. As mentioned earlier, we also give calibration results for SCPS (in the left panels\nand withm/n \u0019 0.5).",
        "51e07c29-bfd0-4a92-b279-bcf09cd7a036": "6.5. Experiments 139\nTABLE 6.1: Best results for the median CRPS loss for SCPS and CCPS for the\nfive datasets and three underlying algorithms.\nDataset underlying algorithm SCPS CCPS\nBoston Housing Least Squares 1.726 1.533\nBoston Housing Random Forest 0.972 0.906\nBoston Housing Neural Network 1.240 1.211\nDiabetes Least Squares 23.74 23.18\nDiabetes Random Forest 24.23 24.33\nDiabetes Neural Network 22.76 22.10\nYacht Hydrodynamics Least Squares 3.840 3.910\nYacht Hydrodynamics Random Forest 0.1615 0.1322\nYacht Hydrodynamics Neural Network 0.1944 0.1725\nWine Quality Least Squares 0.2810 0.2771\nWine Quality Random Forest 0.1681 0.1618\nWine Quality Neural Network 0.2711 0.2693\nNaval Propulsion Least Squares 0.0007812 0.0007866\nNaval Propulsion Random Forest 0.0001259 0.0001242\nNaval Propulsion Neural Network 0.003051 0.003360\nNot only is the e\ufb00iciency of the CCPS with respect to the CRPS loss better than that\nof the SCPS, it can also be argued that the CCPS may be safer from the point of view\nof validity. Suppose that, for some reason, we would like to avoid randomization and\nuse (6.12) (in the case of SCPS) or (6.13) (in the case of CCPS) instead of (6.4) or (6.8),\nrespectively. The CCPS is still empirically valid in our experiments, even in the extreme\ncase ofK = 100 . On the other hand, when using (6.12) in place of (6.4), the SCPS lose\nnot only theoretical but also empirical validity. For example, forBoston Housing and\nm/n = 0 .99 (the right end of the horizontal axis in the left panel of Figure6.2), the length\nof the calibration sequence is4, and so the empirical predictive distribution (6.12) only\ntakesvaluesin f0, 0.25, 0.5, 0.75, 1g;thedistributionofitsvaluesatthetruelabelsisclearly\nvery different from being uniform.",
        "f8d411e4-ec5d-48db-8ec4-d3b95ff04ab3": "140 Chapter 6. Computationally e\ufb00icient probabilistic regression\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\ncalibration curve boston LS SCPS m/n=0.5\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\ncalibration curve boston LS CCPS K=5\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\ncalibration curve boston RF SCPS m/n=0.5\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\ncalibration curve boston RF CCPS K=5\n0.0 0.2 0.4 0.6 0.8 1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\ncalibration curve boston NN SCPS m/n=0.5\n0.0 0.2 0.4 0.6 0.8 1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\ncalibration curve boston NN CCPS K=5\nFIGURE 6.7: The calibration curves (i.e., the distributions of\nQ(z1, . . . , zn, (x, y)) over the test sequence) for the SCPS and CCPS\non theBoston Housing dataset.\n6.6 Conclusion\nThe main contribution of this chapter is extension of the study of probabilistic regression\nto address the relative computational ine\ufb00iciency of classical conformal predictors (pub-\nlishedpaper[ 140]). Twonovelcomputationallye\ufb00icientversionsofconformalpredictive\nsystems are introduced \u2014 split conformal predictive systems (SPC) and cross-conformal\npredictive system (CCPS). The main advantage of split conformal predictive systems is\ntheir guaranteed validity. The main advantage of cross-conformal predictive systems is\ngreater predictive e\ufb00iciency, for such predictive systems validity only holds empirically\nand in the absence of excessive randomization. The main aim of this chapter is to define\nand study computationally e\ufb00icient versions of CPS without any restrictions related to\nthe underlying algorithm.",
        "f376f465-ab16-46d9-b806-6eb39e6deaae": "6.6. Conclusion 141\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\ncalibration curve diabetes LS SCPS m/n=0.5\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\ncalibration curve diabetes LS CCPS K=5\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\ncalibration curve diabetes RF SCPS m/n=0.5\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\ncalibration curve diabetes RF CCPS K=5\n0.0 0.2 0.4 0.6 0.8 1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\ncalibration curve diabetes NN SCPS m/n=0.5\n0.0 0.2 0.4 0.6 0.8 1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\ncalibration curve diabetes NN CCPS K=5\nFIGURE 6.8: The analogue of Figure6.7 for theDiabetes dataset.",
        "5a704c12-49dd-49ba-802c-7230727cbabb": "142 Chapter 6. Computationally e\ufb00icient probabilistic regression\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\ncalibration curve yacht LS SCPS m/n=0.5\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\ncalibration curve yacht LS CCPS K=5\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\ncalibration curve yacht RF SCPS m/n=0.5\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\ncalibration curve yacht RF CCPS K=5\n0.0 0.2 0.4 0.6 0.8 1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\ncalibration curve yacht NN SCPS m/n=0.5\n0.0 0.2 0.4 0.6 0.8 1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\ncalibration curve yacht NN CCPS K=5\nFIGURE 6.9: The analogue of Figure 6.7 for the Yacht Hydrodynamics\ndataset.",
        "6027be36-5ca5-48c0-897c-256ed099bb2a": "6.6. Conclusion 143\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\ncalibration curve wine LS SCPS m/n=0.5\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\ncalibration curve wine LS CCPS K=5\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\ncalibration curve wine RF SCPS m/n=0.5\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\ncalibration curve wine RF CCPS K=5\n0.0 0.2 0.4 0.6 0.8 1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\ncalibration curve wine NN SCPS m/n=0.5\n0.0 0.2 0.4 0.6 0.8 1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\ncalibration curve wine NN CCPS K=5\nFIGURE 6.10: The analogue of Figure6.7 for theWine Quality dataset.",
        "4e4546b4-8052-4838-94ce-fd1a0eb59669": "144 Chapter 6. Computationally e\ufb00icient probabilistic regression\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\ncalibration curve naval LS SCPS m/n=0.5\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\ncalibration curve naval LS CCPS K=5\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\ncalibration curve naval RF SCPS m/n=0.5\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\ncalibration curve naval RF CCPS K=5\n0.0 0.2 0.4 0.6 0.8 1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\ncalibration curve naval NN SCPS m/n=0.5\n0.0 0.2 0.4 0.6 0.8 1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\ncalibration curve naval NN CCPS K=5\nFIGURE 6.11: The analogue of Figure6.7 for theNaval Propulsion dataset.",
        "5553c50d-4c47-4fc4-9b17-39a31cc73111": "145\nChapter 7\nConclusion\nThis thesis expands and advances machine learning research in probabilistic prediction\nand introduces novel methods of producing well-calibrated probabilistic predictions for\nboth machine learning classification and regression problems.\nChapter 1 outlines standard machine learning techniques, introduces the main con-\ncepts in probabilistic prediction and describes probabilistic prediction methods such as\nconformal prediction and Venn prediction. Chapter2 introduces probabilistic machine\nlearning and describes some of the calibration methods, both classical and modern, in-\ncluding recently developed computationally e\ufb00icient algorithms IVAP (inductive Venn\u2013\nAbers predictor) and CVAP (cross Venn\u2013Abers predictor). The classical methods such\nas Platt\u2019s scaling and isotonic regression have disadvantages and do not produce theoret-\nical guarantees of validity of predictions. In addition, modern deep learning computer\nvisionmodelsrelyonconvolution-basedarchitecturesthatresultinoverconfidentpredic-\ntions. Whilst innovations in deep learning continue to maximise predictive accuracy this\nis often at the expense of predictive quality \u2014 whilst accuracy is being maximised, the in-\ncorrect predictions are often accompanied by overconfidence resulting in significant risk\nof wrong decisions, especially in critical applications such as healthcare and self-driving\ncars. Thereisthereforeaclearneedfornon-parametricmachinelearningmethodsthatare\nable to produce well-calibrated class probabilities for classification and valid prediction\nintervals for regression.\nChapter 3 introduces a novel method of probabilistic prediction in multi-class classi-\nfication setting. The multi-class probabilistic prediction problem is solved via dividing it",
        "a3b19cf5-b6ae-4913-861a-3635fa2462b7": "146 Chapter 7. Conclusion\ninto pairwise classification problems, calibrating binary class probabilities and then con-\nverting binary class probabilities into mutli-class probabilities in order to assign each test\nobject to one of thek classes. Such approach allows to compute probabilistic loss metrics\nsuch as the Log loss and the Brier loss and compare them both across various underlying\nmachine learning classification methods, as well across results obtained from applying\nvarious calibration methods. By using IVAP and CVAP to calibrate binary classifiers and\nthencombiningcalibratedbinaryclassificationscorestoobtainwell-calibratedmulti-class\nprobabilities, the resulting multi-class probabilistic classification method results in more\naccurate probabilistic predictions that those obtained from both the underlying machine\nlearning classifiers as well as existing calibration methods such as Platt\u2019s scaling and iso-\ntonic regression applied in multi-class classification setting.\nChapter 4 introduces non-parametric approach to predictive distribution functions\nusing conformal prediction. In statistics, the theory of predictive distributions is based\non the assumption that samples are generated from a parametric model. This chapter\u2019s\nnovel contribution is the non-parametric extension of statistical \u201cprediction confidence\ndistributions\u201d using only limited assumptions customary of machine learning, namely\nthat the observations are generated using the IID model. Unlike in the parametric ap-\nproach of statistical predictive distributions, this non-parametric approach to predictive\ndistributions does not require for the data model to be specified. This approach results\nin predictive distribution functions that are always valid for IID observations in terms of\nguaranteed coverage. The advantage of predictive distribution functions over the usual\nconformal prediction intervals is that conformal predictive distributions contain more in-\nformation - a conformal predictive distributionQn can produce a plethora of prediction\nintervals corresponding to each confidence level1 \u0000 \u03f5.\nChapter 5 extends the study of probabilistic regression and combines conformal pre-\ndictive distributions (CPDs) with kernel methods to derive kernelized versions of the\nalgorithms described in Chapter4. Kernelized versions of conformal predicitive distribu-\ntions are studied theoretically to determine their computational e\ufb00iciency. Experimental\nstudy of the predictive e\ufb00iciency demonstrates important advantages of the kernelized\nversions of CPDs and shows that universal (Laplacian) kernel works remarkably well.",
        "9d35d576-73f8-457b-9b96-67d192cc3a4a": "Chapter 7. Conclusion 147\nChapter6 extends the study of probabilistic regression to address the relative compu-\ntationaline\ufb00iciencyofclassicalconformalpredictors. Twonovelcomputationallye\ufb00icient\nconformal predictive systems are introduced \u2014 split conformal predictive systems (SPC)\nand cross-conformal predictive system (CCPS). Split conformal predictive systems pro-\nvide guaranteed validity, whilst the main advantage of cross-conformal predictive sys-\ntems is greater predictive e\ufb00iciency. For cross-conformal predictive systems validity only\nholds empirically and in the absence of excessive randomization. The main aim of this\nchapter is to define and study computationally e\ufb00icient versions of CPS without any re-\nstrictions related to the underlying algorithm.",
        "ec367c5e-3c33-4979-bd7b-cf68cf63bc31": "148",
        "44756872-f937-4ba6-a126-db8d5e7ebad6": "149\nBibliography\n[1] Anastasios N. Angelopoulos and Stephen Bates.A Gentle Introduction to Conformal\nPrediction and Distribution-Free Uncertainty Quantification. 2021 (Cited on page28).\n[2] Eugene A. Asarin. \u201cSome properties of Kolmogorov\u03c3-Random finite sequences.\u201d\nIn: Theory of Probability & Its Applications32.3 (Jan. 1988), pp. 507\u2013508. DOI:10 .\n1137/1132070 (Cited on page25).\n[3] Thomas Augustin and Frank P. A. Coolen. \u201cNonparametric predictive inference\nandintervalprobability.\u201dIn: Journal of Statistical Planning and Inference124.2(Sept.\n2004), pp. 251\u2013272. DOI:10.1016/j.jspi.2003.07.003 (Cited on page91).\n[4] Anand Avati, Tony Duan, Sharon Zhou, Kenneth Jung, Nigam H. Shah, and An-\ndrew Ng. \u201cCountdown regression: sharp and calibrated survival predictions.\u201d In:\nCoRR abs/1806.08324(June2018).arXiv: 1806.08324 [cs.LG] (Citedonpage 22).\n[5] Miriam Ayer, Daniel Brunk, George Ewing, William Reid, and Edward Silverman.\n\u201cAn empirical distribution function for sampling with incomplete information.\u201d\nIn:The Annals of Mathematical Statistics26.4(Dec.1955),pp.641\u2013647.DOI: 10.1214/\naoms/1177728423 (Cited on pages36, 46).\n[6] Vineeth Balasubramanian, Shen-Shyang Ho, and Vladimir Vovk.Conformal Pre-\ndiction for Reliable Machine Learning: Theory, Adaptations, and Applications.SanFran-\ncisco, CA, USA: Morgan Kaufmann Publishers Inc., 2014. ISBN: 978-0-12-398537-8\n(Cited on pages24, 122).\n[7] R.E.Barlow,D.J.Bartholomew,J.M.Bremner,andH.D.Brunk. Statistical Inference\nunder Order Restrictions; the Theory and Application of Isotonic Regression.Wiley,1972\n(Cited on pages35, 36, 56).",
        "935c5c88-a6b0-48d3-8947-fbce9e8a45d7": "150 Bibliography\n[8] Eric B. Baum and Frank Wilczek. \u201cSupervised learning of probability dis-\ntributions by neural networks.\u201d In: Neural Information Processing Systems .\nNIPS\u201987. Cambridge, MA, USA: American Institute of Physics, 1987, pp. 52\u2013\n61. URL: https : / / proceedings . neurips . cc / paper / 1987 / file /\neccbc87e4b5ce2fe28308fd9f2a7baf3-Paper.pdf (Cited on page43).\n[9] Leo Breiman. \u201cRandom forests.\u201d In:Machine Learning45.1 (Oct. 2001), pp. 5\u201332.\nDOI: 10.1023/A:1010933404324 . URL:https://link.springer.com/article/\n10.1023/A:1010933404324 (Cited on page21).\n[10] Glenn W. Brier. \u201cVerification of forecasts expressed in terms of probability.\u201d In:\nMonthly Weather Review78.1 (1950), pp. 1\u20133 (Cited on pages24, 45).\n[11] Lars Buitinck, Gilles Louppe, Mathieu Blondel, Fabian Pedregosa, Andreas\nMueller, Olivier Grisel, Vlad Niculae, Peter Prettenhofer, Alexandre Gramfort,\nJaques Grobler, Robert Layton, Jake VanderPlas, Arnaud Joly, Brian Holt, and\nGa\u00ebl Varoquaux. \u201cAPI design for machine learning software: experiences from\nthe scikit-learn project.\u201d In:ECML PKDD Workshop: Languages for Data Mining and\nMachine Learning. 2013, pp. 108\u2013122 (Cited on page59).\n[12] Evgeny Burnaev and Vladimir Vovk. \u201cE\ufb00iciency of conformalized ridge regres-\nsion.\u201dIn: Proceedings of The 27th Conference on Learning Theory.Vol.35.Proceedings\nof Machine Learning Research. Barcelona, Spain: PMLR, June 2014, pp. 605\u2013622.\nURL: http://proceedings.mlr.press/v35/burnaev14.pdf (Cited on page85).\n[13] Evgeny Burnaev and Vladimir Vovk. \u201cE\ufb00iciency of conformalized ridge regres-\nsion.\u201dIn: Proceedings of The 27th Conference on Learning Theory.Ed.byMariaFlorina\nBalcan, Vitaly Feldman, and Csaba Szepesv\u00e1ri. Vol. 35. Proceedings of Machine\nLearning Research. 2014, pp. 605\u2013622 (Cited on page86).\n[14] Ben Van Calster and Andrew J. Vickers. \u201cCalibration of risk prediction mod-\nels.\u201d In: Medical Decision Making35.2 (Aug. 2014), pp. 162\u2013169. DOI:10 . 1177 /\n0272989x14547233 (Cited on page30).",
        "ce988917-5eb7-4a31-b611-89258503d7ed": "Bibliography 151\n[15] Lars Carlsson, Martin Eklund, and Ulf Norinder. \u201cAggregated conformal predic-\ntion.\u201d In:Progress in Pattern Recognition, Image Analysis, Computer Vision, and Appli-\ncations. Vol. 437. IFIP Advances in Information and Communication Technology.\nBerlin,Heidelberg:SpringerInternationalPublishing,2014,pp.231\u2013240.ISBN:978-\n3-662-44722-2. DOI:10.1007/978-3-662-44722-2_25 (Cited on page130).\n[16] Rich Caruana and Alexandru Niculescu-Mizil. \u201cAn empirical comparison of su-\npervisedlearningalgorithms.\u201dIn: Proceedings of the 23rd International Conference on\nMachine Learning \u2014 ICML \u201906.Vol.2006.ICML\u201906.Pittsburgh,Pennsylvania,USA:\nAssociation for Computing Machinery, June 2006, pp. 161\u2013168. ISBN: 1595933832.\nDOI: 10.1145/1143844.1143865 (Cited on page50).\n[17] Rich Caruana and Alexandru Niculescu-Mizil. \u201cPredicting good probabilities\nwith supervised learning.\u201d In:Proceedings of the 22nd International Conference on\nMachine Learning. ICML \u201905. New York, NY, USA: Association for Computing Ma-\nchinery, 2005, pp. 625\u2013632. ISBN: 1595931805. DOI:10 . 1145 / 1102351 . 1102430\n(Cited on pages30, 50, 70).\n[18] Samprit Chatterjee and Ali S. Hadi.Sensitivity Analysis in Linear Regression. Wi-\nley Series in Probability and Statistics. John Wiley & Sons, Inc., Mar. 1988. ISBN:\n9780470316764. DOI:10.1002/9780470316764 (Cited on pages81, 83, 84).\n[19] Gemai Chen. \u201cEmpirical processes based on regression residuals: Theory and\napplications.\u201d PhD thesis. Simon Fraser University, Aug. 1991. URL:https : / /\nsummit.sfu.ca/item/4526 (Cited on page94).\n[20] Tianqi Chen and Carlos Guestrin. \u201cXGBoost: A Scalable Tree Boosting System.\u201d\nIn: Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Dis-\ncovery and Data Mining. ACM, 2016, pp. 785\u2013794 (Cited on page59).\n[21] Alexey Chervonenkis.Measures of Complexity : Festschrift for Alexey Chervonenkis.\nEd.byVladimirVovk,HarrisPapadopoulos,andAlexanderGammerman.Cham:\nSpringer International Publishing, 2015. ISBN: 978-3-319-21852-6. DOI:10 . 1007 /\n978-3-319-21852-6 (Cited on page100).",
        "078a27c2-0329-4892-9f7b-dfc8b4699414": "152 Bibliography\n[22] Corinna Cortes and Vladimir Vapnik. \u201cSupport-vector networks.\u201d In:Machine\nLearning 20.3 (Sept. 1995), pp. 273\u2013297. ISSN: 0885-6125. DOI: 10 . 1023 / A :\n1022627411411 (Cited on pages21, 33).\n[23] Thomas Cover and Patrick Hart. \u201cNearest neighbor pattern classification.\u201d In:\nIEEE Transactions on Information Theory13.1(Sept.1967),pp.21\u201327.ISSN:0018-9448.\nDOI: 10.1109/TIT.1967.1053964 (Cited on page21).\n[24] D.R. Cox. \u201cSome problems connected with statistical inference.\u201d In: Annals of\nMathematical Statistics 29.2 (June 1958), pp. 357\u2013372. DOI: 10 . 1214 / aoms /\n1177706618. URL: https : / / projecteuclid . org / euclid . aoms / 1177706618\n(Cited on page104).\n[25] MikhailDashevskiyandZhiyuanLuo.\u201cReliableprobabilisticclassificationandits\napplication to internet tra\ufb00ic.\u201d In:International Conference on Intelligent Computing.\nICIC\u201908.Springer.Shanghai,China:Springer-Verlag,Sept.2008,pp.380\u2013388.ISBN:\n978-3-540-87440-9. DOI:10.1007/978-3-540-87442-3_48 (Cited on page46).\n[26] A. P. Dawid. \u201cThe Well-Calibrated Bayesian.\u201d In:Journal of the American Statisti-\ncal Association 77.379 (Sept. 1982), pp. 605\u2013610. DOI:10 . 1080 / 01621459 . 1982 .\n10477856 (Cited on page23).\n[27] Alexander Philip Dawid. \u201cPresent position and potential developments: some\npersonal views: Statistical theory: the prequential approach.\u201d In:Journal of the\nRoyal Statistical Society.A(General)147.2(Mar.1984),pp.278\u2013292.ISSN:00359238.\nDOI: 10.2307/2981683 (Cited on pages99, 104).\n[28] Alexander Philip Dawid and Vladimir Vovk. \u201cPrequential probability: principles\nand properties.\u201d In:Bernoulli 5.1 (Feb. 1999), pp. 125\u2013162. DOI:10.2307/3318616.\nURL: https : / / projecteuclid . org / download / pdf _ 1 / euclid . bj / 1173707098\n(Cited on pages99, 104).\n[29] Morris H. DeGroot and Stephen E. Fienberg. \u201cThe comparison and evaluation\nof forecasters.\u201d In:Journal of the Royal Statistical Society: Series D (The Statistician)\n32.1/2 (Mar. 1983), pp. 12\u201322. DOI:10.2307/2987588 (Cited on page45).",
        "11e0eae0-f30a-4d46-a7f5-6cb16f8cc45a": "Bibliography 153\n[30] Arthur P. Dempster. \u201cOn direct probabilities.\u201d In:Journal of the Royal Statistical So-\nciety. B (Methodological) 25.1 (1963). ISSN: 00359246. URL:http : / / www . jstor .\norg/stable/2984545 (Cited on page91).\n[31] Monroe Donsker. \u201cJustification and extension of Doobs heuristic approach to\nthe Kolmogorov\u2013Smirnov theorems.\u201d In: Annals of Mathematical Statistics 23.2\n(Nov. 1952), pp. 277\u2013281. DOI: 10 . 1214 / aoms / 1177729445. URL: https : / /\nprojecteuclid.org/euclid.aoms/1177729445 (Cited on page92).\n[32] Anna Veronika Dorogush, Vasily Ershov, and Andrey Gulin. \u201cCatBoost: gradi-\nent boosting with categorical features support.\u201d In:ArXiv abs/1810.11363 (2018)\n(Cited on page59).\n[33] Dheeru Dua and Casey Graff.UCI Machine Learning Repository. http://archive.\nics.uci.edu/ml. University and o, 2017 (Cited on pages38\u201340, 59, 60, 62, 63, 65,\n117, 132).\n[34] Tony Duan, Anand Avati, Daisy Yi Ding, Sanjay Basu, Andrew Y. Ng, and Alejan-\ndroSchuler.\u201cNGBoost:Naturalgradientboostingforprobabilisticprediction.\u201dIn:\nCoRR abs/1910.03225 (Oct. 2019). arXiv:1910.03225 [cs.LG] (Cited on pages23,\n68).\n[35] Bradley Efron. \u201cR. A. Fisher in the 21st century.\u201d In:Statistical Science13.2 (1998),\npp. 95\u2013122. DOI:10 . 1214 / ss / 1028905930. URL:https : / / projecteuclid . org /\ndownload/pdf_1/euclid.ss/1028905930 (Cited on page104).\n[36] TomFawcett.\u201cAnintroductiontoROCanalysis.\u201dIn: Pattern Recognition Letters27.8\n(June 2006), pp. 861\u2013874. ISSN: 0167-8655. DOI:10.1016/j.patrec.2005.10.010\n(Cited on page30).\n[37] Ronald A. Fisher. \u201cConclusions fiduciaires.\u201d In:Annales de l\u2019institut Henri Poincar\u00e9\n10.3 (1948), pp. 191\u2013213. URL:www.numdam.org/item/AIHP_1948__10_3_191_0/\n(Cited on page91).\n[38] Ronald A. Fisher. \u201cStudent.\u201d In:Annals of Eugenics9.1 (Jan. 1939), pp. 1\u20139. DOI:\n10.1111/j.1469- 1809.1939.tb02192.x . URL:https://onlinelibrary.wiley.\ncom/doi/abs/10.1111/j.1469-1809.1939.tb02192.x (Cited on page91).",
        "3bfa2fd4-b0e0-4789-a6a7-73d165d6ab7b": "154 Bibliography\n[39] Valentin Flunkert, David Salinas, and Jan Gasthaus. \u201cDeepAR: probabilistic fore-\ncasting with autoregressive recurrent networks.\u201d In:CoRR abs/1704.04110 (Apr.\n2017). arXiv:1704.04110 [cs.AI] (Cited on page22).\n[40] Tilmann Gneiting and Matthias Katzfuss. \u201cProbabilistic forecasting.\u201d In:Annual\nReview of Statistics and Its Application1.1 (Jan. 2014), pp. 125\u2013151. DOI:10 . 1146 /\nannurev - statistics - 062713 - 085831(Cited on pages21\u201324, 59, 99, 100, 103,\n130).\n[41] Tilmann Gneiting and Adrian E. Raftery. \u201cStrictly proper scoring rules, predic-\ntion,andestimation.\u201dIn: Journal of the American Statistical Association102.477(Mar.\n2007), pp. 359\u2013378. ISSN: 01621459. DOI:10.1198/016214506000001437 (Cited on\npage 117).\n[42] Paul W. Goldberg, Christopher K. I. Williams, and Christopher M. Bishop. \u201cRe-\ngression with Input-Dependent Noise: A Gaussian Process Treatment.\u201d In:Pro-\nceedings of the 10th International Conference on Neural Information Processing Sys-\ntems. Ed. by M. I. Jordan, M. J. Kearns, and S. A. Solla. Vol. 10. MIT Press, Jan.\n1998, pp. 493\u2013499. URL: https : / / papers . nips . cc / paper / 1997 / file /\nafe434653a898da20044041262b3ac74-Paper.pdf (Cited on page112).\n[43] Yael Grushka-Cockayne and Victor Richmond R. Jose. \u201cCombining prediction in-\ntervals in the M4 competition.\u201d In:International Journal of Forecasting36.1 (Jan.\n2020). Part of special issue: M4 Competition, pp. 178\u2013185. ISSN: 0169-2070. DOI:\n10.1016/j.ijforecast.2019.04.015 (Cited on pages30, 31).\n[44] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. \u201cOn calibration\nof modern neural networks.\u201d In:Proceedings of the 34th International Conference\non Machine Learning (PMLR). Vol. 70. Proceedings of Machine Learning Re-\nsearch. Sydney, NSW, Australia: PMLR, Aug. 2017, pp. 1321\u20131330. URL:http :\n/ / proceedings . mlr . press / v70 / guo17a . html(Cited on pages30, 33, 38, 41,\n42, 44, 45, 51, 52, 69, 70).",
        "b7316eff-79f2-43cb-9c66-bc239234daa3": "Bibliography 155\n[45] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. \u201cDeep residual learn-\ning for image recognition.\u201d In:CoRR abs/1512.03385 (Dec. 2015). arXiv:1512 .\n03385 [cs.CV] (Cited on pages30, 51, 69).\n[46] H. V. Henderson and S. R. Searle. \u201cOn deriving the inverse of a sum of matrices.\u201d\nIn:SIAM Review23.1(Jan.1981),pp.53\u201360.ISSN:0036-1445.DOI: 10.1137/1023004\n(Cited on page108).\n[47] Ralph Herbrich. Learning Kernel Classifiers: Theory and Algorithms. Cambridge,\nMass: The MIT Press, Dec. 2001. ISBN: 0-262-08306-X (Cited on page116).\n[48] H. Hersbach. \u201cDecomposition of the Continuous Ranked Probability Score for\nEnsemble Prediction Systems.\u201d In:Weather and Forecasting15 (2000), pp. 559\u2013570\n(Cited on page117).\n[49] BruceM.Hill.\u201cDeFinetti\u2019stheorem,induction,and A(n) orBayesiannonparamet-\nric predictive inference (with discussion).\u201d In:Bayesian Statistics. Ed. by Dennis V.\nLindley, Jos\u00e9 M. Bernardo, Morris H. DeGroot, and Adrian F. M. Smith. Vol. 3.\nOxford: Oxford University Press, 1988, pp. 211\u2013241 (Cited on page91).\n[50] Bruce M. Hill. \u201cPosterior distribution of percentiles: Bayes\u2019 theorem for sampling\nfrom a population.\u201d In:Journal of the American Statistical Association63.322 (June\n1968), pp. 677\u2013691. ISSN: 01621459. DOI:10.2307/2284038 (Cited on pages91, 92).\n[51] Geoffrey Hinton, Oriol Vinyals, and Jeffrey Dean. \u201cDistilling the knowledge in a\nneural network.\u201d In:CoRR (Mar. 2015). arXiv:1503.02531 [stat.ML] (Cited on\npage 41).\n[52] Sepp Hochreiter and J\u00fcrgen Schmidhuber. \u201cLong short-term memory.\u201d In:Neural\nComputation 9 (Dec. 1997), pp. 1735\u20131780. DOI:10 . 1162 / neco . 1997 . 9 . 8 . 1735.\nURL: https : / / www . researchgate . net / publication / 13853244 _ Long _ Short -\nterm_Memory (Cited on page21).",
        "4e5a2ce7-4f45-44db-8bd9-3635e7f7fb0a": "156 Bibliography\n[53] SergeyIoffeandChristianSzegedy.\u201cBatchNormalization:AcceleratingDeepNet-\nwork Training by Reducing Internal Covariate Shift.\u201d In:Proceedings of the 32nd In-\nternational Conference on International Conference on Machine Learning. Vol. 37. Pro-\nceedings of Machine Learning Research. Lille, France: PMLR, July 2015, pp. 448\u2013\n456.URL: http://proceedings.mlr.press/v37/ioffe15.pdf (Citedonpage 41).\n[54] E.T.Jaynes.\u201cInformationtheoryandstatisticalmechanics.\u201dIn: Physical Review106\n(4 May 1957), pp. 620\u2013630. DOI:10.1103/physrev.106.620 (Cited on page41).\n[55] Harold Jeffreys. \u201cOn the theory of errors and least squares.\u201d In:Proceedings of the\nRoyal Society of London. Series A, (Containing Papers of a Mathematical and Phys-\nical Character) 138.834 (Oct. 1932), pp. 48\u201355. ISSN: 09501207. DOI:10.1098/rspa.\n1932 . 0170. URL:https : / / royalsocietypublishing . org / doi / pdf / 10 . 1098 /\nrspa.1932.0170 (Cited on page91).\n[56] Xiaoqian Jiang, Melanie Osl, Jihoon Kim, and Lucila Ohno-Machado. \u201cSmooth\nisotonic regression: a new method to calibrate predictive models.\u201d In:AMIA Joint\nSummits on Translational Science Proceedings. Vol. 2011. American Medical Infor-\nmatics Association. Mar. 2011, pp. 16\u201320 (Cited on pages36, 47).\n[57] Ulf Johansson and Patrick Gabrielsson. \u201cAre Traditional Neural Networks Well-\nCalibrated?\u201d In: 2019, pp. 1\u20138 (Cited on pages30, 38, 50, 51, 70).\n[58] Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qi-\nwei Ye, and Tie-Yan Liu. \u201cLightgbm: A highly e\ufb00icient gradient boosting decision\ntree.\u201d In:Advances in neural information processing systems30 (2017), pp. 3146\u20133154\n(Cited on page59).\n[59] Henry J. Kelley. \u201cGradient Theory of Optimal Flight Paths.\u201d In:ARS Journal30.10\n(Oct. 1960), pp. 947\u2013954. DOI:10.2514/8.5282. URL:https://doi.org/10.2514/\n8.5282 (Cited on page21).\n[60] Alex Kendall and Yarin Gal. \u201cWhat uncertainties do we need in bayesian deep\nlearning for computer vision?\u201d In:31st Conference on Neural Information Processing\nSystems (NIPS 2017). Long Beach, CA, USA: arXiv.org, Mar. 2017, pp. 5580\u20135590\n(Cited on page22).",
        "ee1bd192-fe56-4738-b28c-1caf21276b92": "Bibliography 157\n[61] Frank Knight.Risk, Uncertainty and Profit. Boston, USA: Houghton Mifflin Com-\npany, 1921 (Cited on page103).\n[62] Alex Krizhevsky.Learning multiple layers of features from tiny images. Tech. rep. Uni-\nversity of Toronto, Apr. 2009. URL:https : / / www . cs . toronto . edu / ~kriz /\nlearning-features-2009-TR.pdf (Cited on page38).\n[63] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. \u201cImageNet classification\nwith deep convolutional neural networks.\u201d In:Proceedings of the 25th International\nConference on Neural Information Processing Systems \u2014 Volume 1.AdvancesinNeural\nInformation Processing Systems\u201912 25. Lake Tahoe, Nevada: Curran Associates,\nInc.,Jan.2012,pp.1097\u20131105.DOI: 10.1145/3065386.URL: https://proceedings.\nneurips . cc / paper / 2012 / file / c399862d3b9d6b76c8436e924a68c45b - Paper .\npdf (Cited on pages19, 20, 38).\n[64] Volodymyr Kuleshov, Nathan Fenner, and Stefano Ermon. \u201cAccurate uncertain-\nties for deep learning using calibrated regression.\u201d In:Proceedings of the 35th In-\nternational Conference on Machine Learning. Vol. 80. Proceedings of Machine Learn-\ning Research. Stockholmsm\u00e4ssan, Stockholm Sweden: PMLR, July 2018, pp. 2796\u2013\n2804. URL:http://proceedings.mlr.press/v80/kuleshov18a.html (Cited on\npage 38).\n[65] Meelis Kull, Telmo Silva Filho, and Peter Flach. \u201cBeta calibration: a well-founded\nand easily implemented improvement on logistic calibration for binary classi-\nfiers.\u201d In:Proceedings of the 20th International Conference on Artificial Intelligence and\nStatistics.Vol.54.ProceedingsofMachineLearningResearch.FortLauderdale,FL,\nUSA: PMLR, Apr. 2017, pp. 623\u2013631. URL:http://proceedings.mlr.press/v54/\nkull17a/kull17a.pdf (Cited on pages35\u201338, 50, 51).\n[66] Ananya Kumar, Percy Liang, and Tengyu Ma. \u201cVerified uncertainty calibration.\u201d\nIn: CoRR abs/1909.10155 (Sept. 2019). arXiv: 1909 . 10155 [cs.LG] (Cited on\npages 38, 39).",
        "4f95fdd5-c390-4530-9cc2-499b06457476": "158 Bibliography\n[67] Aviral Kumar, Sunita Sarawagi, and Ujjwal Jain. \u201cTrainable calibration measures\nfor neural networks from kernel mean embeddings.\u201d In:Proceedings of the 35th In-\nternational Conference on Machine Learning. Vol. 80. Proceedings of Machine Learn-\ning Research. Stockholmsm\u00e4ssan, Stockholm Sweden: PMLR, July 2018, pp. 2805\u2013\n2814. URL: http : / / proceedings . mlr . press / v80 / kumar18a . html(Cited on\npages 42, 43).\n[68] Antonis Lambrou, Harris Papadopoulos, Ilia Nouretdinov, and Alexander Gam-\nmerman. \u201cReliable probability estimates based on support vector machines for\nlarge multiclass datasets.\u201d In:IFIP International Conference on Artificial Intelligence\nApplications and Innovations. Vol. 382. Halkidiki, Greece: Springer Berlin Heidel-\nberg, Sept. 2012, pp. 182\u2013191. ISBN: 978-3-642-33411-5. DOI:10.1007/978- 3- 642-\n33412 - 2 _ 19. URL: https : / / hal . archives - ouvertes . fr / hal - 01523046 /\ndocument (Cited on page47).\n[69] Niels Landwehr, Mark Hall, and Eibe Frank. \u201cLogistic model trees.\u201d In:Machine\nLearning 59.1\u20132 (May 2005), pp. 161\u2013205. ISSN: 0885-6125. DOI:10.1007/s10994-\n005-0466-3 (Cited on page39).\n[70] David A. Lane. \u201cFisher, Jeffreys, and the nature of probability.\u201d In:R.A. Fisher:\nAn Appreciation. Ed. by Fienberg S. E. and D. V. Hinkley. New York: Springer New\nYork,1980,pp.148\u2013160.ISBN:978-0-387-90476-4.DOI: 10.1007/978-1-4612-6079-\n0_15 (Cited on page91).\n[71] Nikolay Laptev, Jason Yosinski, Li Erran, and Slawek Smyl. \u201cTime-series extreme\nevent forecasting with neural networks at uber.\u201d In:Time Series Workshop, (ICML\n2017). 34. Sydney, Australia, Aug. 2017, pp. 1\u20135 (Cited on page22).\n[72] SteffenLauritzen. Extremal Families and Systems of Su\ufb00icient Statistics.LectureNotes\nin Statistics. New York: Springer-Verlag, Nov. 1988. ISBN: 0387968725 (Cited on\npage 25).\n[73] Jerry F. Lawless and Marc Fredette. \u201cFrequentist prediction intervals and predic-\ntive distributions.\u201d In:Biometrika 92.3 (Sept. 2005), pp. 529\u2013542. ISSN: 0006-3444.\nDOI: 10.1093/biomet/92.3.529 (Cited on pages72, 73).",
        "6b12af02-7d5d-42ac-88fd-a9f3c3c15c0e": "Bibliography 159\n[74] Quoc Le, Alex Smola, and St\u00e9phane Canu. \u201cHeteroscedastic gaussian process\nregression.\u201d In:Proceedings of the 22nd International Conference on Machine Learn-\ning. ICML \u201905. New York, NY, USA: Association for Computing Machinery,\n2005, pp. 489\u2013496. ISBN: 1595931805. DOI:10 . 1145 / 1102351 . 1102413(Cited on\npage 112).\n[75] Tim Leathart, Eibe Frank, Geoffrey Holmes, and Bernhard Pfahringer. \u201cProba-\nbility calibration trees.\u201d In:CoRR abs/1808.00111 (July 2018). arXiv:1808.00111\n[cs.LG] (Cited on page39).\n[76] Tim Leathart, Eibe Frank, Bernhard Pfahringer, and Geoffrey Holmes. \u201cOn cali-\nbration of nested dichotomies.\u201d In:Advances in Knowledge Discovery and Data Min-\ning. Macau, China: Springer International Publishing, Apr. 2019, pp. 69\u201380. DOI:\n10 . 1007 / 978 - 3 - 030 - 16148 - 4 _ 6. URL:https : / / researchcommons . waikato .\nac . nz / bitstream / handle / 10289 / 12887 / on - calibration - of - nested -\ndichotomies.pdf (Cited on pages37, 47).\n[77] Yann LeCun, Bernhard Boser, John S. Denker, Donnie Henderson, Richard E.\nHoward, Wayne Hubbard, and Lawrence D. Jackel. \u201cBackpropagation applied\nto handwritten zip code recognition.\u201d In: Neural Computation 1.4 (Dec. 1989),\npp. 541\u2013551. ISSN: 0899-7667. DOI:10 . 1162 / neco . 1989 . 1 . 4 . 541(Cited on\npages 30, 69).\n[78] Yann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner. \u201cGradient-based\nlearning applied to document recognition.\u201d In:Proceedings of the IEEE. Vol. 86. 11.\nTorino,Italy,Nov.1998,pp.2278\u20132324.DOI: 10.1109/5.726791 (Citedonpages 19,\n20).\n[79] Jing Lei and Larry Wasserman. \u201cDistribution-free prediction bands for non-\nparametric regression.\u201d In:Journal of the Royal Statistical Society: Series B (Statistical\nMethodology) 76.1 (July 2013), pp. 71\u201396. DOI:https://doi.org/10.1111/rssb.\n12021. URL:https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/rssb.\n12021 (Cited on page86).",
        "aa0e0ea4-81b6-47d2-8030-f7d5fa82a752": "160 Bibliography\n[80] Tsung-Yi Lin, Priya Goyal, Ross B. Girshick, Kaiming He, and Piotr Doll\u00e1r. \u201cFo-\ncal loss for dense object detection.\u201d In:CoRR abs/1708.02002 (Aug. 2017). arXiv:\n1708.02002 [cs.CV] (Cited on page44).\n[81] Henrik Linusson, Ulf Norinder, Henrik Bostr\u00f6m, Ulf Johansson, and Tuve L\u00f6f-\nstr\u00f6m. \u201cOn the calibration of aggregated conformal predictors.\u201d In:Proceedings\nof the Sixth Workshop on Conformal and Probabilistic Prediction and Applications. Ed.\nby Alex Gammerman, Vladimir Vovk, Zhiyuan Luo, and Harris Papadopou-\nlos. Vol. 60. Proceedings of Machine Learning Research. Stockholm, Sweden:\nPMLR, June 2017, pp. 154\u2013173. URL:http : / / proceedings . mlr . press / v60 /\nlinusson17a.html (Cited on pages129, 130, 138).\n[82] Spyros Makridakis, Evangelos Spiliotis, and Vassilios Assimakopoulos. \u201cThe M4\ncompetition: 100, 000 time series and 61 forecasting methods.\u201d In:International\nJournal of Forecasting36.1 (Jan. 2020), pp. 54\u201374. ISSN: 0169-2070. DOI:10 . 1016 /\nj.ijforecast.2019.04.014 . URL:https://www.sciencedirect.com/science/\narticle/pii/S0169207019301128 (Cited on pages22, 31).\n[83] Spyros Makridakis, Evangelos Spiliotis, and Vassilis Assimakopoulos. \u201cThe M5\nAccuracy competition: Results, findings and conclusions.\u201d In: (Oct. 2020) (Cited\non pages22, 23).\n[84] Valery Manokhin. Awesome Conformal Prediction. 2022. DOI: 10 . 5281 / ZENODO .\n6467205. URL:https://zenodo.org/record/6467205 (Cited on page27).\n[85] Valery Manokhin. \u201cMulti-class probabilistic classification using inductive and\ncross Venn-Abers predictors.\u201d In:Proceedings of the Sixth Workshop on Conformal\nand Probabilistic Prediction and Applications.Vol.60.ProceedingsofMachineLearn-\ning Research. Stockholm, Sweden: PMLR, June 2017, pp. 228\u2013240. URL:http :\n/ / proceedings . mlr . press / v60 / manokhin17a / manokhin17a . pdf(Cited on\npage 26).\n[86] Valery Manokhin.valeman/Multi-class-probabilistic-classification: v0.1.0. 2022. DOI:\n10.5281/ZENODO.6685149 . URL:https://zenodo.org/record/6685149 (Cited\non page26).",
        "6211f9f4-8067-4bd5-ad0a-661afcb25516": "Bibliography 161\n[87] Peter McCullagh, Vladimir Vovk, Ilia Nouretdinov, Dmitry Devetyarov, and Alex\nGammerman.\u201cConditionalpredictionintervalsforlinearregression.\u201dIn: 2009 In-\nternational Conference on Machine Learning and Applications. Miami, FL, USA: IEEE,\nDec. 2009, pp. 131\u2013138. DOI:10.1109/icmla.2009.115 (Cited on page104).\n[88] Thomas Melluish, Craig Saunders, Ilia Nouretdinov, and Volodya Vovk. \u201cCom-\nparing the bayes and typicalness frameworks.\u201d In:Machine Learning: ECML 2001.\nBerlin, Heidelberg: Springer Berlin Heidelberg, Oct. 2001, pp. 360\u2013371. ISBN: 978-\n3-540-42536-6. DOI: 10 . 1007 / 3 - 540 - 44795 - 4 _ 31 . URL: https : / / www .\nresearchgate . net / publication / 2393200 _ Comparing _ the _ Bayes _ and _\nTypicalness_Frameworks/link/09e4150cee445659a0000000/download (Cited\non pages30, 31, 100).\n[89] Soundouss Messoudi, Sylvain Rousseau, and S\u00e9bastien Destercke. \u201cDeep confor-\nmal prediction for robust models.\u201d In:Information Processing and Management of\nUncertainty in Knowledge-Based Systems. Ed. by Marie-Jeanne Lesot, Susana Vieira,\nMarek Z. Reformat, Jo\u00e3o Paulo Carvalho, Anna Wilbik, Bernadette Bouchon-\nMeunier, and Ronald R. Yager. Springer, Cham, June 2020, pp. 528\u2013540. ISBN: 978-\n3-030-50146-4. DOI:https://doi.org/10.1007/978- 3- 030- 50146- 4_39(Cited\non pages30, 31, 51, 69).\n[90] DonaldMichie,DavidSpiegelhalter,andCharlesC.Taylor. Machine Learning, Neu-\nral and Statistical Classification. London: Overseas Press, 2009. ISBN: 8188689734\n(Cited on page59).\n[91] Marvin Minsky and Seymour Papert.Perceptrons. An Introduction to Computational\nGeometry. Cambridge, Mass: MIT Press, Jan. 1969. ISBN: 9780262130431 (Cited on\npage 21).\n[92] Tom Mitchell.Machine Learning. Singapore: McGraw-Hill Education, Mar. 1997.\nISBN: 0070428077 (Cited on page19).\n[93] Mohammed Mohammadi. \u201cOn the bounds for diagonal and off-diagonal ele-\nments of the hat matrix in the linear regression model.\u201d In:REVSTAT \u2014 Statistical",
        "7d1130de-6b53-4429-81c0-c046744aa61c": "162 Bibliography\nJournal 14.1 (Feb. 2016), pp. 75\u201387. URL:https : / / www . ine . pt / revstat / pdf /\nrs160104.pdf (Cited on page83).\n[94] Jishnu Mukhoti, Viveka Kulharia, Amartya Sanyal, Stuart Golodetz, Philip H. S.\nTorr, and Puneet K. Dokania. \u201cCalibrating deep neural networks using focal loss.\u201d\nIn: (Feb. 2020). arXiv:2002.09437 [cs.LG] (Cited on pages29\u201331, 38, 44, 45, 51,\n69, 70).\n[95] Rafael M\u00fcller, Simon Kornblith, and Geoffrey E. Hinton. \u201cWhen Does Label\nSmoothing Help?\u201d In: CoRR abs/1906.02629 (June 2019). arXiv: 1906 . 02629\n[cs.LG] (Cited on page43).\n[96] AllanH.Murphy.\u201cTheearlyhistoryofprobabilityforecasts:someextensionsand\nclarifications.\u201d In:Weather and Forecasting13.1 (Mar. 1998), pp. 5\u201315. ISSN: 0882-\n8156. DOI:10.1175/1520- 0434(1998)013<0005:TEHOPF> 2.0.CO;2 . URL:https:\n//journals.ametsoc.org/view/journals/wefo/13/1/1520- 0434_1998_013_\n0005_tehopf_2_0_co_2.xml (Cited on page21).\n[97] Kevin P. Murphy.Probabilistic Machine Learning: An introduction. MIT Press, 2022.\nURL: probml.ai (Cited on page27).\n[98] Mahdi Pakdaman Naeini and Gregory F. Cooper. \u201cBinary classifier calibration\nusing an ensemble of near isotonic regression models.\u201d In:2016 IEEE 16th Interna-\ntional Conference on Data Mining (ICDM). Vol. 2016. Los Alamitos, CA, USA: IEEE\nComputer Society, Dec. 2016, pp. 360\u2013369. DOI:10 . 1109 / ICDM . 2016 . 0047. URL:\nhttps://doi.ieeecomputersociety.org/10.1109/ICDM.2016.0047 (Cited on\npages 40, 47).\n[99] Mahdi Pakdaman Naeini, Gregory F. Cooper, and Milos Hauskrecht. \u201cObtaining\nAccurate Probabilities Using Classifier Calibration.\u201d In:Proceedings of the Twenty-\nNinth AAAI Conference on Artificial Intelligence. Association for the Advancement\nof Artificial Intelligence (AAAI), 2015, pp. 2901\u20132907 (Cited on pages33, 40).\n[100] Mahdi Pakdaman Naeini, Gregory F. Cooper, and Milos Hauskrecht. \u201cObtain-\ning well calibrated probabilities using bayesian binning.\u201d In:Proceedings of the\nTwenty-ninth AAAI Conference on Artificial Intelligence. AAAI\u201915. Austin, Texas:",
        "5ce666d6-f48e-4335-ba1d-6a06be2b5c8d": "Bibliography 163\nAAAI Press, Apr. 2015, pp. 2901\u20132907. ISBN: 0262511290. URL:https://ojs.aaai.\norg/index.php/AAAI/article/view/9602/9461 (Cited on pages39\u201341).\n[101] Ilia Nouretdinov, Denis Volkhonskiy, Pitt Lim, Paolo Toccaceli, and Alexander\nGammerman. \u201cInductive Venn-Abers predictive distribution.\u201d In:Proceedings of\nthe Seventh Workshop on Conformal and Probabilistic Prediction and Applications. Ed.\nby Alex Gammerman, Vladimir Vovk, Zhiyuan Luo, Evgueni Smirnov, and Ralf\nPeeters. Vol. 91. Proceedings of Machine Learning Research. PMLR, June 2018,\npp. 15\u201336. URL: http : / / proceedings . mlr . press / v91 / nouretdinov18a /\nnouretdinov18a.pdf (Cited on pages27, 122).\n[102] YanivOvadia,EmilyFertig,JieRen,ZacharyNado,D.Sculley,SebastianNowozin,\nJoshuaV.Dillon,BalajiLakshminarayanan,andJasperSnoek.\u201cCanyoutrustyour\nmodel\u2019s uncertainty? evaluating predictive uncertainty under dataset shift.\u201d In:\n33rd Conference on Neural Information Processing Systems (NeurIPS 2019). Vancou-\nver, Canada, Dec. 2019. URL:https : / / papers . nips . cc / paper / 2019 / file /\n8558cb408c1d76621371888657d2eb1d-Paper.pdf (Cited on pages41, 51).\n[103] HarrisPapadopoulos.\u201cReliableprobabilisticclassificationwithneuralnetworks.\u201d\nIn:Neurocomputing 107 (May 2013). Part of special issue: Timely Neural Networks\nApplications in Engineering: Selected Papers from the 12th EANN International\nConference, 2011, pp. 59\u201368. ISSN: 0925-2312. DOI:https://doi.org/10.1016/j.\nneucom.2012.07.034 (Cited on pages32, 46).\n[104] Harris Papadopoulos. \u201cReliable probabilistic prediction for medical decision sup-\nport.\u201d In: IFIP Advances in Information and Communication Technology. Ed. by\nLazaros Iliadis, Ilias Maglogiannis, and Harris Papadopoulos. Berlin, Heidelberg:\nSpringer Berlin Heidelberg, 2011, pp. 265\u2013274. ISBN: 978-3-642-23960-1. DOI:10 .\n1007/978-3-642-23960-1_32 (Cited on page32).\n[105] Telma Pereira, Sandra Cardoso, Manuela Guerreiro, Alexandre Mendon\u00e7a, and\nSara C. Madeira. \u201cTargeting the uncertainty of predictions at patient-level using\nan ensemble of classifiers coupled with calibration methods, Venn-ABERS, and\nconformal predictors: a case study in AD.\u201d In:Journal of Biomedical Informatics101",
        "f869c32c-0c28-46c6-a031-22a5f087b471": "164 Bibliography\n(Jan. 2020), p. 103350. ISSN: 1532-0464. DOI:10.1016/j.jbi.2019.103350 (Cited\non pages22, 23, 29, 30, 32, 35, 50\u201352, 69, 70).\n[106] Gabriel Pereyra, George Tucker, Jan Chorowski, \u0141ukasz Kaiser, and Geoffrey E.\nHinton. \u201cRegularizing neural networks by penalizing confident output distribu-\ntions.\u201dIn: CoRR abs/1701.06548(Jan.2017).arXiv: 1701.06548 [cs.NE] (Citedon\npages 41\u201343, 45).\n[107] Donald A. Pierce and Kenneth J. Kopecky. \u201cTesting goodness of fit for the distri-\nbution of errors in regression models.\u201d In:Biometrika 66.1 (Apr. 1979), pp. 1\u20135.\nISSN: 0006-3444. DOI:10.1093/biomet/66.1.1 (Cited on page92).\n[108] Iosif Pinelis. \u201cExact bounds on the closeness between the student and standard\nnormal distributions.\u201d In:ESAIM: Probability and Statistics19 (Mar. 2015), pp. 24\u2013\n27. DOI:10.1051/ps/2014014 . URL:https://www.esaim- ps.org/articles/ps/\npdf/2015/01/ps140014.pdf (Cited on page88).\n[109] JohnC.Platt.\u201cProbabilisticoutputsforsupportvectormachinesandcomparisons\nto regularized likelihood methods.\u201d In:Advances in Large Margin Classifiers. Ed. by\nAlexander J. Smola and Peter Bartlett. Vol. 10. 3. MIT Press, Mar. 1999, pp. 61\u201374\n(Cited on pages33\u201335, 38, 50, 59).\n[110] T. Poggio. \u201cOn optimal nonlinear associative recall.\u201d In:Biological Cybernetics19.4\n(Sept. 1975), pp. 201\u2013209. DOI:10.1007/bf02281970 (Cited on page116).\n[111] David Price, Stefan Knerr, L\u00e9on Personnaz, and G\u00e9rard Dreyfus. \u201cPairwise neural\nnetwork classifiers with probabilistic outputs.\u201d In:Advances in Neural Information\nProcessing Systems. Ed. by G. Tesauro, D. Touretzky, and T. Leen. Vol. 7. MIT Press,\n1994,pp.1109\u20131116.URL: https://proceedings.neurips.cc/paper/1994/file/\n210f760a89db30aa72ca258a3483cc7f-Paper.pdf (Cited on page54).\n[112] Yaniv Romano, Evan Patterson, and Emmanuel J. Cand\u00e8s. \u201cConformalized Quan-\ntile Regression.\u201d In: (2019). arXiv:1905.03222 [stat.ME] (Cited on page22).\n[113] Frank Rosenblatt. \u201cThe perceptron: a probabilistic model for information storage\nand organization in the brain.\u201d In:Psychological Review65.6 (1958), pp. 386\u2013408.\nDOI: 10.1037/h0042519 (Cited on page21).",
        "8a496717-17d4-445d-8ece-6550d5318817": "Bibliography 165\n[114] DavidE.Rumelhart,GeoffreyE.Hinton,andRonaldJ.Williams.\u201cLearningrepre-\nsentations by back-propagating errors.\u201d In:Nature 323.6088 (Oct. 1986), pp. 533\u2013\n536. ISSN: 1476-4687. DOI:10.1038/323533a0 (Cited on pages19, 21, 43).\n[115] Stuart Russell and Peter Norvig.Artificial Intelligence: A Modern Approach. 3rd. Up-\nper Saddle River, New Jersey: Pearson, 2013. ISBN: 0136042597 (Cited on page21).\n[116] Jarkko Saloj\u00e4rvi, Kai Puolam\u00e4ki, Jaana Simola, Lea Kovanen, Ilpo Kojo, and\nSamuelKaski.\u201cInferringRelevancefromEyeMovements:FeatureExtraction.\u201dIn:\n2005 (Cited on page66).\n[117] Tore Schweder and Nils Lid Hjort.Confidence, Likelihood, Probability: Statistical In-\nference with Confidence Distributions. New York, NY: Cambridge University Press,\nFeb. 2016. ISBN: 9780521861601 (Cited on pages71, 95, 104).\n[118] George A. F. Seber and Alan J. Lee.Linear Regression Analysis. Wiley Series in\nProbability and Statistics. Wiley, Jan. 2003. ISBN: 9780471415404. DOI:10 . 1002 /\n9780471722199 (Cited on page88).\n[119] TeddySeidenfeld.\u201cJeffreys,Fisher,andKeynes:PredictingtheThirdObservation,\nGiventheFirstTwo.\u201dIn: History of Political Economy27.5(Jan.1995),pp.39\u201352.DOI:\n10.1215/00182702-27-supplement-39 (Cited on page91).\n[120] Glenn Shafer and Vladimir Vovk. \u201cA tutorial on conformal prediction.\u201d In:CoRR\nabs/0706.3188 (June 2007) (Cited on page25).\n[121] JieliShen,ReginaY.Liu,andMin-geXie.\u201cPredictionwithConfidence\u2014aGeneral\nFramework for Predictive Inference.\u201d In:Journal of Statistical Planning and Inference\n195 (May 2018), pp. 126\u2013140. ISSN: 0378-3758. DOI:10.1016/j.jspi.2017.09.012\n(Cited on pages71, 73, 95, 104).\n[122] Albert N. Shiryaev.\u0412\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u044c (Probability). Vol. 95. Graduate Texts in Math-\nematics. New York: Springer-Verlag, 1989. DOI:10 . 1007 / 978 - 1 - 4757 - 2539 - 1\n(Cited on page102).\n[123] Edward Snelson and Zoubin Ghahramani. \u201cVariable noise and dimensionality re-\nduction for sparse gaussian processes.\u201d In:Proceedings of the Twenty-Second Confer-\nence on Uncertainty in Artificial Intelligence. Ed. by R. Dechter and T. S. Richardson.",
        "0f60c0ec-17a2-428d-a2fe-4ff821a21022": "166 Bibliography\nUAI\u201906.Arlington,Virginia,USA:AUAIPress,2006,pp.461\u2013468.ISBN:0974903922\n(Cited on page112).\n[124] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan\nSalakhutdinov. \u201cDropout: a simple way to prevent neural networks from overfit-\nting.\u201d In:Journal of Machine Learning Research15.1 (Jan. 2014), pp. 1929\u20131958. ISSN:\n1532-4435. URL: https : / / dl . acm . org / doi / pdf / 10 . 5555 / 2627435 . 2670313\n(Cited on page41).\n[125] Ingo Steinwart. \u201cOn the influence of the kernel on the consistency of support vec-\ntor machines.\u201d In:Journal of Machine Learning Research2 (Dec. 2001), pp. 67\u201393.\nISSN: 1532-4435. DOI:10 . 1162 / 153244302760185252. URL:https : / / www . jmlr .\norg/papers/volume2/steinwart01a/steinwart01a.pdf (Cited on page113).\n[126] Ingo Steinwart, Don Hush, and Clint Scovel. \u201cAn explicit description of the repro-\nducingkernelHilbertspacesofGaussianRBFkernels.\u201dIn: IEEE Transactions on In-\nformation Theory52.10(Oct.2006),pp.4635\u20134643.DOI: 10.1109/TIT.2006.881713\n(Cited on page116).\n[127] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbig-\nniew Wojna. \u201cRethinking the inception architecture for computer vision.\u201d In:\nCoRR abs/1512.00567(Dec.2015).arXiv: 1512.00567 [cs.CV] (Citedonpages 42,\n43).\n[128] Christine Thomas-Agnan. \u201cComputing a family of reproducing kernels for sta-\ntistical applications.\u201d In:Numerical Algorithms 13.1 (Mar. 1996), pp. 21\u201332. DOI:\n10.1007/bf02143124 (Cited on pages113, 114).\n[129] Paolo Toccaceli, Ilia Nouretdinov, and Alexander Gammerman. \u201cConformal pre-\ndictors for compound activity prediction.\u201d In:Conformal and Probabilistic Predic-\ntion with Applications (COPA 2016).Vol. 9653. Lecture Notes in Computer Science\n(LNCS). Springer International Publishing, Apr. 2016, pp. 51\u201366. ISBN: 978-3-319-\n33394-6. DOI:10.1007/978-3-319-33395-3_4 (Cited on page22).\n[130] Alan M Turing. \u201cComputing machinery and intelligence.\u201d In:Mind LIX.236 (Oct.\n1950), pp. 433\u2013460. ISSN: 0026-4423. DOI:10.1093/mind/LIX.236.433. URL:https:",
        "57c4ad01-e4c8-4436-bc79-ee5d6ade6dfe": "Bibliography 167\n//academic.oup.com/mind/article- pdf/LIX/236/433/30123314/lix- 236-\n433.pdf (Cited on page21).\n[131] Stanford University.Honesty in statistical models. https://news.stanford.edu/\n2021/03/19/honesty- statistical-models/. Online; Accessed 20/4/2021. Mar.\n2021 (Cited on pages22, 49).\n[132] JoaquinVanschoren,JanN.vanRijn,BerndBischl,andLuisTorgo.\u201cOpenML:net-\nworkedscienceinmachinelearning.\u201dIn: SIGKDD Explorations15.2(2013),pp.49\u2013\n60 (Cited on pages59, 60, 67).\n[133] V.Vapnik.\u201cPatternrecognitionusinggeneralizedportraitmethod.\u201dIn: Automation\nand Remote Control24 (1963), pp. 774\u2013780 (Cited on page21).\n[134] Vladimir N. Vapnik.Statistical learning theory. Vol. 1. New York: Wiley, Sept. 1998.\nISBN: 978-0-471-03003-4 (Cited on page35).\n[135] Vladimir Vovk. \u201cCross-conformal predictors.\u201d In:Annals of Mathematics and Artifi-\ncial Intelligence74 (July 2013), pp. 9\u201328. DOI:10.1007/s10472-013-9368-4 (Cited\non pages129, 138).\n[136] Vladimir Vovk. \u201cUniversally consistent predictive distributions.\u201d In: CoRR\nabs/1708.01902(Aug.2017).arXiv: 1708.01902 [cs.LG] (Citedonpages 104,112,\n130).\n[137] Vladimir Vovk, Alexander Gammerman, and Glenn Shafer.Algorithmic Learning\nin a Random World. Berlin: Springer-Verlag, Mar. 2005. ISBN: 0387001522. DOI:10 .\n1007/b106715 (Cited on pages25,32,46,55,77,80,86,103,108,112,116,117,122,\n123).\n[138] Vladimir Vovk, Ilia Nouretdinov, and Alex Gammerman. \u201cOn-line predictive lin-\near regression.\u201d In:The Annals of Statistics37.3 (June 2009), pp. 1566\u20131590. DOI:\n10.1214/08-AOS622 (Cited on page101).\n[139] Vladimir Vovk, Ilia Nouretdinov, Valery Manokhin, and Alex Gammerman.\n\u201cConformal predictive distributions with kernels.\u201d In:Braverman Readings in Ma-\nchine Learning. Key Ideas from Inception to Current State. Springer International Pub-\nlishing,Apr.2018,pp.103\u2013121.ISBN:978-3-319-99491-8.DOI: 10.1007/978-3-319-",
        "16a4c7a7-162d-46b3-84cd-7c29dc4fd31b": "168 Bibliography\n99492 - 5 _ 4. URL:https : / / www . researchgate . net / publication / 327171113 _\nConformal _ Predictive _ Distributions _ with _ Kernels _ International _\nConference _ Commemorating _ the _ 40th _ Anniversary _ of _ Emmanuil _\nBraverman ' s _ Decease _ Boston _ MA _ USA _ April _ 28 - 30 _ 2017 _ Invited _ Talks\n(Cited on pages27, 100, 103, 104, 106, 115, 118, 122).\n[140] Vladimir Vovk, Ilia Nouretdinov, Valery Manokhin, and Alexander Gammerman.\n\u201cCross-conformal predictive distributions.\u201d In: Proceedings of the Seventh Work-\nshop on Conformal and Probabilistic Prediction and Applications. Vol. 91. Proceed-\nings of Machine Learning Research. PMLR, June 2018, pp. 37\u201351. URL:http : / /\nproceedings . mlr . press / v91 / vovk18a / vovk18a . pdf(Cited on pages27, 130,\n135, 136, 138, 140).\n[141] VladimirVovk,IvanPetej,andValentinaFedorova.\u201cLarge-scaleprobabilisticpre-\ndictors with and without guarantees of validity.\u201d In:Advances in Neural Informa-\ntion Processing Systems\u2014NIPS 2015. Vol. 28. Curran Associates, Inc., Nov. 2015,\npp. 892\u2013900. URL: https : / / proceedings . neurips . cc / paper / 2015 / file /\na9a1d5317a33ae8cef33961c34144f84- Paper.pdf (Cited on pages52, 55\u201359, 70,\n134).\n[142] Vladimir Vovk, Ivan Petej, Ilia Nouretdinov, Valery Manokhin, and Alex J. Gam-\nmerman. \u201cComputationally e\ufb00icient versions of conformal predictive distribu-\ntions.\u201d In:CoRR abs/1911.00941 (Nov. 2019). arXiv:1911.00941 [cs.LG] (Cited\non page27).\n[143] Vladimir Vovk, Ivan Petej, Ilia Nouretdinov, Valery Manokhin, and Alexander\nGammerman. \u201cComputationally e\ufb00icient versions of conformal predictive distri-\nbutions.\u201d In: Neurocomputing 397 (July 2020), pp. 292\u2013308. ISSN: 0925-2312. DOI:\nhttps://doi.org/10.1016/j.neucom.2019.10.110 (Cited on pages27,77,133).\n[144] Vladimir Vovk, Glenn Shafer, and Ilia Nouretdinov. \u201cSelf-calibrating probabil-\nity forecasting.\u201d In:Proceedings of the 16th International Conference on Neural Infor-\nmation Processing Systems. Vol. 16. NIPS\u201903. Whistler, British Columbia, Canada:\nMIT Press, Dec. 2003, pp. 1133\u20131140. URL:https: // proceedings .neurips . cc/",
        "d7f9faee-b2da-4058-bc83-89e08738b44f": "Bibliography 169\npaper/2003/file/10c66082c124f8afe3df4886f5e516e0- Paper.pdf (Cited on\npages 46, 55).\n[145] VladimirVovk,JieliShen,ValeryManokhin,andMin-geXie.\u201cNonparametricpre-\ndictive distributions based on conformal prediction.\u201d In:Proceedings of the Sixth\nWorkshop on Conformal and Probabilistic Prediction and Applications. Ed. by Alex\nGammerman, Vladimir Vovk, Zhiyuan Luo, and Harris Papadopoulos. Vol. 60.\nProceedings of Machine Learning Research. Stockholm, Sweden: PMLR, June\n2017, pp. 82\u2013102. DOI:10.1007/s10994-018-5755-8 . URL:http://proceedings.\nmlr.press/v60/vovk17a/vovk17a.pdf (Cited on pages26, 100, 104, 114, 118).\n[146] VladimirVovk,JieliShen,ValeryManokhin,andMin-geXie.\u201cNonparametricpre-\ndictive distributions based on conformal prediction.\u201d In:Machine Learning 108.3\n(Aug. 2018), pp. 445\u2013474. DOI:10.1007/s10994-018-5755-8 (Cited on pages26,\n75, 86, 89, 95).\n[147] Vladimir Vovk and Ruodu Wang. \u201cCombining p-values via averaging.\u201d In: (Dec.\n2012). arXiv:1212.4966 [math.ST] (Cited on page130).\n[148] C. M. Wang, Jan Hannig, and Hari K. Iyer. \u201cFiducial prediction intervals.\u201d In:\nJournal of Statistical Planning and Inference142.7 (July 2012), pp. 1980\u20131990. DOI:\n10 . 1016 / j . jspi . 2012 . 02 . 021. URL: https : / / www . researchgate . net /\npublication/257199342_Fiducial_prediction_interval (Cited on page88).\n[149] Larry Wasserman. \u201cFrasian inference.\u201d In: Statistical Science 26.3 (Oct. 2011),\npp. 322\u2013325. ISSN: 0883-4237. DOI: 10 . 1214 / 11 - sts352c . URL: https : / /\nprojecteuclid.org/euclid.ss/1320066921 (Cited on page112).\n[150] JasonWestonandChrisWatkins.\u201cSupportvectormachinesformulti-classpattern\nrecognition.\u201d In:European Symposium on Artificial Neural Network \u2014 ESANN\u20191999\nProceedings. Vol. 99. Bruges, Belgium, Apr. 1999, pp. 219\u2013224. URL:https://www.\nelen . ucl . ac . be / Proceedings / esann / esannpdf / es1999 - 461 . pdf(Cited on\npage 53).\n[151] Ronald J. Williams and Jing Peng. \u201cFunction optimization using connectionist re-\ninforcement learning algorithms.\u201d In:Connection Science3.3 (Sept. 1991), pp. 241\u2013",
        "282c2886-0dbc-455b-b20f-330e1c74113e": "170 Bibliography\n268. DOI: 10 . 1080 / 09540099108946587. URL: https : / / www . researchgate .\nnet / publication / 2703232 _ Function _ Optimization _ Using _ Connectionist _\nReinforcement_Learning_Algorithms (Cited on page42).\n[152] Bianca Zadrozny and Charles Elkan. \u201cObtaining calibrated probability estimates\nfrom decision trees and naive bayesian classifiers.\u201d In:Proceedings of the Eighteenth\nInternational Conference on Machine Learning. Vol. 1. ICML \u201901. San Francisco, CA,\nUSA:MorganKaufmannPublishersInc.,May2001,pp.609\u2013616.ISBN:1558607781.\nURL: https: / / cseweb. ucsd . edu/ ~elkan / calibrated .pdf(Cited on pages33,\n38, 51, 55).\n[153] Bianca Zadrozny and Charles Elkan. \u201cTransforming classifier scores into accu-\nrate multiclass probability estimates.\u201d In:Proceedings of the Eighth ACM SIGKDD\nInternational Conference on Knowledge Discovery and Data Mining. KDD \u201902. Ed-\nmonton, Alberta, Canada: Association for Computing Machinery, Aug. 2002,\npp. 694\u2013699. ISBN: 158113567X. DOI:10 . 1145 / 775047 . 775151. URL: https : / /\nwww . researchgate . net / publication / 2571315 _ Transforming _ Classifier _\nScores _ into _ Accurate _ Multiclass _ Probability _ Estimates (Cited on\npages 35, 36, 47, 55, 59).\n[154] JianZhangandYimingYang.\u201cProbabilisticscoreestimationwithpiecewiselogis-\ntic regression.\u201d In:Proceedings of the Twenty-first International Conference on Machine\nLearning \u2014(ICML 2004). Ed. by Carla E. Brodley. Vol. 69. ICML \u201904. Banff, Al-\nberta, Canada: Association for Computing Machinery (ACM), July 2004, pp. 115\u2013\n123. ISBN: 1581138385. DOI:10.1145/1015330.1015335 (Cited on page35).\n[155] Chenzhe Zhou, Ilia Nouretdinov, Zhiyuan Luo, Dmitry Adamskiy, Luke Ran-\ndell, Nick Coldham, and Alex Gammerman. \u201cA comparison of Venn Machine\nwith Platt\u2019s method in probabilistic outputs.\u201d In: 12th Engineering Applications\nof Neural Networks (EANN 2011) and 7th Artificial Intelligence Applications and In-\nnovations (AIAI). Vol. 364. Springer. Berlin, Heidelberg, Sept. 2011, pp. 483\u2013490.\nISBN: 978-3-642-23960-1. DOI:10 . 1007 / 978 - 3 - 642 - 23960 - 1 _ 56. URL: https :\n/ / www . researchgate . net / publication / 220827947 _ A _ Comparison _ of _",
        "5ab4e846-861e-4e27-9609-a5396afb58dc": "Bibliography 171\nVenn _ Machine _ with _ Platt ' s _ Method _ in _ Probabilistic _ Outputs(Cited\non page46).\nView publication stats",
        "2a7ac2ba-6537-401d-96e7-44a9b08df02c": "Prediction Intervals for Model Averaging\u2217\nZhongjun Qu\u2020 Wendun Wang\u2021 Xiaomeng Zhang\u00a7\nOctober 21, 2025\nAbstract\nA rich set of frequentist model averaging methods has been developed, but their ap-\nplications have largely been limited to point prediction, as measuring prediction uncer-\ntainty in general settings remains an open problem. In this paper we propose prediction\nintervals for model averaging based on conformal inference. These intervals cover out-of-\nsample realizations of the outcome variable with a pre-specified probability, providing a\nway to assess predictive uncertainty beyond point prediction. The framework allows gen-\neral model misspecification and applies to averaging across multiple models that can be\nnested, disjoint, overlapping, or any combination thereof, with weights that may depend\non the estimation sample. We establish coverage guarantees under two sets of assump-\ntions: exact finite-sample validity under exchangeability, relevant for cross-sectional data,\nand asymptotic validity under stationarity, relevant for time-series data. We first present\na benchmark algorithm and then introduce a locally adaptive refinement and split-sample\nprocedures that broaden applicability. The methods are illustrated with a cross-sectional\napplication to real estate appraisal and a time-series application to equity premium fore-\ncasting.\nKeywords: model averaging, prediction uncertainty, conformal inference, prediction in-\nterval, model misspecification\n\u2217First version: October 6, 2025. We thank Denis Tkachenko and seminar participants at the National\nUniversity of Singapore for their helpful comments and suggestions.\n\u2020Corresponding author. Department of Economics, Boston University; qu@bu.edu.\n\u2021Econometric Institute, Erasmus University Rotterdam; Tinbergen Institute.\n\u00a7Econometric Institute, Erasmus University Rotterdam; Tinbergen Institute; zhang@ese.eur.nl.\narXiv:2510.16224v1  [econ.EM]  17 Oct 2025",
        "9b0deb89-5040-4dde-83b1-b2fd95a0a49d": "1 Introduction\nPredictive modeling in the presence of model uncertainty is a central problem in econometrics\nand statistics. A large literature, spanning Bayesian and frequentist approaches, has developed\nmodel averaging methods to improve prediction performance when no single model is known\nto be the best choice. Instead of selecting a single model, these procedures combine predictions\nfrom a set of candidate models, typically with weights determined by in-sample fit, predictive\nperformance, or prior beliefs. Bayesian Model Averaging (BMA, Raftery et al. (1997), Hoeting\net al. (1999)); frequentist methods such as the smoothed information criterion (SIC, Buckland\net al. (1997)), Mallows Model Averaging (MMA, Hansen (2007)), and Jackknife Model Aver-\naging (JMA, Hansen and Racine (2012)); as well as more recent ensemble approaches inspired\nby machine learning (e.g., Breiman (2001), Hastie et al. (2009, Chapter 8)), are all examples of\nmethods that aim to improve predictive performance by combining information across models\nand reducing the risk of relying on a single misspecified model. This paper focuses on frequen-\ntist averaging methods, but the proposed algorithms can also be applied to BMA and ensemble\napproaches. Forecast combination is treated as a special case of model averaging.\nDespite the breadth of the model averaging literature, existing work has focused primar-\nily on point prediction. The construction of prediction intervals, which quantify uncertainty\naround point predictions, has received much less attention. While Bayesian model averaging\nproduces credible intervals by integrating over model uncertainty (Raftery et al. (1997)), the\nfrequentist coverage validity of such intervals generally requires correct model specification. To\nour knowledge, there is no published procedure for constructing valid prediction intervals for\nfrequentist model averaging methods, even when averaging weights are fixed and independent\nof the sample. In short, the field has developed methods for estimation, but not for inference.\nThis paper aims to address that gap. We face two issues in obtaining valid interval pre-\ndictions in a general model averaging setting. First, since all models may be misspecified, the\npoint prediction may not converge to its true value even asymptotically. The limit generally\ndepends on both the data-generating process and the models included in the set. One way to\naddress this issue is to assume it away by postulating that the true model is contained in the\nset. However, this runs counter to the idea of model averaging, which is based on combining\nmultiple approximating models for prediction. In empirical work, it is also common for more\nthan one model to receive nontrivial weights. The second issue is technical: when model weights\nare determined endogenously, they depend on both model misspecification and the estimation\nsample, making it even less clear how to construct a valid prediction interval.\nWe propose general-purpose prediction intervals for model averaging using conformal infer-\n1",
        "3e648bd9-4a5e-432d-a76e-5a27bd53b11b": "ence framework of Vovk et al. (2005). We do not require any of the candidate models to be\ncorrectly specified and accommodate a wide range of model combinations, including nested,\ndisjoint, overlapping, or any combination thereof, with weights that may depend on the sample\nin flexible ways. We establish two forms of validity: (i) finite-sample coverage under exchange-\nability, relevant for cross-sectional settings, and (ii) asymptotic coverage under stationarity and\nweak dependence, relevant for time series forecasting. In both cases, the coverage guarantees\nare distribution-free (that is, they do not rely on knowing or estimating the distribution of the\ndata-generating process) and the intervals adapt automatically to the chosen model averaging\nscheme without further modification.\nIn particular, we build on the general conformal inference framework of Vovk et al. (2005)\nand draw on more recent developments in Lei et al. (2018) for regression settings. While\nconformal methods have been widely studied in machine learning for constructing distribution-\nfree prediction intervals, they have not, to our knowledge, been adapted to frequentist model\naveraging in any published work to date. Our results fill this gap by extending conformal\ninference to frequentist model averaging methods, allowing for both model misspecification and\ndependence in the data.\nWe begin by proposing a basic algorithm that directly adapts the standard conformal pre-\ndiction interval, such as Algorithm 1 in Lei et al. (2018) for regression, to the model averaging\ncontext. Although the adaptation is straightforward, its applications are broad, and we show\nthat the resulting method achieves valid coverage in both exchangeable and time-series settings.\nThe procedure is general-purpose in that it applies to a wide range of model averaging schemes.\nIn particular, we consider equal-weight combinations, the regression approach of Granger and\nRamanathan (1984), SIC of Buckland et al. (1997), MMA of Hansen (2007), and JMA of\nHansen and Racine (2012), all of which are examined in our Monte Carlo simulations and\nempirical applications. We then extend this basic algorithm to incorporate local adaptivity,\nallowing the intervals to respond to heterogeneity or serial dependence in the data and poten-\ntially improve finite-sample performance. The main advantage of adaptive intervals is that they\nwiden when variance is high and narrow when variance is low, a feature especially relevant for\nfinancial applications. Finally, we propose split-sample versions of both procedures, which are\ncomputationally more efficient, though they may sacrifice some statistical efficiency relative to\nthe full-sample method. We show that this approach also enables us to encompass Bayesian\nmodel averaging and ensemble methods within our framework. All algorithms apply to both\nexchangeable and time-series settings.\nWe show that, for exchangeable data, the conditions for coverage validity are mild, as they\nmainly rely on a symmetry condition under which the estimates and model weights are invariant\n2",
        "2afadfd0-faa8-4185-8b04-66a36e639539": "to reshuffling of the data. This holds because the proposed algorithms are designed to preserve\nexchangeability. Factors such as the specific method for estimating model weights, model\ndimension, model misspecification, the number of candidate models, or whether the models are\nnested do not affect validity, as none of them alters exchangeability or symmetry. For time\nseries observations, we do not require exchangeability; instead, we rely on the same symmetry\ncondition together with stationarity, ergodicity, and the stability of the model weights and\nparameter estimates. We verify these conditions for linear models under each model averaging\nmethod mentioned above. As a by-product, we also establish the convergence rates of the model\naveraging weights in the linear time-series setting, which may be of independent interest.\nWe examine the finite-sample performance of the proposed methods using both exchangeable\nand time-series data-generating processes. For the exchangeable case, we adopt the DGP of\nHansen (2007), extended to include heteroskedasticity. The DGP is an infinite-order regression,\nso all candidate models are misspecified. The total sample size is small, with 150 observations;\nin the split-sample case, 75 observations are used for training, and averaging is conducted\nover 16 models with 2 to 17 coefficients. We find that coverage is close to the nominal level\nacross all methods. Split-sample intervals are only slightly longer than full-sample intervals\non average, while adaptive methods produce wider intervals when a conservative approach is\nused to estimate conditional variance. For the time-series case, we design an experiment that\nmimics the empirical problem of equity premium prediction with many potential predictors,\nusing parameter values calibrated to empirical estimates. We find that the algorithms deliver\ncoverage close to the nominal level with 100 observations, and coverage improves further when\nthe sample size is increased to 200. In this setting, split-sample intervals are again only slightly\nlonger than full-sample intervals on average; however, adaptive methods do not produce much\nwider intervals, reflecting the fact that fewer additional parameters are estimated here than in\nthe exchangeable setting.\nWe illustrate the proposed algorithms with a cross-sectional application to real estate ap-\npraisal and a time-series application to equity premium prediction. To our knowledge, this is\namong the first empirical examinations of interval prediction in the context of model averaging.\nReal estate appraisal involves estimating property values using covariates such as location,\nhouse age, and proximity to facilities. Model averaging is a natural choice when the covariates\nor the functional form are uncertain. In this case, prediction intervals are especially valuable:\nfor example, a point prediction of 1 million with a 95% interval of [0.9, 1.1] has very different\nimplications than [0.5, 1.5], both as collateral and for potential sellers in the market. We use\nthe dataset of Yeh and Hsu (2018), which consists of 414 housing price observations in Taipei.\nWe consider both nested and non-nested models, a total of 63 models in all, and use a leave-\n3",
        "bb15a396-71ca-4cf8-9930-2c5926af7a1d": "one-out procedure to evaluate coverage. The results show that all methods achieve adequate\ncoverage. Additionally, the weighting favors estimated weights over equal weights, as larger\nmodels generate more accurate forecasts.\nWe then turn to a time-series application, where we use 20 variables from Goyal et al. (2024)\nas potential predictors of the equity premium. Following common practice, each variable enters\na bivariate regression, leading to 20 non-nested models for averaging. In this case, we find that\nusing equal weights produces more accurate forecasts than estimating weights. At the same\ntime, all methods deliver coverage close to the nominal level despite the small sample size.\nThe rest of the paper is organized as follows. Section 2 introduces the issue of interest:\nmeasuring prediction uncertainty under model averaging. Section 3 proposes an algorithm for\nconstructing interval predictions for general model averaging schemes, adapting Algorithm 1 of\nLei et al. (2018) to the model averaging setting. Section 4 establishes formal coverage guar-\nantees under two sets of assumptions: exact finite-sample validity under exchangeability and\nasymptotic validity under stationarity. It also discusses applications to a range of model averag-\ning schemes. Section 5 proposes extensions of the basic algorithm to capture heteroskedasticity\nor serial correlation in the data, while Section 6 provides further extensions based on sample\nsplitting, which also includes applications to BMA and ensemble methods. Section 7 presents\nMonte Carlo experiments to examine coverage properties in finite samples. Section 8 considers\ntwo empirical applications, and Section 9 concludes.\n2 The issue of interest\nThe objective of the paper is to measure prediction uncertainty under model averaging. The\nsetting is general and is as follows. We consider averaging overMcandidate models, indexed\nbym= 1, . . . , M, where themth model is given by\nyi =f (m)(xi;\u03b8 m) +e i, i= 1, . . . , n,\nwithndenoting the sample size;y i is a scalar random variable, andx i is a random vector.\nDifferent models typically contain different explanatory variables (covariates), andx i de-\nnotes the union of all variables that appear in at least one model. These models may be disjoint\n(sharing no common covariates), nested (where one model is a special case of another), or over-\nlapping (sharing some but not all covariates). The functional forms can differ and may be linear\nor nonlinear. None of them is assumed to encompass the true data-generating process. Each\nmodelacts as ifE(e i|xi) = 0, so thatf (m)(xi;\u03b8 m) represents the mean-square optimal predic-\ntion under modelm. This condition need not hold for the true data-generating mechanism,\nallowing all models to be misspecified. The dimension of\u03b8 m generally differs across models.\n4",
        "ac6c4452-105a-4c89-8280-f72a0a89e674": "Although we useito index observations, the proposed algorithms also apply to time series\nregressions. In that context,y i andx i are interpreted asy t andx t\u22121, respectively, and themth\nestimated model is given by\nyt =f (m)(xt\u22121;\u03b8 m) +e t, t= 1, . . . , T.\nAll statements in the previous paragraph apply to the time series case.\nPoint prediction with frequentist model averaging typically proceeds as follows. Using the\ndata sample, each model is estimated and used to generate a predicted value fory n+1 at a new\ncovariate vectorx n+1, given by\nf(m)(xn+1; b\u03b8m),form= 1, . . . , M.(1)\nThis new covariate vectorx n+1 may represent predictors observed at the end of a time window\nin a time-series application or predictors for a new observation in a cross-sectional setting. With\nmodel weights bw= (bw1, . . . ,bwM )\u22a4, the model-averaged prediction atx n+1 is\nb\u00b5n+1 =\nMX\nm=1\nbwmf(m)(xn+1; b\u03b8m).(2)\nThe weights bwmay be fixed (e.g., equal weights) or data-driven; the choice of weights will be\ndiscussed later in the paper. The proposed algorithms allow for both cases. We assume all\nweights are finite and do not assume they must sum up to one. These procedures yield point\npredictions. Bayesian model averaging will be treated separately in Subsection 6.1.\nThe issue of interest, which is also the paper\u2019s main contribution, is to construct valid\nprediction intervals that measure prediction uncertainty at any user-specified significance level\n\u03b1\u2208(0,1). Specifically, we construct an interval that contains the true out-of-sample value of\nywhenx=x n+1, denotedy n+1, with the following coverage property:\nP{y n+1 \u2208 C(xn+1)} \u22651\u2212\u03b1,\nwhich holds in finite samples when the observations are exchangeable (with i.i.d. as a special\ncase) and holds asymptotically when the data are continuous, stationary, and ergodic time\nseries. The coverage probability converges to the nominal level (1\u2212\u03b1) in both cases for\ncontinuous outcomes and is therefore non-conservative asymptotically.\n3 A basic algorithm for interval prediction\nWe need to address two issues in obtaining a valid interval prediction in a general model averag-\ning setting. First, since all models may be misspecified, the point prediction may not converge\n5",
        "eecd6a9d-ffe0-4d40-bb3b-9195c1e1c5da": "to its actual value even asymptotically. The limit generally depends on both the data generat-\ning process and the models allowed in the model set. One way to address this issue is to assume\naway the problem; that is, to assume that the model set contains the true model. However, this\nassumption runs counter to the spirit of model averaging, which rests on the idea that multiple\napproximating models should be combined for prediction. Moreover, in empirical applications,\nit is common for more than one model to receive nontrivial weights. Obtaining valid prediction\nintervals in the presence of general model misspecification has remained an unsolved problem.\nThe second challenge is technical in nature: when model weights are determined endogenously,\nthey depend on both model misspecification and the estimation sample, making it even less\nclear how to construct a valid prediction interval.\nWe use the conformal inference framework of Vovk et al. (2005) to construct a prediction\ninterval. A key insight from conformal inference, which we carry into our setting, is that a\nplausible value for the unobservedy n+1 should be one that conforms to the observed data\nsample. In particular, when we attempt to predicty n+1, it should yield a prediction error that\nis consistent with the distribution of historical prediction errors, which are computable from\nthe data and model. Specifically, given a candidate valueyfory n+1, we compute the implied\nprediction errorben+1 =y\u2212b\u00b5 n+1 as in (2), and compare it to the empirical distribution of in-\nsample prediction errorsbei =y i \u2212b\u00b5i, fori= 1, . . . , n. Ifben+1 is implausibly large relative to the\nhistorical errors at a chosen coverage level, theny n+1 =yis considered a implausible value and\nis excluded from the prediction interval; otherwise,yis included in the interval. The collection\nof such values forms a valid prediction interval at the chosen coverage level. This powerful\nidea has generated a large and expanding literature for conducting distributional inference on\npredictions, and in this paper we adapt it to frequentist model averaging and prove its validity\nin general settings.\nThis approach does not require any model to be correctly specified, as it relies on the notion\nof conformity rather than approximations around a true model. A key contribution of this paper\nis to show that it applies to both cross-sectional and time-series model averaging. The method\nremains valid as long as prediction errors, both in-sample and for the new observation, can be\ntreated symmetrically. This symmetry condition underlies the conformal inference framework\nand is made precise for model averaging in the algorithm and assumptions presented below.\nOur proposed algorithm involves parameter and model weights estimation using augmented\nsamples and can be described as follows. Recall that the observed sample is given by{(xi, yi)}n\ni=1,\nwhereidenotes either a cross-sectional unit or a time index. Letybe a candidate value for\nyn+1. Then an augmented sample is given by{(x i, yi)}n\ni=1 \u222a {(xn+1, y)}. For each model, we\nuse the augmented sample to estimate\u03b8 m to obtain b\u03b8m, where the estimate methods can differ\n6",
        "a7ed3125-d137-4b24-83f1-60064dcc68eb": "across models, for example, using MLE for some models, while (nonlinear) least squares for\nothers. Similarly, the model weights are also determined based on the augmented sample; the\nspecific steps depend on the model averaging method and are described in Section 4.3. Next,\ncompute the point predictions and associated prediction errors as, respectively,\nA:b\u00b5 y,i =\nMX\nm=1\nbwmf(m)(xi; b\u03b8m) fori= 1, . . . , n, n+ 1,(3)\nand\nbey,i =y i \u2212b\u00b5y,i, i= 1, . . . , n,bey,n+1 =y\u2212b\u00b5 y,n+1.(4)\nAfter that, evaluate whether the value ofbey,n+1 is a plausible value in relation to the historical\nrealizationsbey,i fori= 1, ...n, using comformal inference, and keep or reject the corresponding\nyvalue depending on the outcome of the comparsion. Finally, these operations are repeated\nfor other candidate values ofy n+1 to obtain a set of predictions that form a prediction interval.\nThe key difference of this procedure from the standard procedure for obtaining point pre-\ndictions, as in (1), is the involvement of the candidate valuey. This ensures that future and\nhistorical values are treated symmetrically. Changing the value ofygenerally affects both the\npredictionb\u00b5y,i and the residualbey,i for alli= 1, . . . , n+ 1, which is reflected in the notation\nthat both variables are indexed byy. Formally, we propose the following algorithm for both\ncross-sectional and time series data:\n7",
        "75cb1099-e5d4-4548-919c-f2e4b85af25c": "Algorithm 1Prediction Interval for Model Averaging\nInput:Data{(x i, yi)}n\ni=1; coverage level (1\u2212\u03b1)\u2208(0,1); model averaging algorithmA; new ob-\nservationx n+1 at which to compute the prediction; valuesY trial ={y \u2217\n1, y\u2217\n2, . . .}for potential\noutcomey n+1.\nOutput:A prediction interval atx n+1, namelyC(x n+1).\n1:fory\u2208 Y trial do\n2:Using the augmented sample{(x i, yi)}n\ni=1 \u222a {(xn+1, y)}, estimate the parameters and\nmodel weights to construct the model averaging prediction ruleAas in Equation (3).\n3:Calculate conformity scores:\nRy,i =|y i \u2212b\u00b5y,i|fori= 1, . . . , n,andR y,n+1 =|y\u2212b\u00b5 y,n+1|.\n4:Computep-value ofR y,n+1:\n\u03c0(y) = 1 + Pn\ni=1 I(Ry,i \u2265R y,n+1)\nn+ 1 .\n5:end for\n6:return\nC(xn+1) =\n\b\ny\u2208 Ytrial :\u03c0(y)> \u03b1\n\t\n.\nModel averaging enters the algorithm through the first part of the for loop, which involves\nparameter estimation, model weight estimation, and the calculation of model predictions. The\nsubsequent steps (i.e., parts 3 and 4) are identical to those in standard conformal inference.\nIn particular, Algorithm 1 in Lei et al. (2018) contains parts 3 and 4 for general regression\nprediction problems. Thus, this algorithm can be viewed as a direct adaptation of conformal\ninference to the model averaging setting.\nParts 4 and 6 evaluate thep-value of the new prediction residual relative to the empirical\ndistribution of all residuals. Because a smallp-value indicates that the candidateyis implausi-\nble, the algorithm includesyin the prediction set only if itsp-value is strictly greater than\u03b1; see\nVovk et al. (2005) for discussions of this point. This is equivalent to comparing the new residual\nto the empirical (1\u2212\u03b1) quantile with conservative tie handling:yis included ifR y,n+1 \u2264R y,(k),\nwhereR y,(k) is thek-th smallest residual of all residuals andk=\u2308(1\u2212\u03b1)(n+1)\u2309. No estimation\nof nuisance parameters is required.\nThe model weights are calculated using the augmented sample: this is important to ensure\nthat the new observation (x n+1, y) affects the estimation in the same way as any existing\nobservation in the sample. This symmetry implies that the properties of the new prediction\nresidual should behave like the other residuals, thus providing the basis for conformity. These\n8",
        "9dd0414d-0d6a-4888-843f-41eb9c97ac88": "weights need to be recomputed for each candidate value ofyn+1. Because these computations are\nindependent across values and therefore fully parallelizable, we conjecture that the computation\nis feasible for many applications in economics and finance. Nevertheless, to accommodate\napplications where this computation turns out to be too costly, we will in Section 6 provide\nalgorithms that involve sample splitting, which computes the model weights only once for the\nentire procedure.\nThe algorithm is generic in that it applies to a wide range of model averaging methods.\nDifferences across methods affect only the choice of weights, and hence the calculation of pre-\ndiction residuals. We will show that the algorithm remains asymptotically valid for time series\ndata without further modification.\n4 Coverage validity of the algorithm\nIn this section, we establish coverage validity under two sets of assumptions: exact finite-sample\nvalidity under exchangeability (with i.i.d. data as a special case), relevant for cross-sectional\nsettings, and asymptotic validity under stationarity, relevant for time-series predictions.\n4.1 Finite-sample validity under exchangeability\nThe assumptions we need are standard assumptions in the conformal prediction literature and\nare nonrestrictive in the context of model averaging.\nAssumption 1The random vectors(x 1, y1), . . . ,(xn, yn),(x n+1, yn+1)are exchangeable.\nNote that a sequence of random vectors is exchangeable if its joint distribution is invariant\nunder permutations of the indices. In other words, the ordering of the observations does not\nmatter. Formally, a sequence (V 1, V2, . . . , Vn+1) is exchangeable if, for any permutation\u03c0of\n{1,2, . . . , n+ 1}, (V1, V2, . . . , Vn+1)\nd\n= (V \u03c0(1), V\u03c0(2), . . . , V\u03c0(n+1)),where\nd\n= denotes equality in\ndistribution. Every i.i.d. sequence is exchangeable. Conditional heteroskedasticity is permitted:\nthe variance ofyi givenx i may differ across observations. Exchangeability is uncommon in time-\nseries settings, since temporal ordering typically matters and permuting time indices alters the\ndependence structure. A notable exception is equicorrelated Gaussian vectors, whose symmetry\nmakes the joint distribution permutation invariant. This assumption will not be required for\ntime-series model averaging.\nFor any augmented sample{(x i, yi)}n\ni=1 \u222a {(xn+1, y)}in Algorithm 1, let\nb\u03b8= ( b\u03b81, b\u03b82, . . . ,b\u03b8M )\n9",
        "39ee05a2-f679-49ba-beb6-475b3cd85dca": "denote the collection of parameter estimates from the individuals models, and let\nbw= (bw1,bw2, . . . ,bwM )\ndenote the estimated model weights. Their dependence onyis omitted for simplicity.\nAssumption 2The parameter estimates b\u03b8and model weights bware invariant to the permu-\ntation of the augmented sample{(x i, yi)}n\ni=1 \u222a {(xn+1, y)}.\nIn this assumption, the elements of each vector (x i, yi) are permuted together as a set.\nThe assumption ensures that reshuffling the sample does not affect the estimates. As a result,\nthe fitted residuals as in (4) inherit the exchangeability of the original data. Most estimation\nmethods for cross-sectional data satisfy this condition. In particular, weighted least squares\nsatisfies it as long as weights are given by the same function applied to each observation\u2019s own\ndata, which is typically the case. Time-series multiple regression, wherey i is regressed onx i via\nOLS, also satisfies this assumption. A counterexample is an estimation procedure that assigns\nzero weight to the observation (x n+1, y) only, meaning that this observation is excluded from\nthe estimation. In that case, the new observation is treated differently from the others, and\nits prediction residual may have different properties. As a result, the residuals are no longer\nexchangeable. This assumption is not tied to any particular estimation method or specific\nmodel averaging scheme. It allows users to apply the procedure with any method, including\nnewly proposed ones, by simply verifying whether the assumption holds in their specific setting.\nUnder these two assumptions, Algorithm 1 provides correct coverage in finite samples. It\napplies to models with either discrete or continuous outcomes, in each case allowing for discrete\nand continuous explanatory variables.\nTheorem 1Suppose Assumptions 1 and 2 hold. Then\nP{y n+1 \u2208 C(xn+1)} \u22651\u2212\u03b1.\nIf, in addition, for ally\u2208Rthe fitted absolute residuals{R y,i}n+1\ni=1 have a continuous joint\ndistribution, then it also holds that\nP{y n+1 \u2208 C(xn+1)} \u22641\u2212\u03b1+ 1\nn+ 1 .\nThe first inequality reflects a standard property of conformal inference; see Vovk et al.\n(2005). The second is established in Theorem 1 of Lei et al. (2018) for regression settings. Both\nresults hold here because Algorithm 1 is constructed to ensure that prediction residuals under\nmodel averaging are exchangeable. A proof is provided in the appendix, focusing on how model\n10",
        "fbd40123-f2f1-4964-8e18-f19ebcb000f9": "averaging interacts with exchangeability. Taken together, the two inequalities imply that the\nproposed prediction interval is essentially exact: it never undercovers and its overcoverage does\nnot exceed 1/(n+ 1) for any sample size. In practice, coverage may deviate slightly from these\nbounds because implementation requires searching over a finite grid, though the difference is\nminor and can be reduced by further refining the grid.\nSince coverage validity requires only the exchangeability of the data and the symmetry of the\nestimation algorithm, it holds for both fixed and data-dependent model weights. The difference\nis that fixed and data-dependent weights lead to different distributions forR y,i; as a result, the\ncorresponding prediction intervals are generally different. Similarly, model misspecification, the\nnumber of candidate models, and whether the models are nested do not affect coverage validity,\nas none of these alter either exchangeability or symmetry.\n4.2 Asymptotic validity under stationarity\nFor time series model averaging, instead of exchangeability, we assume that the data are strict\nstationary and ergodic.\nAssumption 3The random vectors(x 1, y1), . . . ,(xn, yn),(x n+1, yn+1)are stationary and er-\ngodic.\nRecall that a process (x i, yi) is stationary if the joint distribution of (x i1, yi1), . . . ,(xik, yik) for\nanyk >0 is invariant to shifts in the index. That is, for allh,\n(xi1, yi1), . . . ,(xik, yik)\nd\n= (xi1+h, yi1+h), . . . ,(xik+h, yik+h).\nThis strict stationarity assumption generally cannot be weakened to weak stationarity (i.e.,\ntime-invariant means, variances, and covariances), because the objective here is to cover a\nrandom variable, not a fixed parameter, with a specified probability. Even without model av-\neraging, for a linear model given byy i =x \u2032\ni\u03b2+e i, with known\u03b2, it is still not possible to\nconstruct a valid prediction interval fory n+1 under weak stationarity alone, since the higher\norder moments ofy n+1 may differ arbitrarily from those of past observations. No method can\nguarantee coverage if the underlying distribution is allowed to evolve freely over time. See also\nBarber et al. (2023, pp. 828\u2013829) for a discussion of this point. Meanwhile, we conjecture that\nthis assumption may be relaxed to allow for restricted distributional changes with appropriate\nmodifications to the algorithm. Tibshirani et al. (2019) show that a weighted version of confor-\nmal inference can be used to construct distribution-free prediction intervals when the covariate\ndistributions (i.e., ofx) differ between the training and test data, provided the likelihood ra-\ntio between the two distributions is known. Barber et al. (2023) use weighted quantiles to\n11",
        "5b151b1b-c291-471c-a5e4-2d3368a7e129": "introduce robustness to distributional drift and derive bounds on the coverage gap under fixed\nweighting schemes. The latter results quantify how coverage deteriorates under distributional\nshifts, though they do not prescribe how to select the weights. We leave the adaptation of such\nmethods to model averaging for further work.\nThe ergodicity condition in Assumption 3 implies that a single long realization of the pro-\ncess is representative of its stationary distribution. This is often a minimal requirement for\nconducting inference in stationary time series settings. It covers weakly dependent processes,\nincluding mixing and near-epoch dependence, and allows for conditional variation in all mo-\nments, including conditional heteroskedasticity.\nThe next assumption requires that b\u03b8and bwconverge in probability to some pseudo-true\nvalues, denoted by\u03b8 0 = (\u03b8 0,1, . . . ,\u03b80,M ) andw 0 = (w 0,1, . . . ,w0,M ), respectively, where all\ncomponents are finite constants. These pseudo-true values generally depend on both the model\nset and the data-generating process.\nAssumption 4Asn\u2192 \u221e, b\u03b8\np\n\u2212 \u2192\u03b80 and bw\np\n\u2212 \u2192w0 for some finite\u03b8 0 andw 0 independent ofn.\nThis assumption implies that all models included in the averaging are finite-dimensional;\nthat is, their dimensions do not diverge with the sample size. The first part of the assumption,\nb\u03b8\np\n\u2212 \u2192\u03b80, concerns the convergence of parameters in individual candidate models, which is\nindependent of model weights and not specific to model averaging. Conditions for ensuring this\nhas been extensively studied for both linear and nonlinear models; see, for example, White\u2019s\nwork on asymptotic analysis under model misspecification in White (1982, 1984, 1994). These\nconditions are not restrictive.\nThe second part of the assumption, bw\np\n\u2212 \u2192w0 concerns the asymptotic behavior of model\nweights, which depends on the model set and the averaging scheme. Zhang et al. (2020)\nprove weight convergence when averaging linear models with i.i.d. data and when at least one\nmodel in the candidate set encompasses the data-generating process. Their weights minimize\na generalized Mallows-type criterion, with a penalty term that diverges slowly to infinity. In\naddition, if all models are misspecified, their Theorem 3 implies that the estimated weights\nconverge in probability to the minimizer of the asymptotic risk function, provided that this\nlimiting criterion has a unique minimizer. 1 Meanwhile, Hjort and Claeskens (2003) study\nthe limiting distribution of model averaging estimators when all candidate models lie in a\nlocal neighborhood of the data-generating process. They show, among other results, that\n1Their proof on p. 982 shows thatS \u2217\nn(w)/Rn(w) converges uniformly to 1 over a compact set ofw, or\nequivalently, thatS \u2217\nn(w)/ndivided byR n(w)/nconverges uniformly to 1. IfR n(w)/nconverges uniformly to\nR(w) andR(w) has a unique minimizer, then the convergence of the estimated weights follows.\n12",
        "5fb32acd-24fa-4dfd-bc66-153002580c27": "the AIC weights remain random in the limit, partly because AIC is not a consistent model\nselection criterion. Therefore, there are practical situations where this assumption holds and\nalso important cases where it fails. To accommodate the possibility that the weights may not\nconverge, we will also introduce an alternative assumption to Assumption 4, which is stated\nas Assumption 7 later in this section. In addition, we will further examine this assumption\nunder more primitive time series conditions when explicitly considering various model averaging\nschemes in Subsection 4.3.\nWe also need to impose regularity conditions on model residuals in a local neighborhood of\nthe pseudo-true value specified in Assumption 4, as is typical in asymptotic analysis. For any\ngiven parameter value and model weights, and anyi\u2208 {1, . . . , n, n+ 1}, define\ne\u00b5i(w,\u03b8) =\nMX\nm=1\nwmf(m)(xi;\u03b8 m),\neei(w,\u03b8) =y i \u2212e\u00b5i(w,\u03b8).\nAssumption 5The random variableee i(w0,\u03b8 0)is integrable and has a densityfthat is uni-\nformly bounded in an open neighborhood of its(1\u2212\u03b1)quantiles: there exists \u00aff <\u221esuch that\nf(u)\u2264 \u00afffor allunears. In addition, its distribution function is strictly increasing ats.\nAssumption 6LetB(\u03b8 0)andB(w 0)denote open neighborhoods of\u03b8 0 andw 0, respectively,\nthat are independent ofn. Assume thatee i(w,\u03b8)is continuously differentiable inB(w 0)\u00d7B(\u03b8 0)\nwith probability one. For eachi, define\nZi := sup\nw\u2208B(w0),\u03b8\u2208B(\u03b80)\n\r\r\r\r\n\u2202\n\u2202(w,\u03b8) eei(w,\u03b8)\n\r\r\r\r.\nAssume either:\n(i)E[Z i]<\u221e, and the conditional density ofee i(w0,\u03b8 0)givenZ i is finite; or\n(ii) A sub-Weibull tail bound holds: there exist constantsC 0 >0,a >0, andk >1such that\nfor alli,\nP(Zi > t)\u2264C 0 exp(\u2212atk),\nand the estimators b\u03b8and bwconverge to\u03b8 0 andw 0 at rate(logn) \u22122/k or faster.\nAssumption 5 is needed to ensure that the empirical distribution ofee i(w0,\u03b8 0) converges\nuniformly to its population cumulative distribution function in a neighborhood of the (1\u2212\u03b1)\nquantile. Assumption 6, similar to Assumption 3 in Andrews (2003), ensures that the quantile\nfunction ofeei(w,\u03b8) approach that ofeei(w0,\u03b8 0) aswand\u03b8converge tow 0 and\u03b8 0, respectively.\n13",
        "714f7f83-96d5-4d25-9786-35bd9ea6a1a7": "Its part (i) requires that the conditional density of the residual is finite, without imposing any\ncondition on the convergence rate of the estimators. Alternatively, part (ii) places no restriction\non the density but restricts the tails of the gradient and requires a mild convergence rate of\nthe parameter and weight estimates. It accommodates fat-tailed distributions. As shown in\nthe appendix, this condition can be further relaxed to allow polynomial tails, provided the\nconvergence rate is faster than some threshold below the root-nrate. We will further this\nassumption when considering various model averaging schemes in Subsection 4.3.\nWe now present an assumption to accommodate situations where the estimated model\nweights may not converge to fixed values. It is essentially a stability condition which requires\nthat the sequence of fitted values from model averaging concentrate around a sequence of sta-\ntionary and ergodic random variables, so that the resulting prediction errors are approximately\nstationary and ergodic. It is analogous to the stability condition in (Lei et al., 2018, Assumption\nA2), adapted to a time series model averaging setting.\nAssumption 7There exists a sequence of stationary and ergodic variablese\u00b5 i independent of\nn, such thaty i admits the decomposition:y i =e\u00b5i +eei,whereeei satisfies Assumption 5, and the\nfitted values after model averaging are stable arounde\u00b5 i:\n|e\u00b5i( bw, b\u03b8)\u2212e\u00b5i| \u2264Zi\u03f5n,\nwhereZ i is a random variable that satisfies a sub-Weibull tail bound: there exist constantsC 0 >\n0,a >0, andk >1such that, for alli,P(Z i > t)\u2264C 0 exp(\u2212atk),and\u03f5 n =O p((logn) \u22122/k).\nAs in Lei et al. (2018), we do not assume thate\u00b5 i represents the true model. Note also that\nthe stability condition of Lei et al. (2018) amounts to assuming, in the current notation, that\nfor large enoughn,P(max i\u2208{1,...,n+1} |\u00b5i( bw, b\u03b8)\u2212e\u00b5i| \u2265\u03b7n)\u2264\u03c1 n,for some sequences satisfying\n\u03b7n =o(1) and\u03c1 n =o(1) asn\u2192 \u221e. In our setup, we introduce a dominating variableZ i\nand specify a convergence rate, thereby avoiding the need to take a supremum overi. Our\nformulation is more natural for time series variables, which are often unbounded. The original\ncondition in Lei et al. (2018) remains sufficient for our result. The next Theorem shows that\nAlgorithm 1 is asymptotically valid. 2\nTheorem 2If Assumptions 2-6 hold or Assumptions 2, 3 and 7 hold, then\nP{y n+1 \u2208 C(xn+1)}\np\n\u2212 \u21921\u2212\u03b1.\n2The theorem is currently proved under the first set of assumptions. To prove it under Assumption 7 instead,\nonly Step 1 needs to be modified: replaceC n with the supremum overZ i.\n14",
        "31e69899-ec98-47c8-80cd-e1d581052756": "We prove this result by adapting the strategy used in the proof of Theorem 1 in Andrews\n(2003), which establishes the asymptotic validity of critical values derived from a subsampling-\nlike method for an F-test designed to detect structural breaks near the end of the sample.\nIn their approach, critical values are obtained by repeatedly re-estimating the model while\nexcluding a subset of observations\u2014a procedure that contrasts with the conformal inference\napproach used in Algorithm 1, which evaluates residuals using a fixed parameter estimate and\nbased on a symmetry-based ranking.\nTheoretical analyses of conformal inference for time series are far less developed than for the\nexchangeable case. Chernozhukov et al. (2018) make an important contribution by studying a\nprocedure that constructs conformal scores from blocks of residuals rather than from individual\nobservations. Our result shows that blocking is not required for asymptotic validity. This does\nnot rule out the possibility that blocking could improve finite-sample performance by reducing\nserial dependence across conformal scores. Later in the paper we take a different approach to\ndeal with such dependence and obtain algorithms that are adaptive to serial correlation and\nconditional heteroskedasticity.\n4.3 Applications to model averaging methods\nIn this section, we show how the algorithm can be applied to various model averaging methods\nand examine the plausibility of the assumptions in each case. We focus on linear models, both to\nprovide explicit expressions and because they account for most applications of model averaging\nin practice. All methods considered are also evaluated through Monte Carlo experiments and\napplied in the empirical applications.\nFor notation, lety= (y 1, . . . , yn, y)\u22a4 \u2208R n+1 denote the observed outcomes augmented with\na candidate valueyfory n+1. LetX= [x \u22a4\n1 , . . . ,x\u22a4\nn ,x \u22a4\nn+1]\u22a4 \u2208R (n+1)\u00d7p be the regressor matrix\ncontaining allpexplanatory variables that appear in any of theMcandidate models. For each\nmodelm= 1, . . . , M, letpm \u2264pbe the number of regressors and define the selection matrix\nCm \u2208R p\u00d7pm so thatXC m \u2208R (n+1)\u00d7pm extracts the columns ofXused in modelm. Then\nmodelmcan be written as\ny=XC m\u03b2m +\u03b5,(5)\nwith\u03b2 m \u2208R pm the coefficient vector and\u03b5\u2208R n+1 the error term.\nAll candidate models are estimated by OLS. For themth model, denote the fitted values\nby bf(m) = ( bf(m)\n1 , . . . ,bf(m)\nn , bf(m)\nn+1)\u22a4 \u2208R n+1. Collecting these fitted vectors across all candidate\nmodels yields the matrix\nbF= ( bf(1), . . . ,bf(M) )\u2208R (n+1)\u00d7M ,\n15",
        "353f188d-f0bc-4610-bb70-9ddbff02f283": "whoseith row is bfi = ( bf(1)\ni , . . . ,bf(M)\ni )\u2208R M . As before, letw= (w 1, . . . , wM )\u22a4 denote the\nvector of model weights.\nFor assumptions, since the models are estimated with OLS, Assumption 2 is automatically\nsatisfied, and Assumptions 1, 3, and 5 are standard. We therefore do not examine these further\nand instead focus on Assumptions 4 and 6, which we verify using primitive conditions familiar\nfrom the time series literature for each model averaging method considered below.\n4.3.1 Fixed weights\nSimple model averaging methods with fixed weights (including equal weights) often perform well\nrelative to more sophisticated approaches, since estimating weights can substantially increase\nprediction bias or variance. Stock and Watson (2006) and Timmermann (2006) discuss this\nin the context of macroeconomic forecasting, and Claeskens et al. (2016) provide expressions\nquantifying the effect of weight estimation on forecast precision. With fixed weights, computa-\ntion reduces to repeated OLS regressions and is therefore straightforward. We now turn to the\nassumptions.\nAssumption 4 holds under mild conditions, as it only requires\nb\u03b2m\np\n\u2212 \u2192\u03b20,m =\n\u0000\nE\n\u0002\nC\u22a4\nmxix\u22a4\ni Cm\n\u0003\u0001\u22121\nE\n\u0002\nC\u22a4\nmxiyi\n\u0003\n(6)\nfor eachm\u2208 {1, . . . , M}. This is satisfied, for example, when the regressor matrix has full\ncolumn rank and satisfies weak moment restrictions that ensure a law of large numbers for time\nseries data. Explicitly, it holds under Assumption 3 together with Condition 1 stated below:\nCondition 1(i)E[\u2225x i\u22252]<\u221e, (ii)E[\u2225x iyi\u2225]<\u221e, and (iii)E[x ix\u22a4\ni ]is positive definite.\nThe proof of (6) is straightforward and given in the appendix for completeness, where we\nclarify that it does not require correct model specification (see Lemma 4). Similar conditions\nare used in Theorem 3.3.7 of White (2001), which establishes a law of large numbers for the\nOLS estimator under dependent and identically distributed observations.\nNow consider Assumption 6, whereZ i only consists of derivatives with respect to\u03b2 m for\nm= 1, . . . , M. In particular,\n\u2202ei(w,\u03b2)\n\u2202\u03b2m\n=\u2212w mC\u22a4\nmxi,\nwhich involves no unknown parameters. Thus, the inequality in Assumption 6 is satisfied if the\nregressors meet the sub-Weibull tail condition specified there, including normal distribution as\na special case. The only remaining requirement in that assumption is the convergence rate of\nb\u03b2m to\u03b2 0,m for anym. In the appendix, Lemma 5 shows that\nb\u03b2m \u2212\u03b2 0,m =O p(n\u22121/2),\n16",
        "34cb5a3e-1acf-4a4d-a9d2-5867d0558853": "under Condition 1, Assumption 3, and the additional condition stated below:\nCondition 2(i)E[\u2225x ix\u22a4\ni \u2225r]<\u221eandE[\u2225x iyi\u2225r]<\u221efor somer >2; and (ii)(x i, yi)form\na strong mixing sequence with size2r/(r\u22122).\nCondition 2 permits commonly used weakly dependent processes, such as stationary finite-order\nvector autoregressive moving-average (VARMA) processes. In summary, the assumptions are\nnot restrictive for applying the algorithm to model averaging methods with fixed weights in\nboth cross-sectional and time-series settings.\n4.3.2 Granger and Ramanathan\u2019s (1984) regression approach\nGranger and Ramanathan (1984) proposed selecting the model weightswby regressingyon\nthe candidate model predictions bF:\ny= bFw+e,(7)\nwheree= (e 1, . . . , en+1) denotes a vector of residuals. Granger and Ramanathan (1984) also\nconsidered including an intercept in the regression, leading toy=w 0 + bFw+e. This is\nequivalent to adding a candidate model with only an intercept as its explanatory variable, and\nthus still falls within the current framework.\nThe application of Algorithm 1 to this approach proceeds as follows. First, runMseparate\nregressions as in (5) on a sample augmented byyto construct bF. Second, regressyon bFas\nin (7) to obtain the weights bw. Third, compute the empirical quantiles of the residuals beto\ndecide whether to keepy. Finally, repeat the procedure with alternative augmented samples\nto obtain an interval. This is simple to implement.\nWe next turn to the assumptions. For Assumption 4, the regression coefficients still satisfy\nb\u03b2m \u2212\u03b20,m =O p(n\u22121/2) since their estimation is the same as in the fixed-weights case. To study\nthe convergence of bw, letf (m)\ni denote the population projection ofy i onto the explanatory\nvariables of modelm:f (m)\ni =x \u22a4\ni Cm\u03b20,m,and letf i = (f (1)\ni , . . . , f(M)\ni )\u22a4 denote the vector of\nprojected values under theMmodels. Definew 0 =\n\u0000\nE[fif\u22a4\ni ]\n\u0001\u22121\nE[fiyi].Then, Lemma 6 in the\nAppendix shows that\n\u221an( bw\u2212w 0) =O p(1) (8)\nunder Assumption 3, Conditions 1 and 2, and the following condition:\nCondition 3E[f if\u22a4\ni ]is nonsingular.\nThis condition holds as long as there is no collinearity among model predictions asymptotically.\nA typical violation occurs when two models are nested and the additional regressors in the larger\n17",
        "ced4cdb9-c171-4dc1-a5d5-f5c9450a2b49": "model have zero population coefficients. Although Granger and Ramanathan\u2019s (1984) proce-\ndure is not intended for such settings, our method remains applicable provided Assumption 7\nholds. The intuition behind Assumption 7 in this case is that, even under multicollinearity,\nthe projection ofyonto the column space of bFis unique and well-defined. Consequently, the\ncombined predictor is identified and concentrates around the population projection, which is\nstationary and ergodic by Assumption 3.\nThe last assumption to verify is the inequality in Assumption 6. Because\nei(w,\u03b2) =y i \u2212\nMX\nm=1\nwm x\u22a4\ni Cm \u03b2m,\nwe obtain\n\u2202ei(w,\u03b2)\n\u2202\u03b2m\n=\u2212w mC\u22a4\nmxi and \u2202ei(w,\u03b2)\n\u2202wm\n=\u2212x \u22a4\ni Cm \u03b2m.(9)\nHence, the inequality in Assumption 6 holds if the regressors satisfy a sub-Weibull tail condition,\nas in the fixed-weight case. We conclude that the algorithm is valid under mild assumptions\nfor the intended applications of this model averaging approach.\nThe conclusions reached for (8) and (9) apply equally to the model averaging methods\nconsidered below. Accordingly, we do not re-examine them for those methods.\n4.3.3 Smoothed information criteria\nBuckland et al. (1997) proposed defining model weights using the Akaike Information Criterion\n(AIC) or the Bayesian Information Criterion (BIC):\nbwm = exp(\u2212ICm/2)PM\ns=1 exp(\u2212ICs/2)\n,\nwhereIC m is the information criterion value for themth model. For linear regression models\nestimated by OLS, these are\nAICm = (n+ 1) log(b\u03c32\nm) + 2pm,and BIC m = (n+ 1) log(b\u03c32\nm) +p m log(n+ 1),\nwhereb\u03c32\nm is the estimated error variance from modelm,p m is the number of parameters in\nmodelm, andn+ 1 is the augmented sample size.\nTo implement Algorithm 1 with this model averaging approach, first estimate theMmodels\nfor each augmented sample and compute the corresponding information criteria to obtain the\nmodel weights. The same procedure is then repeated for other augmented samples to construct\nthe prediction interval. These steps mirror the application of our conformal inference procedure\nto Granger and Ramanathan (1984)\u2019s model averaging approach.\n18",
        "bbe7ac46-47be-4fb7-9810-87f8110df30d": "We next examine Assumptions 4 and 6. We focus on the model weights; the remaining\nparts require no further discussion, as noted above. Lemma 7 in the appendix extends Sin and\nWhite (1996) and shows that, under fixed parameters and model dimensions, the estimated\nweights asymptotically concentrate on the model(s) that minimize the per-observation Kull-\nback\u2013Leibler information criterion (KLIC), with exponential or polynomial rates in the sample\nsize determined by the KLIC gaps.\nWe now provide more details for this convergence result. Note that the KLIC for themth\nmodel, up to an additive constant, is\nQm(\u03b8m) =E\n\u0002\n\u2212log\u03d5\n\u0000\nyi;x \u22a4\ni Cm\u03b2m, \u03c32\u0001\u0003\n,\nwith\u03d5(y;\u00b5, \u03c32) = (2\u03c0\u03c3 2)\u22121/2 exp{\u2212(y\u2212\u00b5) 2/(2\u03c32)}. LetQ m denoteQ m(\u03b8m) evaluated at its\npseudo-true value\u03b8 0,m. Letm \u22c6 \u2208arg min m Qm denote an index of a model that attains the\nsmallest KLIC (when the minimizer is unique,m \u22c6 is that model). Writing the IC gap for model\nmfromm \u22c6 asIC m \u2212IC m\u22c6 = (n+ 1) \u2206 m +c n (pm \u2212p m\u22c6) +\u03b5 m,n, where \u2206 m =Q m \u2212Q m\u22c6\nis the KLIC gap,c n = 2 for AIC andc n = log(n+ 1) for BIC, and\u03b5 m,n =O p(1). Then,\nunder Assumption 3, Conditions 1 (withoutiii) and 2, Lemma 7 implies the following three\nresults:(1) Ifm \u22c6 is unique, then for both AIC and BIC weights,bw m\u22c6 \u22121 =O p\n\u0000\ne\u2212n\u2206min/2\u0001\nandbwm =O p\n\u0000\ne\u2212n\u2206m/2\u0001\nfor everym\u0338=m \u22c6, where \u2206 min = minj\u0338=m\u22c6 \u2206j. Therefore, in this case\nthe estimated model weights converge to 1 or 0 at exponential rates.(2)If multiple models\nachieve the smallest KLIC, then the BIC weights concentrate on the tied model with the fewest\nparameters, denotedm 0 withp min parameters:bwm0 \u22121 =O p\n\u0000\nn\u2212dmin/2\u0001\nand, for any tied model\nwithp m > pmin parameters,bwm =O p\n\u0000\nn\u2212(pm\u2212pmin)/2\u0001\n, whered min >0 is the parameter gap\nbetween the two most parsimonious tied models. For the remaining models,bwm =O p\n\u0000\ne\u2212n\u2206m/2\u0001\n.\nThus, the BIC model weights converge to 1 or 0 at polynomial rates (within the tied set) or at\nexponential rates (outside the tied set).(3)If multiple models attain the smallest KLIC, then\nthe AIC weights satisfy P\nm\u2208Kmin bwm \u22121 =O p\n\u0000\ne\u2212n\u2206\u25e6/2\u0001\nandbwm =O p\n\u0000\ne\u2212n\u2206m/2\u0001\nform /\u2208 Kmin,\nwhereK min ={m: \u2206 m = 0}and \u2206 \u25e6 = minm/\u2208Kmin \u2206m >0. Thus, the total weight on the best-\nperforming models converges to 1, and the weights on all other models converge individually\nto 0 at exponential rates. However, within the best-performing set, the weights may remain\nrandom even asymptotically.\nTogether, the results show that Assumptions 4 and 6 hold under mild conditions, except\nwhen more than one model attain the smallest KLIC and AIC weights are used for model\naveraging. The theoretical properties of Algorithm 1 in this case are left for future work.\n19",
        "66d5d71e-55ee-40dd-b36e-694abf2056dc": "4.3.4 Mallows criterion\nHansen (2007) proposes selecting the weights by minimizing a Mallows criterion:\nC(w) =\u2225y\u2212 bFw\u22252 + 2\u02c6\u03c32\nMX\nm=1\nwmpm,(10)\nwhereb\u03c32 is an estimator of the error variance, typically obtained from the largest fitted model,\nand PM\nm=1 wmpm denotes the effective number of parameters of the averaged model, where\npm is the number of regressors in modelm. Hansen (2007) shows that the resulting weights\nare asymptotically mean-square optimal for independent data. Hansen (2008) considers its\napplication to time series forecasting, while Liao et al. (2021) establishes its optimality when\nthe data-generating process is an infinite-order autoregression. The latter paper also establishes\nconvergence rates for the estimated weights when the limit is unique and an interior point.\nApplying Algorithm 1 here involves runningMregressions on an augmented sample, min-\nimizing the objective function (10) to obtain Mallows weights, computing empirical residual\nquantiles, and repeating the process with alternative augmented samples. The key difference\nfrom the three model averaging approaches discussed earlier is an optimization step to com-\npute the model weights. Since this step is a quadratic program inMvariables, the additional\ncomputational cost is typically small.\nTo verify Assumptions 4 and 6, Lemmas 8 and 9 in the Appendix show that\n\u2225bw\u2212w 0\u2225=O p(n\u22121/2),\nin two practical settings: (i) when a single model uniquely minimizes the population risk\n(equivalently, has the lowest per-observation KLIC, as defined in smoothed information-criterion\nweighting), in which case the estimated weights concentrate on that model, so thatw 0 is a unit\nvector with a one in the position of the best model; and (ii) when several models tie in KLIC,\nin which case convergence holds provided the population loss\nC\u221e(w) =E\n\"\u0010\nyi+1 \u2212\nMX\nm=1\nwmf(m)\ni+1\n\u00112\n#\nhas a unique minimizer on the simplex (i.e., a unique \u201cbest mix\u201d of models), so that the limiting\nvectorw 0 contains multiple nonzero entries corresponding to the optimal mix. These results\nhold under Conditions 1 and 2 and Assumption 3.\nThese two lemmas do not cover the case where the population loss has multiple minimizers,\ni.e., when different model combinations yield the same population risk. In that case, the\nestimated weights may remain random asymptotically. Analysis of the algorithm in this setting\nis left for future work.\n20",
        "f70e7265-1c09-4354-a16f-6a4046aa6b08": "4.3.5 Jackknife weighting\nHansen and Racine (2012) proposed a jackknife model averaging (JMA) estimator which selects\nthe weights by minimizing a cross-validation criterion:\nCV(w) =|y\u2212 \u00afFw|2,\nwhere \u00afFis the same as bF, except that itsith row, which corresponds to the prediction fory i, is\nobtained from OLS regressions excluding theith observation, rather than using all observations\nas in bF. This leave-one-out construction is applied to each row of \u00afF. Hansen and Racine (2012)\nshowed that the estimator is asymptotically optimal in the sense of achieving the lowest possible\nexpected squared error for random samples, allowing for conditional heteroskedasticity. Zhang\net al. (2013) extended the optimality result to a limited class of time series models, including\ninfinite-order autoregressions and models with serially correlated errors and strict exogeneity.\nThe application of the algorithm to JMA follows similar steps as in the Mallows case, except\nthat it\u2019s more computationally costly due to the repeated leave-one-out estimation. The conver-\ngence properties of the weights are similar to the Mallows case as well. If the population loss has\na unique minimizer on the simplex, then the jackknife model averaging weights converge to that\ntarget at the usual \u221anparametric rate under standard time-series conditions (Assumption 3,\nConditions 1 and 2). Intuitively, the leave-one-out criterion uniformly tracks the population\nprediction riskC \u221e(w), and the risk\u2019s positive curvature around its unique minimum delivers\nthe \u221anrate. This covers both the case where a single model is optimal (weights concentrate\non that model) and the case where a unique mixture is optimal (weights concentrate on that\nmix). If the population loss admits multiple minimizers, one may obtain convergence only to\nthe minimizer set unless a tie-breaking rule is imposed, again mirrors the Mallows case.\n5 Locally adaptive predictive intervals\nAlgorithm 1 is simple to implement; however, it does not make use of properties of the model\nresiduals beyond basic assumptions. As a result, the prediction interval is not adaptive and\nremains approximately constant across different covariate values, leaving useful information\nunused. For example, if the residuals are conditionally heteroskedastic, this information can\nbe incorporated to obtain intervals that are narrower when the conditional variance is low and\nwider when it is high. This issue arises for conformal prediction broadly and is not specific to\nmodel averaging. It has been examined in recent work such as Lei et al. (2018), Romano et al.\n(2019), and Chernozhukov et al. (2021). Building on this line of research, we develop methods\ntailored to the model averaging setting. The main idea is to introduce a statistical model for\n21",
        "2d4ebd7b-e175-4a14-ae1a-e2737b959868": "the model-averaging residuals and use it to prewhiten them before constructing the prediction\ninterval.\nAs before, letybe any candidate value fory n+1. For simplicity, denote the model-averaging\nresidualbey,i in (4) bybei. We introduce a statistical model for these residuals. This model is\nnot assumed to coincide with the true data-generating process; rather, it serves as a device to\ncapture some degree of conditional heteroskedasticity and, in the time-series setting, potential\nserial correlation. We treat the exchangeable and time-series cases separately, with the former\nbuilding on the approach of Lei et al. (2018).\nFor exchangeable observations, we consider the following model for the model averaging\nresiduals:\nbei =\u03c3 i\u03b7i (i= 1, . . . , n+ 1),\nwith\n\u03c32\ni = exp(x\u22a4\ni \u03b3).\nThe term\u03b7 i is assumed to be i.i.d. with mean zero and unit variance. The parameter\u03b3\ncan be estimated by regressing log(be2\ni ) onx i using OLS, or by maximum likelihood under the\nassumption thatbei \u223c N(0,exp(x\u22a4\ni \u03b3)). If only a subset of covariates is expected to influence\nheteroskedasticity, one may consider a subset or use a penalized regression (e.g., LASSO) to\nestimate\u03b3. Once\u03b3is estimated, the standardized residuals are computed as\nb\u03b7y,i :=b\u03b7i = bei\nb\u03c3i\n,withb\u03c3 2\ni = exp(x\u22a4\ni b\u03b3).(11)\nLei et al. (2018) regress absolute residuals on covariates to estimate local variability, which\ncan also be adopted here. By contrast, the log-linear specification guarantees positivity and\naccommodates both OLS and maximum likelihood estimation.\nIn the time-series case, we propose modeling the residuals with an AR(1)\u2013GARCH(1,1)\nspecification:\nbei+1 =\u03b4+\u03c1be i +\u03c3 i\u03b7i+1, \u03c3 2\ni =c+\u03b1be 2\ni +\u03b2\u03c3 2\ni\u22121,\nwherei= 1, . . . , n, and\u03b7i+1 is assumed to be i.i.d. with mean zero and unit variance. The AR(1)\ncomponent is in the same spirit as Andrews (1991), who used it as a parsimonious approxima-\ntion to more general data-generating processes to obtain improved covariance estimates in the\npresence of heteroskedasticity and autocorrelation of unknown forms. The GARCH(1,1) spec-\nification is motivated by empirical evidence that it is often a competitive model for capturing\nfinancial volatility; see Hansen and Lunde (2005). This proposal does not preclude alternative\nspecifications\u2014for example, replacing AR(1) with an MA(k) process in the conditional mean\n22",
        "975968d5-1eb9-4e8c-b2c9-59620706ef6d": "equation when appropriate for the application, or omitting the AR(1) component entirely if\nresidual serial correlation is expected to be negligible.\nThe model is estimated by Gaussian maximum likelihood, as is standard practice. To ini-\ntialize the recursion, we set\u03c3 2\n0 =c/(1\u2212\u03b1\u2212\u03b2),which corresponds to the unconditional variance\nof a stationary GARCH(1,1) process. After estimation, the conditional standard deviationsb\u03c3 i\nare obtained recursively using the fitted values of (\u03b4, \u03c1, c, \u03b1, \u03b2). The residuals are then standard-\nized by first removing serial correlation and subsequently dividing by the conditional standard\ndeviation, as\nb\u03b7y,i+1 :=b\u03b7i+1 = bei+1 \u2212 b\u03b4\u2212b\u03c1bei\nb\u03c3i\n.(12)\nPutting the pieces together, we propose the following algorithm, which replaces the residuals\nbei in Algorithm 1 with the standardized residualsb\u03b7i. The procedure applies to both exchange-\nable and time-series data.\nAlgorithm 2Locally Adaptive Prediction Interval for Model Averaging\nInput:Data{(x i, yi)}n\ni=1; coverage level (1\u2212\u03b1)\u2208(0,1); model averaging algorithmA; new ob-\nservationx n+1 at which to construct the prediction interval; valuesY trial ={y \u2217\n1, y\u2217\n2, . . .}for\npotential outcomey n+1; and a parametric model to approximate conditional heteroskedas-\nticity or serial correlation of the prediction residuals.\nOutput:Prediction interval atx n+1, namelyC(x n+1).\n1:fory\u2208 Y trial do\n2:Using the augmented data{(x i, yi)}n\nit=1 \u222a{(xn+1, y)}, compute the weights and the model\naveraging prediction ruleAas in Equation (3).\n3:Compute the standardized residualsb\u03b7 y,i, using equation (11) for the exchangeable data\ncase and equation (12) for the time series case. Obtain generalized conformity score:\nRy,i =|b\u03b7y,i|fori= 1, . . . , n, n+ 1.\n4:Compute p-value ofR y,n+1:\n\u03c0(y) = 1 + Pn\ni=1 I(Ry,i \u2265R y,n+1)\nn+ 1 .\n5:end for\n6:return\nC(xn+1) ={y\u2208 Ytrial |\u03c0(y)> \u03b1}.\nFor exchangeable data, the algorithm is valid in finite samples because standardizing the\nresiduals does not affect exchangeability, since each residuale i is specified as a function of\n23",
        "8f70f98e-19ab-4098-a7d4-dd0f38921364": "its own covariatesx i only. For time series data, where asymptotic validity does not require\nexchangeability, the algorithm is asymptotically valid if the standardized residuals are approx-\nimately stationary and ergodic in large samples. We formalize these results in the following\ncorollary. Let\u03d5=\u03b3in the exchangeable case, and\u03d5= (\u03b4, \u03c1, c, \u03b1, \u03b2) in the time series case.\nCorollary 1(i) For the exchangeable data case, the conclusion of Theorem 1 holds under the\nsame conditions stated there. (ii) For the time series case, if Assumptions 2 and 3 hold and\nAssumptions 4-6 are satisfied with\u03b8replaced by(\u03b8,\u03d5)ande i replaced by\u03b7 i, then\nP{y n+1 \u2208 C(xn+1)}\np\n\u2212 \u21921\u2212\u03b1.\nThe assumptions in Corollary 1 are similar to those in Theorems 1 and 2, with modifications\narising from standardization. Assumptions 1\u20133 are unchanged, and Assumption 5 remains\nstandard. For Assumption 4, the convergence of b\u03b8and bwis unchanged, but the assumption\nis strengthened to also require convergence of the additional parameters\u03d5to fixed values.\nAssumption 6 is also modified: it now requires that the derivatives of the standardized residuals\n(rather than the original residuals) with respect to the coefficients and weights satisfy regularity\nconditions. Assumption 7 does not enter the corollary. Thus, relative to the earlier results, the\nmain strengthenings are in Assumptions 4 and 6, together with the exclusion of cases covered\nby Assumption 7; these changes pertain only to the time-series setting.\nThe proof for (i) is the same as that of Theorem 1. The proof for (ii) is the same as that of\nTheorem 2 after replacing\u03b8ande i with (\u03b8,\u03d5) and\u03b7 i when relevant. We therefore we do not\ninclude separate proofs in the appendix.\n5.1 Applications to model averaging methods\nAlgorithm 2 can be applied to all model averaging methods discussed in Subsection 4.5. Be-\ncause the standardization step is applied after the model weights are computed, the additional\ncomputational cost is marginal.\nRegarding the assumptions for the time series case, verifying the convergence of the AR(1)-\nGARCH(1,1) parameters and the properties of the residual gradients in Assumption 6 is cum-\nbersome due to the added nonlinearity and dynamics and provides little additional insight.\nInstead, we evaluate the procedure through simulations and empirical applications to assess\nthe trade-offs introduced by the additional assumptions. The results suggest that standardiza-\ntion does not affect coverage, but may lead to noticeably wider intervals when many covariates\nare included in the variance model.\n24",
        "75f8df93-4af0-4b48-a0d8-ecf44cb44a9d": "6 Split-sample prediction intervals\nIf computational feasibility outweighs prediction accuracy, for example when the sample is\nlarge or the models are complex, split-sample procedures are attractive. Sample splitting is\ncommon in the conformal inference literature (see, e.g., Lei et al. (2018)). Below we introduce\na sample-split version of Algorithm 1.\nAlgorithm 3Split-sample Prediction Interval for Model Averaging\nInput:Data{(x i, yi)}n\ni=1; coverage level (1\u2212\u03b1)\u2208(0,1); model averaging algorithmA; new ob-\nservationx n+1 at which to construct the prediction; valuesY trial ={y \u2217\n1, y\u2217\n2, . . .}for potential\noutcomey n+1.\nOutput:Prediction intervalC A(xn+1).\n1:Split the index set{1, . . . , n}into two subsetsI1 andI 2 of (approximately) equal size.\n2:Estimate coefficients and model weights onI 1 to obtain the prediction ruleb\u00b5(\u00b7) inA. 3\n3:Compute conformity score for eachi\u2208 I 2 using the estimated prediction ruleb\u00b5(\u00b7):\nRi =|y i \u2212b\u00b5(xi)|.\n4:fory\u2208 Y trial do\n5:Compute conformity score for candidatey:\nRy,n+1 =|y\u2212b\u00b5(x n+1)|.\n6:Compute p-value ofR y,n+1:\n\u03c0(y) = 1 + P\ni\u2208I2 I(Ri \u2265R y,n+1)\n|I2|+ 1 .(13)\n7:end for\n8:return\nC(xn+1) ={y\u2208 Ytrial |\u03c0(y)> \u03b1}.(14)\nFor exchangeable data, the sample splitting should be random, independent of both co-\nvariates and outcomes. For time series data, the split should follow the time order, with the\nfirst half used for estimation and the second half for inference. Computationally,R y,n+1 ex-\nceeds the (1\u2212\u03b1) quantile of{R i}i\u2208I2 \u222a {Ry,n+1}if and only if it exceeds (1\u2212\u03b1) quantile of\n{Ri}i\u2208I2 \u222a {\u221e}(see Lemma 1 in Tibshirani et al. (2019)). Therefore, the setC(x n+1) can be\ncomputed equivalently asC(x n+1) = [ \u02c6\u00b5(xn+1)\u2212d,\u02c6\u00b5(x n+1) +d], wheredis thek-th smallest\n3Since the coefficient and weight estimates do not depend on candidateyfory n+1,b\u00b5y,i simplifies tob\u00b5(xi).\n25",
        "87649eb0-a402-418d-9591-922a452c87f5": "value of{R i :i\u2208 I2}withk=\n\u0006\n(|I2|+ 1)(1\u2212\u03b1)\n\u0007\n. This formula is used in Algorithm 2 of Lei\net al. (2018) to construct split conformal prediction intervals for general regression problems.\nThe computational saving comes from estimating model weights only once. This makes\nit feasible to apply otherwise costly methods, such as ensemble approaches (e.g., bootstrap\naggregation, or bagging; Breiman (1996)) and Bayesian model averaging. In bagging, the\nbootstrap is applied to a training subsample to obtain estimates, with the remaining sample used\nto construct the prediction interval. In Bayesian model averaging, the training subsample is used\nto estimate model weights and parameters (e.g., posterior means), and the remaining sample\nto construct the interval. The tradeoff is a potential loss of prediction accuracy relative to the\nfull-sample case, possibly leading to wider intervals. We examine this tradeoff in simulations\nand empirical applications. The next result establishes finite-sample validity for exchangeable\ndata and asymptotic validity for time series data.\nCorollary 2(i) For exchangeable data, under Assumption 1,P{y n+1 \u2208 C(xn+1)} \u22651\u2212\u03b1.If,\nin addition, the fitted absolute residuals|y i \u2212b\u00b5(xi)|,i\u2208 {[n/2+1], . . . , n+1}have a continuous\njoint distribution for ally\u2208R, then it also holds thatP{y n+1 \u2208 C(xn+1)} \u22641\u2212\u03b1+ 1/(|I 2|+\n1).(ii) For time series data, if Assumptions 3-6 hold, or if Assumptions 3 and 7 hold, then\nP{y n+1 \u2208 C(xn+1)}\np\n\u2212 \u21921\u2212\u03b1.\nFor exchangeable data, because parameters and model weights are estimated onI 1, they do\nnot interact with the exchangeability of the residuals onI2, therefore Assumption 2 is no longer\nneeded. For the time-series case, since the (n+ 1)th observation does not enter the estimation,\nthis assumption is similarly not relevant. This simplification implies that the resulting predic-\ntion interval is valid for estimation methods involving resampling, such as bagging. It\u2019s also\nvalid for Bayesian model averaging, where the prior and estimates are obtained independent of\nI2. More details on these two applications are given at the end of this section. The proof of\nthis corollary is essentially the same as that of Theorems 1 and 2, in fact simpler because the\n(n+ 1)th observation is independent of the estimation, and is therefore omitted.\nFor some applications, combining sample splitting with local adaptability is useful, as it\nreduces computational cost relative to full-sample estimation while still adapting to the residual\nstructure atn+ 1. We propose the following algorithm:\n26",
        "9c1be897-6142-4185-a037-6cc2959d7b85": "Algorithm 4Locally Adaptive Prediction Intervals with Sample Splitting for Model Averaging\nInput:Data{(x i, yi)}n\ni=1; coverage level (1\u2212\u03b1)\u2208(0,1); model averaging algorithmA; new ob-\nservationx n+1 at which to construct the prediction; valuesY trial ={y \u2217\n1, y\u2217\n2, . . .}for potential\noutcomey n+1; and a parametric model for residuals to capture conditional heteroskedastic-\nity or serial correlation.\nOutput:Prediction intervalC A(xn+1).\n1:Split the index set{1, . . . , n}into two subsetsI1 andI 2 of (approximately) equal size.\n2:Estimate coefficients and model weights onI 1 to obtain the prediction ruleb\u00b5(\u00b7) and resid-\nuals. Fit a parametric model to these residuals to estimate the heteroskedasticity and/or\nserial dependence parameters b\u03d5.\n3:Applyb\u00b5(\u00b7) and b\u03d5toI 2 to compute the standardized residuals, using equation (11) for\nexchangeable data and (12) for time series data. Calculate the generalized conformity score\nfori\u2208 I2:R i =|b\u03b7i|.\n4:fory\u2208 Y trial do\n5:Compute conformity score for candidatey:R y,n+1 =|b\u03b7y,n+1|.\n6:Compute p-value ofR y,n+1:\n\u03c0(y) = 1 + P\ni\u2208I2 I(Ri \u2265R y,n+1)\n|I2|+ 1 .\n7:end for\n8:return\nC(xn+1) ={y\u2208 Ytrial |\u03c0(y)> \u03b1}.\nAs in Algorithm 3, the model weights are computed only once. All model coefficients,\nincluding those capturing heteroskedasticity and serial correlation, are estimated from the first\nsubsample. Also, the prediction interval can be equivalently computed asC(x n+1) = [\u02c6\u00b5(xn+1)\u2212\nd\u02c6\u03c3n+1,\u02c6\u00b5(xn+1) +d\u02c6\u03c3n+1] for exchangeable data andC(x n+1) = [\u02c6\u00b5(xn+1) + \u02c6\u03b4+ \u02c6\u03c1{yn \u2212\u02c6\u00b5(xn)} \u2212\nd\u02c6\u03c3n,\u02c6\u00b5A(xn+1) + \u02c6\u03b4+ \u02c6\u03c1{yn \u2212\u02c6\u00b5A(xn)}+d\u02c6\u03c3n] for time series data, wheredis thek-th smallest\nvalue of{R i :i\u2208 I2}, withk=\n\u0006\n(|I2|+ 1)(1\u2212\u03b1)\n\u0007\n. The resuting prediction interval is valid in\nfinite samples for exchangeable data and asymptotically valid for time series data:\nCorollary 3(i) For exchangeable data, under Assumption 1,P{y n+1 \u2208 C(xn+1)} \u22651\u2212\u03b1.If,\nin addition, the fitted absolute residuals|b\u03b7 i|,i\u2208 {[n/2 + 1], . . . , n+ 1}have a continuous joint\ndistribution ally\u2208R, then it also holds thatP{y n+1 \u2208 C(xn+1)} \u22641\u2212\u03b1+ 1/(|I 2|+ 1).(ii)\nFor time series data, if Assumption 3 hold and Assumptions 4-6 hold with\u03b8replaced by(\u03b8,\u03d5)\nande i replaced by\u03b7 i, thenP{y n+1 \u2208 C(xn+1)}\np\n\u2212 \u21921\u2212\u03b1.\nIn (i), the upper bound of coverage depends on the size of the second subsample rather\n27",
        "7986e018-181d-4d9e-8bc2-0eb38e998ca3": "than the full sample, reflecting the cost of sample splitting. In both (i) and (ii), because the\nparameters, including those for heteroskedasticity and serial correlation, are estimated from\nonly half of the observations, the added estimation noise may lead to wider intervals. The\nproof follows the same arguments as Theorems 1 and 2 and is omitted.\n6.1 Applications to model averaging\nAlgorithms 3 and 4 apply to all model averaging methods discussed in Subsection 4.3. They\nalso allow the construction of prediction intervals with methods that are either too costly in\ntheir full-sample versions or that require resampling. We illustrate this with one example of\neach.\nBayesian model averaging.To apply Algorithms 3 or 4 , split the index set{1, . . . , n}into\na training setI 1 and a calibration setI 2, and denote the corresponding data byD 1 andD 2.\nFor each candidate modelM m, specify a priorp(\u03b8 m | Mm) and a likelihood function. Using\nD1, compute the posterior mean \u02c6\u03b8m fromp(\u03b8 m | D1,Mm) and the marginal likelihood:\np(D1 | Mm) =\nZ\np(D1 |\u03b8 m,Mm)p(\u03b8 m | Mm)d\u03b8 m.\nThe posterior model probability is then\nbwm = p(D1 | Mm)\u00b7p(M m)PM\nj=1 p(D1 | Mj)\u00b7p(M j)\n.\nNext, apply{ \u02c6\u03b8m,bwm}M\nm=1 toI 2. For eachi\u2208 I2, the BMA prediction is\n\u02c6\u00b5BMA(xi) =\nMX\nm=1\nbwm f(m)(xi; \u02c6\u03b8m),\nand the conformity score is\nRi =\n\f\fyi \u2212\u02c6\u00b5BMA(xi)\n\f\f.\nFinally, compute the conformity score for any candidate value ofyn+1. The prediction interval is\nthe set of candidate values whose conformity scores do not exceed the desired empirical quantile.\nIf local adaptability is desired, an additional step can be included: obtain the heteroskedasticity\nor serial correlation coefficients at the posterior mean fromI 1, and then compute residuals for\nI2 using these estimates, as in Algorithm 4.\nThe resulting prediction interval is valid even when all models are misspecified. This con-\ntrasts with conventional Bayesian credible sets, whose frequentist coverage depends on model\nspecification. Although the interval is computed using a single parameter value (the posterior\nmean) and a single set of model weights, it does not understate uncertainty because it relies\n28",
        "2cb70189-7584-4913-968e-656e3c64784a": "on conformity rather than the sampling distributions of the parameters and weights. This\ndistinction is central to its validity.\nBagging.We consider a linear model setting for simplicity. In this context, bagging may\ninvolve generating multiple bootstrap resamples of the original prediction problem, applying a\nmodel selection rule to each resample, and averaging the resulting predictions. For example,\nInoue and Kilian (2008) used this approach to forecast U.S. inflation.\nSupposex i \u2208R p andy i \u2208R, and we treat\nyi =x \u22a4\ni \u03b2+e i\nas the most general predictive model, with an unknown subset ofxi potentially having predictive\npower. The model may be misspecified; for example, the true data-generating process could be\nnonlinear, or the model may omit important predictors.\nTo apply Algorithms 3 and 4, split the sample into two subsets of approximately equal size,\nas in the Bayesian model averaging case, withI 1 as the training set andI 2 the calibration set.\nFit bagged linear predictors onI 1 possibly as follows. Forb= 1, . . . , B: Draw a bootstrap\nsample of size|I 1|fromI 1 (sampling with replacement). Fit an OLS regression to the most\ngeneral model on the bootstrap sample. Apply a method to determine which variables to\ninclude in this bootstrap sample. For example, one can perform individualt-tests on estimated\ncoefficients against a fixed threshold and discard insignificant coefficients, as in Inoue and Kilian\n(2008). LetC b \u2208R p\u00d7pb be the resulting selection matrix such thatXC b \u2208R (n+1)\u00d7pb contains\nonly the selected columns ofX. Re-estimate the model using the selected regressors to obtain\nthe final OLS estimate for this bootstrap sample, denoted \u02c6\u03b2\n(b)\n. The prediction at anyx\u2208R p\nis then\n\u02c6f(b)(x) =x \u22a4Cb \u02c6\u03b2\n(b)\n.\nRepeat this for allb= 1, . . . , B, and average the predictions across bootstrap models to obtain\n\u02c6f(x) = 1\nB\nBX\nb=1\n\u02c6f(b)(x) = 1\nB\nBX\nb=1\nx\u22a4Cb \u02c6\u03b2\n(b)\n.\nAll computations above use only the training setI 1. Next, compute residuals onI 2. For each\ni\u2208 I2, the prediction, representing model averaging with equal weights, is given by\n\u02c6\u00b5(xi) = 1\nB\nBX\nb=1\nx\u22a4\ni Cb \u02c6\u03b2\n(b)\n,(i\u2208 I 2),\nand the conformity score is\nRi =|y i \u2212\u02c6\u00b5(xi)|.\n29",
        "3a4685b3-0875-4101-91e8-610620c43d98": "Compute the conformity score for any candidate value ofy n+1. The prediction interval consists\nof candidate values whose conformity scores fall within the desired empirical quantile. The\nsame remarks on local adaptability from the Bayesian model averaging case also apply here.\n7 Monte Carlo experiments\nWe examine the coverage properties of the proposed algorithms for all model averaging methods\nin Subsection 4.3. The following methods are reported, each implemented as originally pro-\nposed without modification: equal weighting (Equal), the regression approach of Granger and\nRamanathan (1984) (Regression), Smoothed AIC (SAIC), Smoothed BIC (SBIC), Mallows\u2019\nmodel averaging (MMA), and Jackknife model averaging (JMA). We present results separately\nfor the exchangeable and time-series cases. The coverage level of the prediction intervals is set\nat 90%, and each reported value is based on 500 replications.\n7.1 Experiment I: exchangeable data\nWe consider the same DGP as in Hansen (2007), with an extension to allow for heteroskedastic-\nity. The DGP is an infinite-order regression model (so all candidate models are misspecified):\nyi =\n\u221eX\nj=1\n\u03b2jxji +\u03c3 iei,\nwherex 1i = 1 is the intercept, the remainingx ji are i.i.d.N(0,1), and the error terme i \u223c\nN(0,1) is independent ofx i. The coefficients are\n\u03b2j =c\n\u221a\n2\u03b1 j\u2212\u03b1\u22121/2;\nin the homoskedastic case (\u03c3 i = 1), the populationR 2 =c 2/(1 +c 2), controlled bycand\nindependent of\u03b1. This corresponds exactly to the design in Hansen (2007).\nWe also consider a heteroskedastic extension in which the log standard deviation is\nlog(\u03c3i) =\n\u221eX\nj=1\n\u03b2jxji,\nso that all variables potentially affect the variance. The same coefficients\u03b2are used in both\nthe mean and variance equations, which is not an issue here because the focus is on coverage\nproperties of intervals rather than efficient estimation. This represents a more general setting\nwith misspecification in both the mean and variance equations.\n30",
        "11479571-1bd7-40af-9619-1e527120471c": "The sample size is set ton= 150, so the split-sample case has 75 training observations. The\nnumber of modelsMfollows Hansen (2007), using the ruleM= round(3n 1/3), which gives 16\nmodels for this sample size. Them-th candidate model is\nyi =\nm+1X\nj=1\n\u03b2jxji +\u03c3 iei.\nFor locally adaptive intervals, the log conditional variance is modeled as a linear function of all\nregressors in the largest candidate model. This reflects a conservative choice for local adaptive\nmethods, since researchers may in practice use additional information to select covariates and\nobtain tighter intervals. We varied\u03b1= 0.5,1.0,and 1.5 as in Hansen (2007); larger\u03b1implies\nfaster decay of the coefficients\u03b2 j withj. We also considered differentcvalues, such that the\npopulationR 2 = 0.2,0.5,and 0.8. To avoid repetition, results are reported only for\u03b1= 1 and\nR2 = 0.5, since conclusions are similar across values.\nFigure 1: Coverage for Exchangeable Data with Homogeneous Variance\n80%\n85%\n90%\n90.7%\n91.3%\n95%\nEqual (full)S\u2212AIC (full)S\u2212BIC (full)\nRegression (full)\nMMA (full)JMA (full)Equal (split)S\u2212AIC (split)S\u2212BIC (split)\nRegression (split)\nMMA (split)JMA (split)\nMethod\nCoverage\nNon\u2212adaptive Adaptive\nFigure 1. Coverage for exchangeable data with homogeneous variance\nRed dashed: target 90% coverage; Blue dashed: finite\u2212sample upper bounds.\nFigures 1 and 2 report the coverage rates and interval lengths for the case with homogeneous\nvariance. Figure 1 shows empirical coverage rates, with bars grouped into full-sample (left) and\nsplit-sample (right) implementations. Within each group, methods are further distinguished\nby whether they are adaptive to heteroskedasticity (blue) or non-adaptive (gray). Error bars\nrepresent one standard error. The red dashed line marks the 90% target coverage, and the blue\n31",
        "708d1e6c-ec2f-4c53-9280-1ad66d6ea0ed": "Figure 2: Interval Length for Exchangeable Data with Homogeneous Variance\n0\n10\n20\n30\nEqual (full)S\u2212AIC (full)S\u2212BIC (full)\nRegression (full)\nMMA (full)JMA (full)Equal (split)S\u2212AIC (split)S\u2212BIC (split)\nRegression (split)\nMMA (split)JMA (split)\nMethod\nInterval length\nNon\u2212adaptive Adaptive\nFigure 2. Interval length for exchangeable data with homogenous variance\nBars show mean interval length \u00b1 one standard error.\ndashed lines indicate finite-sample upper bounds. Across all methods, coverage is close to or\nabove the nominal level. Adaptive and non-adaptive versions perform similarly, showing that\nmodeling heteroskedasticity does not compromise validity.\nFigure 2 reports the corresponding average interval lengths in the same format. For non-\nadaptive methods, split-sample intervals are only slightly longer than full-sample intervals,\nsuggesting a small efficiency loss from splitting. In contrast, adaptive methods produce no-\nticeably wider intervals, reflecting the additional estimation variability introduced by modeling\nheteroskedasticity. The efficiency cost therefore stems less from sample splitting than from local\nadaptation. It is worth noting, however, that the effect of variance estimation will naturally\ndepend on the application; the current design represents a conservative case, since all variables\nfrom the largest model are used to model the variance. Together, Figures 1 and 2 show that\nall methods maintain valid coverage, with efficiency tradeoffs depending on the chosen design.\nFigures 3 and 4 present coverage and interval length when the data are generated with het-\nerogeneous variance. Coverage remains close to or above the nominal level across all methods,\nmuch like in the homogeneous case, with little difference between adaptive and non-adaptive\nversions. Efficiency patterns are also similar: split-sample intervals are only slightly longer\nthan full-sample ones for non-adaptive methods, while adaptive methods produce wider inter-\nvals. The main advantage of adaptive intervals is their ability to adjust to local variability,\n32",
        "f0d5096e-1dc5-41bb-839f-49c0e27b34a5": "Figure 3: Coverage for Exchangeable Data with Hetergeneous Variance\n80%\n85%\n90%\n90.7%\n91.3%\n95%\nEqual (full)S\u2212AIC (full)S\u2212BIC (full)\nRegression (full)\nMMA (full)JMA (full)Equal (split)S\u2212AIC (split)S\u2212BIC (split)\nRegression (split)\nMMA (split)JMA (split)\nMethod\nCoverage\nNon\u2212adaptive Adaptive\nFigure 3. Coverage for exchangeable data with heterogeneous variance\nRed dashed: target 90% coverage; Blue dashed: finite\u2212sample upper bounds.\nFigure 4: Interval Length for Exchangeable Data with Hetergeneous Variance\n0\n10\n20\n30\nEqual (full)S\u2212AIC (full)S\u2212BIC (full)\nRegression (full)\nMMA (full)JMA (full)Equal (split)S\u2212AIC (split)S\u2212BIC (split)\nRegression (split)\nMMA (split)JMA (split)\nMethod\nInterval length\nNon\u2212adaptive Adaptive\nInterval Length for Exchangeable Data with Heterogeneous Variance\nBars show mean interval length \u00b1 one standard error.\n33",
        "1b635e04-193b-45ed-921a-2b6ccab0d158": "intervals should widen when variance is high and narrow when it is low, and we plan to include\na closer examination of this pattern in the paper\u2019s next version.\n7.2 Experiment II: time series data\nWe design an experiment that mimics the empirical problem of predicting stock returns with\nmany potential predictors, using parameter values calibrated to empirical estimates in Table 1\nof Boudoukh et al. (2008). The data are generated from an infinite-order regression,\nri+1 =\u03b4+\n\u221eX\nj=1\n\u03b2jxj,i +u i+1, x j,i+1 =\u03c1 jxj,i +v j,i+1, u i =\n\u221eX\nj=1\n\u03b3jvj,i +e i,\nwithu i \u223ci.i.d.N(0, \u03c3 2\nu),v j,i \u223ci.i.d.N(0, \u03c3 2\nv) independent acrossj, ande i \u223ci.i.d.N(0, \u03c3 2\ne ).\nThe coefficients\n\u03b2j =c\n\u221a\n2\u03b1 j\u2212\u03b1\u22121/2\ndecay only gradually, so that many predictors contribute to predictability. Each predictor\nfollows an AR(1) process, a common assumption in the return-prediction literature. The errors\nui andv j,i are set up to be negatively correlated, which is known to bias estimates of\u03b2 j, a\nfeature emphasized in earlier empirical work, including Table 1 of Boudoukh et al. (2008).\nFigure 5: Coverage for Time-Series Data with Homogeneous Variance (n=100)\n80%\n85%\n90%\n95%\nEqual (full)S\u2212AIC (full)S\u2212BIC (full)\nRegression (full)\nMMA (full)JMA (full)Equal (split)S\u2212AIC (split)S\u2212BIC (split)\nRegression (split)\nMMA (split)JMA (split)\nMethod\nCoverage\nNon\u2212adaptive Adaptive\nCoverage for Time Series Data, n = 100\nRed dashed: target 90% coverage.\n34",
        "ea9fe0d7-1e41-4634-b129-be76e53dbaa8": "We set\u03b4= 0 (without loss of generality), take\u03c1 j = 0.953 for allj(so all regressors are\npersistent), and set\u03c3 u = 0.202 and\u03c3 v = 0.154. The correlation corr(v j,i, ui) =\u22120.712 implies\na projection coefficient cov(u i, vj,i)/var(vj,i)\u2248 \u22120.933;\u03b3j is drawn uniformly from [\u22120.933,0];\nand\u03c3 e =\nq\n\u03c32\nu \u2212\u03c3 2\nv\nP\u221e\nj=1 \u03b32\nj . This specification coincides with the DGP in Boudoukh et al.\n(2008) when there is only a single predictor and\u03b3=\u03b3 j =\u22120.933. Next, we set\u03b1= 1 as in the\nexchangeable simulation case and choosecso that theR 2 of the return prediction equation,\nR2 = 1\u2212var(u i)/var(ri), equals 0.2, consistent with weak empirical predictability. Results for\nR2 = 0.5 and 0.8 are similar and are omitted.\nFigure 6: Interval Length for Time-Series Data with Homogeneous Variance (n=100)\n0\n2\n4\n6\n8\nEqual (full)S\u2212AIC (full)S\u2212BIC (full)\nRegression (full)\nMMA (full)JMA (full)Equal (split)S\u2212AIC (split)S\u2212BIC (split)\nRegression (split)\nMMA (split)JMA (split)\nMethod\nInterval length\nNon\u2212adaptive Adaptive\nInterval Length for Time Series Data, n = 100\nBars show mean interval length \u00b1 one standard error.\nWe consider two sample sizes,n= 100 andn= 200, with split-sample training sets of 50\nand 100 observations, respectively. The number of models follows the rule in Hansen (2007),\nM= round(3n 1/3). Them-th candidate model is\nri+1 =\u03b4+\nm+1X\nj=1\n\u03b2jxj,i +u i+1.\nFigures 5 and 6 report coverage and interval length for the time-series design (n= 100).\nCoverage remains at or above the 90% target across all methods, with little distinction between\nadaptive and non-adaptive versions. Interval length shows the expected small loss from sample\nsplitting, but the gap between adaptive and non-adaptive intervals is much smaller compared\n35",
        "3bd96605-63e9-4d1b-abd3-24ea8ed5d888": "with the exchangeable case. This reflects the design: the AR(1)\u2013GARCH(1,1) model requires\nfar fewer parameters to estimate than the high-dimensional specification in the exchangeable\ncase, so local adaptation adds less variability. Overall, adequate coverage is observed even for\nsuch a small sample size, splitting costs are small, and the efficiency penalty from adaptation is\nmilder in the time-series setting. Among the methods, however, the regression-based approaches\nyield noticeably wider intervals and somewhat lower coverage, a pattern that warrants further\ninvestigation.\nFigure 7: Coverage for Time-Series Data with Homogeneous Variance (n=200)\n80%\n85%\n90%\n95%\nEqual (full)S\u2212AIC (full)S\u2212BIC (full)\nRegression (full)\nMMA (full)JMA (full)Equal (split)S\u2212AIC (split)S\u2212BIC (split)\nRegression (split)\nMMA (split)JMA (split)\nMethod\nCoverage\nNon\u2212adaptive Adaptive\nCoverage for Time Series Data, n = 200\nRed dashed: target 90% coverage.\nFigures 7 and 8 present results for the time-series design withn= 200. Coverage remains\nclose to or above the 90% target and is more stable than in then= 100 case. Interval lengths\nagain show a slight cost from sample splitting and a modest gap between adaptive and non-\nadaptive methods. Regression-based methods still yield wider intervals and lower coverage,\nthough the gap is smaller than in then= 100 case. Results forn= 400 (omitted) show further\nimprovements along the same lines.\nTaken together, these results show that the methods achieve adequate coverage in a challeng-\ning setting with small samples, model misspecification, biased parameter estimates, persistent\nregressors, and many predictors. Further work on their performance under stochastic volatility\nis currently in progress.\n36",
        "e9030ff9-2710-48bf-b2a1-e028b97de27d": "Figure 8: Interval Length for Time-Series Data with Homogeneous Variance (n=200)\n0\n2\n4\n6\nEqual (full)S\u2212AIC (full)S\u2212BIC (full)\nRegression (full)\nMMA (full)JMA (full)Equal (split)S\u2212AIC (split)S\u2212BIC (split)\nRegression (split)\nMMA (split)JMA (split)\nMethod\nInterval length\nNon\u2212adaptive Adaptive\nInterval Length for Time Series Data, n = 200\nBars show mean interval length \u00b1 one standard error.\n8 Empirical Applications\nSince the proposed algorithms are intended for both cross-sectional and time-series applications,\nwe consider two empirical applications in these respective settings to evaluate their performance.\nWe do not focus on efficient prediction, but on coverage properties in complex situations with\nlimited sample sizes and either many candidate models or many potential predictors, as often\nencountered in empirical work.\n8.1 Real estate appraisal\nHousing is the largest asset for most households. Many predictors of housing values, such as\nlocation, proximity to facilities, house age, and transaction date, have been proposed, but their\neffects are often uncertain and vary across datasets. Model averaging offers a natural way to\naddress this issue and potentially improve predictive accuracy. Yet accurate prediction is only\npart of the picture: a point estimate of 1 million with a 95% interval of [0.9, 1.1] has very\ndifferent implications than [0.5, 1.5]. In this section, we illustrate how our methods can provide\nboth point estimates and prediction intervals to capture this uncertainty.\nWe consider housing prices in Taipei using the dataset of Yeh and Hsu (2018), available\nthrough the UC Irvine Machine Learning Repository. The dataset contains 414 observations,\n37",
        "bc42a5fd-25d4-44eb-a405-8e1901a3eb70": "Table 1: Real estate appraisal with 95% and 90% prediction intervals\n95% 90%\nMethod RMSPE HitNon-adaptive Adaptive Non-adaptive Adaptive\nCoverage Ave.Len Sd.Len Coverage Ave.Len Sd.Len Coverage Ave.Len Sd.Len Coverage Ave.Len Sd.Len\nEqual (full) 9.28 0.64 0.95 32.67 0.202 0.95 33.99 8.790 0.90 25.90 0.336 0.90 26.43 6.773\nEqual (split) 0.96 33.87 0.324 0.94 32.66 8.513 0.90 25.34 0.135 0.90 25.85 6.747\nReg (full) 8.94 0.70 0.95 31.57 0.446 0.95 34.28 10.188 0.90 23.65 0.316 0.90 25.30 7.567\nReg (split) 0.95 32.92 0.428 0.95 34.19 6.860 0.91 23.88 0.190 0.90 22.25 4.368\nSAIC (full) 8.93 0.70 0.95 31.54 0.358 0.95 33.58 9.532 0.90 23.64 0.256 0.90 25.09 7.231\nSAIC (split) 0.96 33.47 0.517 0.95 32.18 6.664 0.91 24.45 0.284 0.89 22.62 4.572\nSBIC (full) 8.93 0.70 0.95 31.72 0.369 0.95 33.38 8.960 0.90 23.61 0.269 0.90 24.63 6.832\nSBIC (split) 0.96 34.22 0.608 0.95 32.12 7.690 0.91 24.43 0.091 0.90 23.40 5.502\nMMA (full) 8.93 0.70 0.95 31.46 0.478 0.95 33.74 8.443 0.90 23.76 0.249 0.90 24.33 6.068\nMMA (split) 0.96 32.85 0.416 0.95 33.95 8.117 0.91 24.63 0.120 0.90 24.06 5.786\nJMA (full) 8.93 0.70 0.95 31.37 0.511 0.95 34.07 8.279 0.90 23.80 0.264 0.90 24.34 5.941\nJMA (split) 0.95 32.64 0.347 0.95 33.71 7.622 0.91 24.38 0.121 0.90 24.23 5.419\nLG (full) 8.94 0.70 0.95 31.57 0.447 0.95 34.29 10.186 0.90 23.65 0.316 0.91 25.31 7.586\nLG (split) 0.95 32.92 0.428 0.95 34.20 6.877 0.91 23.88 0.190 0.90 22.25 4.371\nNotes: RMSPE is root mean squared prediction error. Coverage is the empirical coverage probability of the prediction intervals. Ave.Len and Sd.Len\nare the average and standard deviation of interval lengths. \u201cFull\u201d and \u201csplit\u201d indicate whether the full training sample is used or split into halves (for\nparameter and weight estimation).",
        "57e90a2b-16d5-4b89-9ef5-cc319088d526": "Coverage is the empirical coverage probability of the prediction intervals. Ave.Len and Sd.Len\nare the average and standard deviation of interval lengths. \u201cFull\u201d and \u201csplit\u201d indicate whether the full training sample is used or split into halves (for\nparameter and weight estimation). \u201cAdaptive\u201d intervals adjust for serial dependence and heteroskedasticity; \u201cnon-adaptive\u201d intervals do not. Equal =\nequal weighting, Reg = the regression approach, SAIC = smoothed AIC weights, SBIC = smoothed BIC weights, MMA = Mallows model averaging,\nJMA = jackknife model averaging, LG= the largest model.\n38",
        "8bf7aff9-a4ae-44a7-bdbd-171c953daa1a": "with the outcome variable defined as the residential housing price per unit area. Prices were\ncollected from the Ministry of the Interior\u2019s public database for the period June 2012 to May\n2013, covering two districts in Taipei City and two districts in New Taipei City. Six appraisal\npredictors are included: distance to the nearest MRT station, number of convenience stores\nwithin walking distance, house age, transaction date, and geographic coordinates (latitude and\nlongitude). See Yeh and Hsu (2018) for details on the construction of these variables. For\nprediction, we use a leave-one-out procedure: each property is in turn treated as the test\nobservation, with the remaining 413 serving as the training sample. We also experiment with\nrandomly selecting 100 test observations, and the results are similar.\nWe average over alternative candidate models that differ in their choice of covariates. Specif-\nically, we estimate the following regressions\nyi =\u03b2 \u2032\nmx(m)\ni +e i,\nwherey i denotes the transaction price per unit area for thei-th property, andx (m)\ni is a vector\nof potential predictors of its price, which always contains the intercept but differs in observable\ncharacteristics. We consider all possible combinations of six appraisal factors, leading to 63 =\n26 \u22121 candidate models. Predictions from these models are combined using six model averaging\nmethods: equal weighting, regression-based weighting, SAIC, SBIC, MMA, and JMA. For\ncomparison, we also report results from the largest model containing all predictors, a common\npractice in the price-prediction literature. The prediction interval for the largest model is\ncomputed as in the general case by setting the weight on the largest model to 1 and all others\nto 0.\nWe evaluate the methods through 95% and 90% coverage and average and standard error\nof the interval length. We also report root mean squared prediction error (RMSPE) and 20%\nhit rate of the point prediction, both standard criteria in house appraisal studies, and defined\nas\nRMSPE =\nvuut 1\nN\nNX\ni=1\n1(\u02c6yi \u2212y i)2,and Hit = 1\nN\nNX\ni=1\n1\n\u0012|\u02c6yi \u2212y i|\nyi\n\u2264\u03c4\n\u0013\n,\nwhere\u03c4= 0.2 is the tolerance threshold. Reported statistics are averages over all 414 evaluation\npoints and are reported in Table 1.\nWe find that the RMSPE of point predictions is broadly similar across methods, with the\nnotable exception of equal-weight averaging. Equal weighting underperforms because most\npredictors are informative and larger models generally produce more accurate forecasts; giving\nthe same weight to both small and large models leads to suboptimal results. The other methods\nperform comparably to the largest model, in some cases achieving slight improvements. This is\n39",
        "beb17477-5802-44c4-a6e2-a7b72715d345": "encouraging because it shows that model averaging tilts the weights toward richer models even\nwith 63 candidates to sift through.\nAll methods achieve coverage close to the nominal levels, with deviations at most 1%.\nSample splitting only slightly increases interval length, consistent with our simulations. With\nadaptive conformal methods, intervals are only mildly wider on average, suggesting a small\nefficiency loss. Meanwhile, the variation in interval lengths is strikingly different: without\nadaptation, lengths are essentially constant across predictions, whereas with adaptation their\nstandard deviation is often comparable to the RMSPE. This is not a defect. By construction,\nadaptive intervals are meant to vary with the conditional variance of the residual, and here\nsome covariates help explain that variance. In particular, a simple plot of interval length\nagainst property value shows narrower intervals for lower-priced properties and wider intervals\notherwise, a desirable feature from a valuation perspective.\nTo summarize, for this cross-sectional application, coverage is accurate when averaging over\na large number of models that include both nested and non-nested specifications. The resulting\nweights are informative, tilting toward richer and more predictive specifications. Split-sample\nprocedures achieve efficiency comparable to full-sample procedures at much lower computational\ncost. Finally, adaptive intervals yield shorter intervals where variance is low and longer where\nit is high, while preserving nominal coverage and only mildly increasing average length. We\nnext consider a time series application.\n8.2 Equity premium prediction\nPrediction of the equity premium (the difference between equity returns and the risk-free rate)\nhas long been a central topic in finance. Many predictors have been proposed, each showing\npredictive power in some contexts, but none consistently dominates. A natural consideration\nis to apply model averaging across predictors, while quantifying prediction uncertainty. To our\nknowledge, however, no studies have offered frequentist prediction intervals in this setting. Our\nanalysis provides such results.\nWe use the dataset and model specifications from Goyal et al. (2024), which reexamined\n29 predictors proposed in recent papers (listed their Table 1), of which 20 are available at the\nmonthly frequency. Our analysis includes all 20 monthly predictors. Their study, as is common\nin this literature, focused on bivariate regressions that include a single predictor at a time:\nrt+1 =\u03b1 i +\u03b2 ixi,t +u i,t+1,\nwherer t+1 denotes the equity premium (the value-weighted S&P 500 return including dividends\nminus the one-month T-bill rate), andx i,t is predictori, as defined in Table 2 of Goyal et al.\n40",
        "8576696e-a17c-485a-96aa-97500940ef92": "(2024). We adopt the same practice, obtaining 20 candidate models for averaging. The sample\nperiod follows their study but begins in January 1996 to ensure all predictors are available,\nproducing a balanced panel of 312 monthly observations through December 2021, with forecasts\nfrom all candidate models at each point.\nWe use a rolling-window design common in this literature. A fixed sample of 212 observations\nis moved forward one period at a time, with the subsequent observation serving as the prediction\ntarget, which produces 100 predictions in total. For methods involving sample splitting, half\nof the 212 observations are used for estimation. For all methods, we report the mean squared\nprediction error (MSPE) of the point prediction, the 95% and 90% coverage rate, and the mean\nand standard deviation of interval lengths. All statistics are averaged over the 100 predictions.\nTable 2 shows that equal weighting achieves the lowest MSPE, and the remaining MSPEs\nare similar. This general pattern suggests that the uncertainty associated with estimating 19\nmodel weights outweighs the potential gains from unequal weighting, even if such weighting\nwould be optimal in principle.\nMore central to our analysis are the coverage properties of the prediction intervals. All\nmethods deliver coverage close to the nominal level: between 0.94 and 0.96 at the 95% level,\nand between 0.90 and 0.91 at the 90% level. Average interval lengths are also similar across\nmethods, roughly 0.18\u20130.19 for 95% intervals and 0.14\u20130.15 for 90% intervals.\nA sharper difference appears in the variability of interval lengths. The split-sample versions\nconsistently show larger standard deviations, in some cases much higher than those of the full-\nsample versions. This arises from dividing the estimation sample, which increases variation even\nthough average lengths remain similar. Adaptive intervals are slightly more variable than their\nnon-adaptive counterparts, suggesting limited gains or losses from allowing adaptivity, since the\nrolling-window scheme already provides some adjustment. These findings are consistent with\nthe simulation evidence documented in the previous section.\nTaken together, the results suggest that model averaging yields prediction intervals with\nreliable coverage in this time-series setting and broadly comparable efficiency across methods.\nEqual weighting performs favorably relative to more elaborate schemes, though in settings with\nfewer models or larger samples, unequal weighting may perform better. Our methods can\nreadily be applied to such settings without modification.\n9 Conclusion\nWe have proposed prediction intervals for general model averaging using conformal inference.\nThese intervals cover out-of-sample realizations of the outcome variable with a chosen proba-\n41",
        "db6af6d2-41e4-4053-a495-5cf0a16add41": "Table 2: Equity premium prediction with 95% and 90% prediction intervals\nMSPE 95% coverage 90% coverage\nNon-adaptive Adaptive Non-adaptive Adaptive\nCoverage Ave.Len Sd.Len Coverage Ave.Len Sd.Len Coverage Ave.Len Sd.Len Coverage Ave.Len Sd.Len\nEqual (full) 0.0016 0.95 0.18 0.004 0.95 0.18 0.004 0.91 0.15 0.003 0.91 0.15 0.003\nEqual (split) 0.94 0.17 0.014 0.94 0.18 0.010 0.90 0.15 0.017 0.91 0.15 0.015\nReg (full) 0.0020 0.96 0.18 0.015 0.94 0.18 0.017 0.91 0.14 0.015 0.90 0.15 0.014\nReg (split) 0.96 0.19 0.021 0.95 0.20 0.029 0.91 0.15 0.021 0.91 0.15 0.024\nSAIC (full) 0.0020 0.94 0.17 0.016 0.94 0.18 0.017 0.91 0.14 0.015 0.91 0.15 0.014\nSAIC (split) 0.95 0.18 0.018 0.95 0.19 0.017 0.91 0.15 0.016 0.91 0.15 0.016\nSBIC (full) 0.0020 0.94 0.17 0.016 0.94 0.18 0.017 0.91 0.14 0.015 0.91 0.15 0.014\nSBIC (split) 0.95 0.18 0.018 0.95 0.19 0.017 0.91 0.15 0.016 0.91 0.15 0.016\nMMA (full) 0.0020 0.96 0.18 0.015 0.96 0.18 0.016 0.91 0.14 0.015 0.91 0.15 0.014\nMMA (split) 0.96 0.19 0.021 0.95 0.19 0.023 0.91 0.15 0.016 0.91 0.15 0.020\nJMA (full) 0.0019 0.95 0.18 0.014 0.94 0.18 0.014 0.91 0.14 0.015 0.91 0.15 0.013\nJMA (split) 0.96 0.19 0.018 0.95 0.19 0.020 0.90 0.15 0.020 0.90 0.15 0.020\nRW (full) 0.0016 0.95 0.18 0.003 0.95 0.18 0.002 0.93 0.15 0.004 0.91 0.15 0.003\nRW (split) 0.95 0.17 0.014 0.94 0.18 0.010 0.90 0.15 0.017 0.90 0.15 0.015\nNotes: MSPE is mean squared prediction error. Coverage is the empirical coverage probability of the prediction intervals. Ave.Len and Sd.Len are\nthe average and standard deviation of interval lengths. \u201cFull\u201d and \u201csplit\u201d indicate whether the full training sample is used or split into halves (for\nparameter and weight estimation).",
        "28e70dc2-19a8-4abe-b9a5-a26ff0d30d19": "Coverage is the empirical coverage probability of the prediction intervals. Ave.Len and Sd.Len are\nthe average and standard deviation of interval lengths. \u201cFull\u201d and \u201csplit\u201d indicate whether the full training sample is used or split into halves (for\nparameter and weight estimation). \u201cAdaptive\u201d intervals adjust for serial dependence and heteroskedasticity; \u201cnon-adaptive\u201d intervals do not. Equal =\nequal weighting, Reg = the regression approach, SAIC = smoothed AIC weights, SBIC = smoothed BIC weights, MMA = Mallows model averaging,\nJMA = jackknife model averaging, RW = random walk. Note that SAIC=SBIC because all models contain the same number of regressors.\n42",
        "5a3a2859-ac46-4c15-bf7a-b66ac1a93941": "bility, thereby quantifying predictive uncertainty beyond point prediction. A point prediction\nwithout a measure of uncertainty offers limited guidance for empirical analysis and decision\nmaking. Our framework allows for general model misspecification and applies to averaging\nacross multiple candidate models\u2014nested, disjoint, overlapping, or any combination\u2014with\nweights that may depend on the estimation sample. We established coverage guarantees un-\nder two sets of assumptions: exact finite-sample validity under exchangeability, relevant for\ncross-sectional data, and asymptotic validity under stationarity, relevant for time series. We\ndescribed a benchmark algorithm, along with locally adaptive refinements and split-sample\nprocedures that broaden the scope of applications. We also illustrated the methods across a\nwide range of model averaging schemes, including fixed weights, weights determined by OLS\nregressions, smoothed information criteria, the Mallows criterion, jackknife weighting, Bayesian\nmodel averaging, and bootstrap aggregation.\nTaken together, these results show that valid interval prediction for model averaging is\nfeasible even when the underlying models are misspecified and the data exhibit dependence.\nBeyond providing a practical tool for empirical work, the study also suggests open questions\nfor conformal methods in dependent data settings, including extensions to more complex forms\nof serial correlation, as well as to high-dimensional model averaging.\nReferences\nAndrews, D. W. K. (1991). Heteroskedasticity and autocorrelation consistent covariance matrix\nestimation.Econometrica 59(3), 817\u2013858.\nAndrews, D. W. K. (2003). End-of-sample instability tests.Econometrica 71(6), 1661\u20131694.\nBarber, R. F., E. J. Cand` es, A. Ramdas, and R. J. Tibshirani (2023). Conformal prediction\nbeyond exchangeability.The Annals of Statistics 51, 816\u2013845.\nBoudoukh, J., M. Richardson, and R. F. Whitelaw (2008). The myth of long-horizon pre-\ndictability.The Review of Financial Studies 21(4), 1577\u20131605.\nBreiman, L. (1996). Bagging predictors.Machine Learning 24(2), 123\u2013140.\nBreiman, L. (2001). Random forests.Machine Learning 45(1), 5\u201332.\nBuckland, S. T., K. P. Burnham, and N. H. Augustin (1997). Model selection: An integral part\nof inference.Biometrics 53, 603\u2013618.\n43",
        "e913add9-9a50-4fc4-9f16-0fb36c1e4cde": "Chernozhukov, V., K. Wuthrich, and Y. Zhu (2018). Exact and robust conformal inference\nmethods for predictive machine learning with dependent data. InProceedings of the 31st\nConference On Learning Theory, Volume 75 ofProceedings of Machine Learning Research,\npp. 732\u2013749. PMLR.\nChernozhukov, V., K. W\u00a8 uthrich, and Y. Zhu (2021, November). Distributional conformal\nprediction.Proceedings of the National Academy of Sciences 118(48).\nClaeskens, G., J. R. Magnus, A. L. Vasnev, and W. Wang (2016). The forecast combination\npuzzle: A simple theoretical explanation.International Journal of Forecasting 32, 754\u2013762.\nGoyal, A., I. Welch, and A. Zafirov (2024). A comprehensive 2022 look at the empirical perfor-\nmance of equity premium prediction.The Review of Financial Studies 37(11), 3490\u20133557.\nGranger, C. W. J. and R. Ramanathan (1984). Improved methods of combining forecasts.\nJournal of Forecasting 3, 197\u2013204.\nHansen, B. E. (2007). Least squares model averaging.Econometrica 75, 1175\u20131189.\nHansen, B. E. (2008). Least-squares forecast averaging.Journal of Econometrics 146(2),\n342\u2013350.\nHansen, B. E. and J. Racine (2012). Jacknife model averaging.Journal of Econometrics 167,\n38\u201346.\nHansen, P. R. and A. Lunde (2005). A forecast comparison of volatility models: Does anything\nbeat a garch(1,1)?Journal of Applied Econometrics 20, 873\u2013889.\nHastie, T., R. Tibshirani, and J. Friedman (2009).The Elements of Statistical Learning: Data\nMining, Inference, and Prediction(2 ed.). New York: Springer.\nHjort, N. L. and G. Claeskens (2003). Frequentist model average estimators.Journal of the\nAmerican Statistical Association 98(464), 879\u2013899.\nHoeting, J. A., D. Madigan, A. E. Raftery, and C. T. Volinsky (1999). Bayesian model averaging:\nA tutorial.Statistical Science 14(4), 382\u2013401.\nInoue, A. and L. Kilian (2008, June). How useful is bagging in forecasting economic time\nseries? a case study of U.S. consumer price inflation.Journal of the American Statistical\nAssociation 103(482), 511\u2013522.\n44",
        "3709cdd2-2595-49b5-bb29-090e81c8b07b": "Lei, J., A. R. Max G\u2019Sell, R. J. Tibshirani, and L. Wasserman (2018). Distribution-free predic-\ntive inference for regression.Journal of the American Statistical Association 113, 1094\u20131111.\nLiao, J., G. Zou, Y. Gao, and X. Zhang (2021). Model averaging prediction for time series\nmodels with a diverging number of parameters.Journal of Econometrics 223, 190\u2013221.\nRaftery, A. E., D. Madigan, and J. A. Hoeting (1997). Bayesian model averaging for linear\nregression models.Journal of the American Statistical Association 92(437), 179\u2013191.\nRomano, Y., E. Patterson, and E. Candes (2019). Conformalized quantile regression.Advances\nin neural information processing systems 32.\nSin, C.-Y. and H. White (1996). Information criteria for selecting possibly misspecified para-\nmetric models.Journal of Econometrics 71(1-2), 207\u2013225.\nStock, J. H. and M. W. Watson (2006). Forecasting with many predictors. In G. Elliott,\nC. W. J. Granger, and A. Timmermann (Eds.),Handbook of Economic Forecasting, Volume 1\nofHandbooks in Economics, Chapter 10, pp. 515\u2013554. Elsevier.\nTibshirani, R. J., R. Foygel Barber, E. Candes, and A. Ramdas (2019). Conformal prediction\nunder covariate shift.Advances in neural information processing systems 32.\nTimmermann, A. (2006). Forecast combinations. In G. Elliott, C. W. J. Granger, and A. Tim-\nmermann (Eds.),Handbook of Economic Forecasting, Volume 1, pp. 135\u2013196. Elsevier.\nVovk, V., A. Gammerman, and G. Shafer (2005).Algorithmic Learning in a Random World.\nSpringer.\nWhite, H. (1982). Maximum likelihood estimation of misspecified models.Econometrica 50(1),\n1\u201325.\nWhite, H. (1984).Asymptotic Theory for Econometricians. Orlando: Academic Press.\nWhite, H. (1994).Estimation, Inference and Specification Analysis. Cambridge: Cambridge\nUniversity Press.\nWhite, H. (2001).Asymptotic Theory for Econometricians(Revised Edition ed.). San Diego:\nAcademic Press.\nYeh, I.-C. and T.-K. Hsu (2018). Building real estate valuation models with comparative\napproach through case-based reasoning.Applied Soft Computing 65, 260\u2013271.\n45",
        "8ec17857-976b-47b4-a7be-bfab6b488e37": "Zhang, X., A. T. Wan, and G. Zou (2013). Model averaging by jackknife criterion in models\nwith dependent data.Journal of Econometrics 174, 82\u201394.\nZhang, X., G. Zou, H. Liang, and R. J. Carroll (2020). Parsimonious model averaging with a\ndiverging number of parameters.Journal of the American Statistical Association 115(530),\n972\u2013984.\n46",
        "f2fde255-1fa3-4336-b31b-69b41693147f": "supchains.com \nnicolas.vandeput@supchains.com \nHow to Forecast New Products: Challenges and Solutions \nThis article tackles the challenge of forecasting new products . First, w e divide  product \nintroduction s into three cases : new version, line extension, and brand -new products . Then \npropose methods to forecast  demand in each case .  \nBusinesses like to renew their product portfolio by releasing new versions of existing products or \nlaunching brand-new products. By providing new value propositions for consumers, companies hope \nto increase (or secure) market share and enjoy overall sales and profit growth. Unfortunately, \nforecasting new products is especially challenging for demand planners, marketing teams, and \nstatistical forecasting models. This problem is specifically serious for the fashion an d IT industries as \nwell as retailers, as they renew their products regularly.  \nSegmentation \nLet\u2019s tackle this challenge by dividing product introductions into three sub-cases: \n- #1. New Version: New product launch replacing existing products (clear predecessors).  \n- #2. Line Extension : New product added on top of the current  lineup (with a few similar \nproducts). \n- #3. Brand-new Product: New product without predecessors. \nTable 1 Examples of product launches. \n With replacement Without replacement  \nWith predecessor(s) #1. New version \niPhone 12 -> iPhone 13 \n#2. Line extension \nNew color for the iPhone \nWithout predecessor(s)  #3. Brand-new product \nApple launches the iMotor  \n \nCase 1: New Version (Replacing Predecessors) \nLaunching a new version of an existing product is the simplest case of product introduction: you can \nsimply copy relevant data from the older product to the new one  (relevant data typically includes \nforecasts, historical demand, and historical demand drive rs such as pricing, promotions, and \nshortages). We also say that you use the older product\u2019s historical data as a proxy for the new one. This \nwill give you a good starting point for estimating the future demand for this new version.  \n \nFigure 1 Example of product transition.",
        "d9614bd5-412c-4847-930f-9b5d8098095f": "supchains.com \nnicolas.vandeput@supchains.com \nHowever, when forecasting the introduction of a new version, you should also consider possible launch \neffects, such as marketing , promotions, and timing , as well as  discontinuation effects for the older \nversion (such as price discounts until the stocks are depleted), and the risk of customers not adopting \nthe innovation. \nTo do so, you can gather insights from various teams (marketing, sales, and product management). Or \nrely on  machine learning models: these advanced forecasting models can accurately predict sales \nuplifts resulting from promotions, features, and price changes(see this case study for an example). In \nany case, a model-made forecast can always be enriched by humans.  \nTo help you understand how this works in practice, consider the following example: Apple releases a \nnew iPhone every year, with the latest version typically being called the iPhone X+1. To forecast sales \nof the new iPhone X+1, Apple uses historical sales data of the previous iPhones and copies the forecast \nfor the iPhone X to the new version. However, they also need to consider any promotional activity that \nmight affect demand for the new iPhone, such as discounts on the older iPhone model or a new \nadvertising campaign. By taking these factors into account (either in their forecast engine or in their \njudgmental forecasts), they can adjust the forecast to predict demand for the new product more \naccurately. \nCase 2: Line Extension \nWhen introducing a new product to your existing product line, you need to forecast its demand and \nassess its impact on the sales of your current products (a phenomenon known as cannibalization).  \nFor example, let\u2019s imagine you are selling your best -selling product (the famous myPhone) in two \ncolors: black and white. You want to extend your offering by introducing a blue version. This product \nintroduction will cause,   \n\u2022 Overall, slightly higher sales for the myPhones as you will attract more clients thanks to more \noptions. \n\u2022 Lower sales for both existing versions (black and white) as some clients prefer the blue version. \nLet\u2019s discuss these two points in detail. \nForecasting Demand for the New Product  \n There are different ways to forecast the sales of the new product. You can, \n1. Find similar products in your portfolio (based on relevant features) and use their historical \n(demand) data as a proxy for the new product. For example, if you are a retail store, adding a \nnew LED strip to your offering will require you to assess its similarity with the ones already \ndisplayed in your store. To evaluate this similarity, relevant features could be co lors, length, \nconnectivity, app integration, and price. However, identifying similar products (and allocating \nsimilarity scores) is a time-consuming and tedious task for humans, who will often overlook it. \nUnfortunately, Finding similar products is also challenging for usual machine learning models.  \n2. Use machine learning models to forecast future demand by feeding them with relevant product \ncharacteristics. This method can work well if you manage a business with many product \nintroductions and specific features, such as in the fashion industry.  \n3. You can also forecast the overall sales of the product family (rather than each product \nseparately). This can help you better understand the impact of the new product on the segment \nas a whole.  Unfortunately, this forecasting technique is not likely to result in accurate \npredictions, mainly because most top -down disaggregation techniques aren\u2019t accurate.",
        "b900fee9-c0a5-4443-b990-c7e26ca1e6f5": "supchains.com \nnicolas.vandeput@supchains.com \nNevertheless, this first statistical baseline could be a starting point for a conversation with sales \nand marketing. \nAssess the Impact on Existing Products (Cannibalization) \nYou can rely on expert opinion to estimate the cannibalization effect. One way to do this would be to \norganize dedicated workshops and leverage the wisdom of the crowd . Alternatively, you can use \nmachine learning models that can learn demand and cannibaliz ation patterns from previous (similar) \nproduct introductions. \nCase 3: Introducing Brand-New Products \nForecasting sales for brand-new products without any existing similar products is the most challenging \ncase.  \nUnfortunately, even if technologies and models are evolving, statistical and machine learning models \nwill struggle to make relevant forecasts beca use they can\u2019t rely on similar products to generate \npredictions. \nInstead, you will have to use judgmental forecasts. Be cautious that relying on judgmental forecasts can \nbe risky as team members may be prone to intentional and unintentional biases. Here are a few critical \nsteps to ensure objectivity and get unbiased forecasts:  \n- Work with your colleagues to identify intentional biases  such as enforcing, sandbagging, or \nhedging (I discuss these in detail in my book, Demand Forecasting Best Practices). You will have \nto discuss and solve their specific fears and biases one by one.  \n- Once intentional biases have been overcome, organize specific workshop sessions to leverage \nthe wisdom of the crowd  to its full potential. During these sessions, s pecific insights, such as \nfeedback from pilot trials as well as market and competition analysis , can be shared with the \nrest of the team. When the product hits the market, usual machine learning models can take \nover from the initial judgmental forecast and update the demand prediction regularly.  \nConclusion \nBusinesses like introducing new products to increase market share and get an edge over the \ncompetition. Unfortunately, forecasting demand for new products can be especially c hallenging and \nshould be one of the priorities of demand planners during the forecast enrichment process.  \nWe tackled the challenge of forecasting demand for new products by dividing it into three cases (new \nversion, line extension, and brand-new products) and solving each with different approaches. \n- New version: use the demand of the predecessor as a proxy for the forecast. Don\u2019t forget to \naccount for launch effects. \n- Line extension: use machine learning models to find similar products, then create the new \nforecast using a proxy history and assess the cannibalization on similar products. \n- Brand-new products: use judgmental forecasts and the wisdom of the crowd."
    },
    "relevant_docs": {
        "09af3cb7-62fb-4f7d-92a1-2afcd5fd24a4": [
            "583470eb-5d1f-40c5-9be4-61ea684c0553"
        ],
        "39e45d81-3ca3-4405-b199-5493d7dd4809": [
            "b5b6f4eb-de8d-4e80-9537-59495ba7ad88"
        ],
        "8e86acec-befb-46c8-8c5a-b1afbf27385e": [
            "968a6d8a-91f7-402f-88d1-ffedabea44a0"
        ],
        "389b4347-6f1d-49b7-a8e0-650af9aa11f7": [
            "d5d9ff3a-745f-41a2-9655-0dfcc8a2f008"
        ],
        "c88fafe6-e9a9-410c-be1f-98311b68636a": [
            "2edc00c7-f6e2-41ce-aabf-0a0178ae03a1"
        ],
        "3dbdc20d-1b84-495d-abbc-d6b775ec8272": [
            "949a4ed3-9ba1-4303-a517-c867d4cef9fa"
        ],
        "21b8d086-cb8a-4ea2-bf10-60088515edfa": [
            "6123b9cb-0b4b-4d63-b9a2-57dea9344e18"
        ],
        "6f09f150-2d5a-4a99-9891-0f58e6973479": [
            "c8a2a07d-7293-49b7-8411-fb5ffe6f1b86"
        ],
        "77a09d1e-ac49-4ac4-9cd1-a53c5851111f": [
            "e264403c-3f0e-4789-bd16-39bea5601256"
        ],
        "241e152b-7e49-4a8c-97d3-5c3fe3ff4da1": [
            "b30d68e4-286a-48f7-bf2b-5a95e4ed5750"
        ],
        "ad1f31a5-9e06-4aa8-941e-7035f657ea56": [
            "ab3b95c9-7f6d-4822-bc6e-602585a9aa26"
        ],
        "effbc2cf-0203-4890-af6f-349e5c776563": [
            "4e26a6e0-d781-4bb2-93c4-942b178d1b8e"
        ],
        "fac1a2bf-fd74-465b-8e01-41ea12cc05c1": [
            "664b3ca3-eb19-416a-b0dc-dbcce5aa662b"
        ],
        "89b40ab9-77c9-4fab-92e3-b5de96cf4b79": [
            "f70a61a1-c1e0-4e96-a604-5f653916144d"
        ],
        "582b415f-4333-4bf8-a53f-34a674dba3f7": [
            "0fb1e234-72f9-477e-abd8-abc57e286ca6"
        ],
        "2b607b06-2bbb-4745-9c40-3a34b3f79040": [
            "2a27f39c-a501-414f-b151-e9924ce81180"
        ],
        "d9024f08-70df-44b5-afd8-38a389903d72": [
            "743144c7-22a3-4d1d-a9c1-0ad2b9effb2b"
        ],
        "4c19eeb7-d081-4967-a79f-183d366899bc": [
            "809458dd-bea5-42f0-af35-bd20a1521da7"
        ],
        "e1dd2ce4-727f-4631-a101-c73127ea07d4": [
            "0a9dec01-cf05-41a2-970c-b3f4479e986c"
        ],
        "292b0b00-e11c-4e56-9170-e40f5c46b1fc": [
            "1757a188-51b7-41e6-8b11-b85ecc638e4e"
        ],
        "c8bc2709-8e59-449d-aeae-149eae441ae3": [
            "e5682f5b-f8a9-4125-92ec-542c6a2d336b"
        ],
        "5697015c-2091-45e6-89d1-0b646348d6ae": [
            "b33b57e6-fb46-417e-8e14-6cb495a6f602"
        ],
        "5c5b3c61-23e4-404d-84a7-878f21a00094": [
            "244d8390-3c98-4487-b416-1df75e7d915c"
        ],
        "ad962565-85c8-4da4-8aaa-1c1e02e6b9a1": [
            "8ae92c42-32f8-4b26-a844-1126f6cf6eda"
        ],
        "d83fe079-a94b-4880-adf1-bae29ed840ec": [
            "3c5c92e6-c8a3-4ad4-a5c0-f0101d1b7a3c"
        ],
        "502d27f8-8781-4e6a-9ac8-caa3b6293330": [
            "a1a6de1d-13a8-4da9-a677-11a4ad655b91"
        ],
        "ccdb0290-e5b6-4e51-b272-9da89e87f5cb": [
            "e2b52fe9-3b34-43b5-b2b8-e7e5fa7df693"
        ],
        "bc79daef-9504-4d79-acd4-0ba8eff965ec": [
            "5e64f834-ca5e-488a-bb63-e99521762ef3"
        ],
        "29808cde-dc50-4ae8-8f60-ee406c063730": [
            "9958311d-394b-482c-8528-c8a34d2c1075"
        ],
        "9f5c2f20-f22e-47a7-9c59-318f79ff78f4": [
            "a34a549a-f349-4d90-a938-9d5d5f56fdf2"
        ],
        "856f3a89-91d8-498f-a508-9344de5126b1": [
            "7a1e151e-6002-46cd-8c29-249fd4424162"
        ],
        "cefe7491-0d9d-4f04-aed7-9addb1f23d89": [
            "b0e00c06-c7c3-46f2-ab85-c0084e387f98"
        ],
        "244637a4-e942-4815-9d55-359c2e4b9369": [
            "191a4eb0-c36f-40aa-8f33-dcdab36f220f"
        ],
        "c8ec4850-c9fb-40c6-b7f2-6a331600d81c": [
            "bb80f00b-72fc-46f6-a47b-777ad90f379b"
        ],
        "4e34f0c1-6b56-4e06-b643-0519f4b72693": [
            "ca802da6-caf1-47db-8aab-c910f0ca88b5"
        ],
        "58774d08-cc65-4aed-bf16-ea7948df38c0": [
            "2d8c541d-f9df-4346-9089-a31d9fece198"
        ],
        "0493a894-3328-4cc8-880e-f7106f702e69": [
            "f3a3153b-9384-4ce8-aa3c-5da457f50906"
        ],
        "cba020d6-fc6b-4711-aa76-b6ba59c2a7da": [
            "f1f592fc-43b0-4ba2-aa9d-972bdd75febd"
        ],
        "5c368c9c-e745-49f5-9421-39606d1e3296": [
            "806de7af-3a9a-460d-9fbe-f7c74ce99a74"
        ],
        "62f8edd2-115c-4618-8842-4784e71216d2": [
            "f5900889-ad88-4ee9-aa60-f1f913993a39"
        ],
        "d36a47f5-e6ef-4300-9e01-c05669765493": [
            "00e11d7c-f189-41cb-adc4-fd827046f561"
        ],
        "3264614a-8ba8-44d9-8ff6-02bb5ee0bfd3": [
            "62be9298-7f8d-4ae2-aa0e-5e252af4a290"
        ],
        "a3572584-6c30-4412-91e6-60ed08aa289b": [
            "a817832b-89dc-4eca-8830-5f62a61cd72c"
        ],
        "8e1c3fa3-8e6b-464f-80f1-bd10e133d796": [
            "c3c228d2-e7c7-4e08-b9cf-30a669d42a8b"
        ],
        "f10523b0-c0cd-47c4-a975-87f109944888": [
            "76f23785-ff0b-433b-8948-797b0929ba71"
        ],
        "4b830c81-af12-4032-954a-f9e540156316": [
            "fa5d50f2-9db0-4fd2-b906-4d89acd9120c"
        ],
        "5bbd664c-f10c-4e85-acaf-ae37dc105017": [
            "1dd0fc41-f675-40ce-b5e9-bf94bec004af"
        ],
        "79b143c7-b937-463e-b7b0-cb163418aa5e": [
            "ab4094ed-1a3d-4623-92ff-e4f670f92e37"
        ],
        "cc93e5bb-7424-4d90-bcb0-a83decc3e9be": [
            "72efe35b-0c09-48c1-8155-a71ec9e315cd"
        ],
        "ff4d4a7b-4449-4192-a5a3-feefb076dc01": [
            "c4a3c71f-1cc7-41a0-87df-c170a45127ea"
        ],
        "a92ed316-c0ee-4cef-a8d9-e88e64855daf": [
            "bcc01933-bd64-495d-af2b-1ff81dc1f158"
        ],
        "ed73ab95-2822-4506-863f-f14b0ac4b797": [
            "32be630f-90be-45e8-97d8-abf5a357ad6c"
        ],
        "83841e33-52cc-4d05-8091-19154423577f": [
            "9d0ea767-eecf-4a89-a888-986b1dfd8d2d"
        ],
        "44bb95bd-6a3a-45e2-94fd-fd9f5114a96e": [
            "bc9e7cca-c5ad-4e20-ae12-445edbfae19e"
        ],
        "344eadcf-ad1a-41d8-9e9d-18f59ce1f2a8": [
            "90678aa4-28b3-40c7-bf35-c25f8e5777d1"
        ],
        "6b1fe7ec-fcbc-4430-9d6c-253fbcce2d87": [
            "13611d95-bfff-4fbc-b8fe-c8915f04aca9"
        ],
        "2b1e3002-8e1d-424d-8c93-1c05b4845f11": [
            "a6cf2ccf-3ad9-42ef-b989-eabd4924f6a2"
        ],
        "3793f4b4-04b8-4747-864e-ed5b09d89911": [
            "58239f41-5cc6-40ef-bd59-58f4bfe0adec"
        ],
        "44997ad5-04d3-475c-a411-b6922dceaba6": [
            "14b2b90d-2ad4-4129-b601-53bfdb43ada4"
        ],
        "5fcceda7-7888-45b9-a43d-1d6f3570b655": [
            "8f6fbc4b-8c57-4633-bb84-4f0ebd046956"
        ],
        "020611fd-adaf-497c-b951-31532b92a71a": [
            "b21cc981-3972-46f1-a86b-31f8ccc52300"
        ],
        "0dca39f3-1d86-49fe-88d1-c2ac0b30f28b": [
            "fb2304a6-e8ef-4008-8fee-3641ddada0c0"
        ],
        "0c6c339b-6f1b-46ac-ae9d-a5e9ad92a7ba": [
            "c2dbe4a9-46af-4d14-98d7-cf4c1d196c7d"
        ],
        "72e2f601-d940-44a2-aded-6d86d6e240ee": [
            "7e016e18-0809-44d6-9b8e-be38a478ebb3"
        ],
        "ddd539f2-3184-45ca-b43d-d5b794c819a3": [
            "e4a8dee1-4d2a-4e01-8af3-78f6f25576b5"
        ],
        "a8ff838b-3a07-4979-80cd-84bbe50a94e6": [
            "ab5f6c1a-cd01-4bb7-abf6-222c38ef34dd"
        ],
        "a1be058e-b352-4427-8eff-3950b429e1ce": [
            "f971683e-11a9-451c-828b-522656dd11b5"
        ],
        "787d7c77-8319-4df5-a7f3-e28ad27d7e94": [
            "8965224d-5e20-46d0-90c1-9b44598c18c0"
        ],
        "c072666c-d8ef-4cc5-945d-b6562876b39e": [
            "c4b926cc-2396-467f-b990-b5d16c9ad8d5"
        ],
        "4a3c126f-1d68-49fb-a9e5-c017c46d41cb": [
            "dfad6a64-17e2-4ee2-a3cc-cc5db72f686f"
        ],
        "36888c0e-8a40-47e3-80b1-25df552cbb39": [
            "72b450d1-b233-4ebd-badd-18121cffefe1"
        ],
        "e685e769-382d-4f25-93b0-7e700c252e49": [
            "ae050f1c-447b-4856-b4f6-8a39a46d09c2"
        ],
        "989e63a9-126e-4a5c-a740-de15b9c5bbd7": [
            "bda9d01d-7727-45b5-92c9-8344f6c91810"
        ],
        "6d09f340-4f06-4198-a3a3-22695802ab33": [
            "d595edc7-f54a-4d61-b0b9-9ffd95909a30"
        ],
        "6d23580b-a335-4233-916f-6a2e61f2c4b8": [
            "50aa5daf-fc0d-4b9b-9b5f-9acf5b3ebca2"
        ],
        "3520e313-d7c1-4a64-8c34-6c1e056f4f0d": [
            "51f6fd46-b0f1-4105-a25d-c9426cd0bf07"
        ],
        "73dc2856-fc3e-4f24-8a3d-8da82f94df3b": [
            "85c8b298-ecb6-4a17-8e36-13bce7cf2c1e"
        ],
        "915528af-c3f3-4532-bc2a-633fee1c9d12": [
            "ab70e7a6-b44b-4392-8797-cba0dd6f1178"
        ],
        "84c87b3a-3f7e-4f46-82f0-38f38f6d876e": [
            "71cceb93-c1c5-4ddf-851a-225c691419b2"
        ],
        "13742d7e-3669-4bdb-91ca-96a54c6a1cb6": [
            "9f46f9bb-58ff-4881-9ffd-c62631d2d13a"
        ],
        "85dff050-73e5-40a6-a1fe-8737dd8db5f3": [
            "02e34e08-6531-4430-844e-3d2fc6db3317"
        ],
        "3f158e77-0f6e-48c7-b5c5-858bc6464da3": [
            "a3fd74b1-d02b-4125-bebf-a07665c4cdb8"
        ],
        "0b3de7b7-7168-40f4-a011-7fa012fc2ba5": [
            "9ea3d45e-5442-49fe-91a2-2976b4f82474"
        ],
        "97f41b7b-1363-4442-9029-f50ce86d8275": [
            "73ba7aea-65cc-4cfb-92a4-8643a3372670"
        ],
        "9bc285be-8b5f-4321-9155-16e1cd8c47e2": [
            "a9a3cc98-4bd8-40c9-8f53-49913012ca37"
        ],
        "73ac4dd2-8743-4d40-ad3c-47f0698204c4": [
            "17c02e5f-eb32-4595-ab92-40b88133541c"
        ],
        "6930a6c2-8219-49cf-aac3-298191aff77d": [
            "002ff489-9ba9-4de0-85f8-8211e8041c1b"
        ],
        "12b202e5-fc75-4e39-a719-56acf60480b4": [
            "3fe020d7-543c-4c54-bb8a-28befaa1807f"
        ],
        "1e7248b3-45a6-42ea-89d3-5ecb04f9f2fe": [
            "ee7980fb-341d-4bc5-aef6-4a4abc954be3"
        ],
        "57a47a18-a394-41c0-9080-291ce2201581": [
            "518cf108-850c-4442-b991-c07fab6dc407"
        ],
        "913fbe80-93ee-4f49-b9fa-f733150aca51": [
            "549abd68-7012-45be-80c1-6a9b7fcf9888"
        ],
        "567d2b9e-79dc-4369-9c0e-b1a3d2e4d492": [
            "a582ce48-0a1d-4730-8ad6-84f9e66e87b2"
        ],
        "b738d220-30c9-43e9-8812-f2f7f541b8c3": [
            "62385e41-2deb-4f07-8a1d-cfbbb42c7bf7"
        ],
        "3d92a995-ce2b-4599-8fc8-29d2a27f88d7": [
            "367d6db0-8e41-42ee-a9f6-8bd0a874f253"
        ],
        "b61ac721-466f-4638-99be-34d261702082": [
            "7eea41de-6009-4342-8128-3f3da5ef4905"
        ],
        "2be77e43-c7fb-429a-91fe-226b9f4fa56e": [
            "1765e6b9-6ac3-4347-8e19-2b9b063cbb4f"
        ],
        "deaf8337-7088-4461-884c-87e2e0b3b8b6": [
            "b9d63325-412a-46ff-afdd-b1bb65add8eb"
        ],
        "9a270e1f-9b73-4e17-92e6-2d27d7e27ff9": [
            "054b7b9c-7651-40a0-b2d8-a370b825794b"
        ],
        "2306d947-508a-4a41-b550-6e1acfb888db": [
            "3a8a5528-6e4e-4f54-b52f-c2dfd37c8a92"
        ],
        "b809969d-bb22-4128-907f-0311f0dfd0d9": [
            "5ba90cdd-e9cc-415a-a73b-c475d2dd4e95"
        ],
        "7a961aa2-ff2a-486d-8c58-d6e15a249d4a": [
            "c54d1e29-e4cc-4382-a12b-1944cf131c34"
        ],
        "2abefde1-7851-4c1c-a405-6511c38d98e9": [
            "dbd958f8-8977-44e1-a1b4-d75b83746d7e"
        ],
        "8ba502e8-fe77-4094-8aea-e617620066f7": [
            "c2550989-8606-4dd7-a8fc-53622502749f"
        ],
        "47ff6045-269e-454b-a569-26b6fb0a07a6": [
            "c7d434aa-c831-4c83-8025-4e282a1835f7"
        ],
        "003c54b2-17ef-46bc-9e74-f249d9c5c52f": [
            "842b61b5-62f1-4b5f-bf90-997949a256ee"
        ],
        "9ecc914a-f1ed-4b4e-899c-ba3a7d133109": [
            "b53580d7-7c1b-427c-9c46-ce92ed103771"
        ],
        "e8c252ab-3f9b-4b09-81eb-9e800dc6b4d6": [
            "9758fdc6-6b8c-4e96-a1c3-75640613046e"
        ],
        "02d11b55-16de-4ad5-9d9d-d4a8f842ba9c": [
            "a863ec4e-b18a-4e82-b78f-1cde864ab19e"
        ],
        "d0c9a7f6-5870-4d55-a91d-a85e77179fa2": [
            "f946905a-75d5-445d-9c5a-749347fb8f48"
        ],
        "f6af7927-a13d-425a-9752-9711564b11f5": [
            "29de542d-42d9-4fdd-9e3b-18dd08395d3e"
        ],
        "5aa1f8f1-45c2-4152-8b39-0e249eb6470a": [
            "1a6c2715-3c52-46a9-bd0d-f50d99cc767f"
        ],
        "6e9bd473-3372-4724-a8c0-384ec2141236": [
            "64f34f35-6d60-421f-8b41-579efa2d8d5d"
        ],
        "a7f74465-b38f-44e8-9bc4-f608ad23e85e": [
            "caef3238-448b-4ff4-8d37-453505896e09"
        ],
        "08a2ecc4-d1be-4005-83c1-9d935910824a": [
            "52444999-e17f-42da-a188-08a10837125a"
        ],
        "baa603f1-ec1d-4fe5-9617-9ca775462980": [
            "22bb6e38-dee0-4121-9243-10ae0eac8f33"
        ],
        "1e7e1d9a-0b8b-4475-a6df-a5645252416d": [
            "edcd5348-264d-4e12-a57e-76941a1e6529"
        ],
        "91fa9e99-ae12-414c-9a2f-b8930cec42fc": [
            "bcccdf0d-362f-4ca4-b561-6603c50784c3"
        ],
        "f3cd0e50-571e-4102-be24-be40edecc833": [
            "3ac305ee-5644-4643-b441-febcd5943a12"
        ],
        "f310eb66-fe6b-4e3a-857e-86987f9f17fb": [
            "efa8ef36-2ef9-448c-a362-238168acbd3c"
        ],
        "3420d3cd-7071-4d7b-8aa8-7a1e7ef9c3cb": [
            "5b5a558c-9dba-4932-be92-68645f47d82b"
        ],
        "c281e395-25f7-447c-9a73-43b46fcc31fe": [
            "8db3973e-3e7f-4108-9ec0-75f0a500f07a"
        ],
        "3bae0033-b03e-4666-bc97-7b688a64d319": [
            "fd82ca9e-5ccb-4ad1-823a-adfb3a87878a"
        ],
        "ae08e1f4-ef87-4f99-8c2f-28b54cab2582": [
            "3247ab11-164b-4b40-91ea-1ace20cb9ce0"
        ],
        "6a7900a1-9f01-45e3-857a-0cf30f3dfe89": [
            "0365e678-2851-43a1-b33e-a28600de4705"
        ],
        "00c28c7e-d867-4462-a04f-6baba010eb93": [
            "99eb5b42-3cce-4750-bc6c-aa72a7017d2e"
        ],
        "5663b875-92fa-481b-b284-21cfb4ebaf7d": [
            "79839053-88ca-4238-b9db-2bb50f66f370"
        ],
        "f32eed23-f8f3-4120-bac2-e9ae15320a1c": [
            "f87cb1d9-837a-4ae4-8974-919ad2d096d4"
        ],
        "dc5a85ed-bf83-417e-aa78-c6e83d3a28fe": [
            "d721139a-015e-4525-96ba-0f83735bf82f"
        ],
        "d3e7cb6a-fccc-4d4f-9b6c-ff015d87458f": [
            "9a2fd43a-b480-4239-bd7c-48c295e74f19"
        ],
        "d7a7d8f7-8c02-4858-9873-1d3099d9a794": [
            "5eb7d61e-e0a8-439a-aa78-c1dcc8d4c41a"
        ],
        "faad5ecf-b598-4141-8f22-8b4409c4b144": [
            "7225ed0e-e76b-4da3-8d29-35958b732063"
        ],
        "e0b4aad3-4191-4726-9c32-681572efc50f": [
            "f4df1c68-81c9-404a-bdcb-44c4fadcd6a3"
        ],
        "d5270374-616a-453a-a1ed-0120e35a7237": [
            "c75478e5-0f6c-4c8c-b5e9-7bd1b56329af"
        ],
        "4aa509cc-c909-444a-981e-dcca52909ffc": [
            "a3a39d85-0be5-4dc1-b690-0bb25d32fb69"
        ],
        "6dcd67f5-9de3-4f2a-99fb-ac55079e4e48": [
            "33014d2f-841f-45f7-b449-dfeb053306b9"
        ],
        "ee5bde17-bad4-46a0-9fd8-044817953d6f": [
            "f5344be8-dfa6-4c72-8979-87d2e7a6f4c9"
        ],
        "ae3bdcdb-9f2e-462e-980f-78d1067e8c66": [
            "35e0085f-11b4-42ad-9052-a44b52753fcb"
        ],
        "75f8914c-336d-4a30-a3a2-9cb771882748": [
            "b29100a8-9968-42fc-b982-122cb627c1d5"
        ],
        "331d7f81-65b0-4892-b801-5e79d0389291": [
            "b2512ac5-6f04-4f06-b094-ba8fa8664fe0"
        ],
        "6552a900-3f1a-4f75-84d9-42b047a2c9a6": [
            "809592e0-d4ec-49cf-b3c5-ed5e200b4fc1"
        ],
        "7bb975c4-f1fb-4a05-ab1a-45460352a973": [
            "c2f12592-7aae-48b9-b8c6-3a496837d8d5"
        ],
        "09cd4d33-01cd-4241-94a5-62a522c97cd8": [
            "e5e4305d-a822-4f5c-a70c-66856aa46083"
        ],
        "a0cfd0e7-b100-4d64-91e7-6497558f52a3": [
            "e065658b-5cf5-4ba2-86ba-7fb6b731029f"
        ],
        "3b26892e-79ab-4cf0-8a35-eb0ed136d4c2": [
            "5f8e8fab-c686-4b9d-afa3-e9e037c0ea68"
        ],
        "40007f7a-d098-43ea-b8a1-ef19bd6e745d": [
            "7469f7ed-d234-4ea2-9a98-584f4e98e653"
        ],
        "eb29bd23-70bf-40b0-baaa-d24dde0daa84": [
            "a506794d-24c1-460f-8b6a-403d4d48d0fb"
        ],
        "fa1fc746-76da-4405-a724-00d0e6d00b42": [
            "aae37e37-7e42-480a-bcc1-cb47457e3ee8"
        ],
        "0229ba2e-0659-40b1-bb67-47965951b3ac": [
            "781de399-d0f5-4d86-82d0-44cb7c59f30b"
        ],
        "68bab876-0ea7-412f-b7a9-eb48bfc6eb80": [
            "86beef95-fdc3-4111-beec-07856012eb03"
        ],
        "4640118b-4da7-4db3-8507-50918d6da389": [
            "c08b5023-ef37-42da-ba7b-048fd75b090a"
        ],
        "15c7f33f-8dcd-4871-9912-ec7be0f5306f": [
            "9f7856fe-9196-4eec-bac2-622e3f60f0be"
        ],
        "ee0ce1c0-4e88-4dd7-9277-74bd1583db3e": [
            "a41528cd-829e-46e3-9b9e-17cfb8e41d3b"
        ],
        "f475f776-941a-4358-8fed-8bd172e38472": [
            "4a6be992-c09f-4b45-a124-a76a5e3334e3"
        ],
        "912713a7-9fa0-456c-86b1-7e6688c6d18f": [
            "4a92987a-a879-4820-a2b2-f511360a2498"
        ],
        "f4f2c554-0ba8-40d5-8994-01b4a9ccac81": [
            "80c134fc-bd73-47ef-919e-8612edddf3b8"
        ],
        "ea52b092-0a69-45b4-8b42-3b6262ca72d0": [
            "8be7962c-3382-4981-ad8e-2ae083f17031"
        ],
        "2c5d5a04-aa71-4ed3-8152-8dc3e44535a7": [
            "84285a97-c448-44c9-9db0-c6c9abe20c1f"
        ],
        "27a4b239-6ad7-4fec-84cd-f16a37d1e55e": [
            "16e3ed71-76d1-432c-b64f-c32e9d47c7c7"
        ],
        "ce95ec81-6472-44ca-a8f1-ed42b3c29077": [
            "9cdcfc6a-3886-4e19-8065-2cf3fcfaba58"
        ],
        "6919b019-9392-49a6-be7a-53246cd2fe5d": [
            "737f2521-66f2-4df1-924b-4e74b3617e5e"
        ],
        "331c1ced-a458-468e-9950-4a89bcefc2d1": [
            "8a82bdfc-4de1-49e9-866b-12dc1e5c03b6"
        ],
        "565996d9-b8d1-467a-836e-0eb228100f5e": [
            "51ead69e-05b8-4c3d-abf8-cdb0a357529a"
        ],
        "1b840020-3c32-4d46-9688-788ab8377543": [
            "2e8c564b-036b-4fbd-80e3-b1ef818a0a12"
        ],
        "58dfdc75-896e-4b7d-9f73-a311c86aba25": [
            "88c76ae4-1baf-4312-aedd-c34b013995d6"
        ],
        "087ae2c9-2b1c-4393-8ade-bf0e2c90f762": [
            "ebd53ff3-029e-4c33-8b33-6aa42b913ab2"
        ],
        "1ff263f3-b890-4f59-99af-38228a951978": [
            "1f95a53d-cf2e-40e1-93f0-66bcd0e12320"
        ],
        "05aec974-cb15-4f6d-979e-733ce9879a24": [
            "504c1d2c-c476-4337-b8c8-f61a130a7a36"
        ],
        "dd449b69-5d43-4907-961d-c8967b8f964a": [
            "cf1905da-a519-445a-8842-a696dc0acd45"
        ],
        "3358b0f5-13a8-4773-9da7-ab0dea7c8ec7": [
            "27c6a33f-2a77-4ad3-90e7-e4ccacd866fc"
        ],
        "7703e75e-4ddc-4614-a44d-0c5d94c68d4b": [
            "cf8e4728-7986-46aa-8762-6a16bb12a108"
        ],
        "88e5477c-2ebf-4680-95c3-a669c15b82d8": [
            "bc5c3845-985d-4643-a5b9-ee6a5a03cadf"
        ],
        "680b57d7-38cd-4687-aa7e-02656a36c441": [
            "e718bd53-be56-461e-85d9-f3bdb6ac3132"
        ],
        "50bd4267-9369-4210-a7cd-b1a284a277d0": [
            "a529d40f-5016-4b59-97ef-77b09b3ef892"
        ],
        "bb100f55-b3c5-4247-a8e6-1a7c6b4a8b1f": [
            "02e35ff5-605a-4cea-a7d1-65f150bdcc4f"
        ],
        "d2757dc9-33d4-4956-9ab6-e3eaad86213f": [
            "9751d48b-4cc0-4a8b-a9f0-89c6e43d22b5"
        ],
        "ff895aac-dc32-47cb-8747-56a77c716a49": [
            "fdec4acc-384f-41bc-9421-dd237f191c1f"
        ],
        "18a82f69-788d-4d86-90cd-2e7b083f77d3": [
            "641075e3-d586-4844-b56a-9c5a76361f91"
        ],
        "26cd582c-6ce9-45dd-8c88-d03b04555df5": [
            "5609ce6a-0093-458c-bc0f-6a0d59437b6d"
        ],
        "69889ce1-f92c-43ed-8bf6-1e32f249cf83": [
            "1ec8bbb9-23b6-4113-bce6-e36a140e32fd"
        ],
        "4e1fc80f-6cea-49ff-8517-b3b759433805": [
            "128aae5c-13ab-40f1-9338-178b5a5c058a"
        ],
        "de678e85-2d8e-478c-9b9e-8f26025dee89": [
            "8439118e-24c1-4b12-b944-7a0c86ee1705"
        ],
        "1a86e287-1680-4afa-8559-f5ecda761f60": [
            "d7299c50-ce78-4d67-9b23-6776041c7df9"
        ],
        "6f1249dd-1f36-49a5-bf96-f2868f0eb0e3": [
            "f41498de-6bc7-4c5e-8dcc-a1bdda0f6270"
        ],
        "fccc2cff-dd45-459b-a908-054f3fd61d75": [
            "9fbe671a-f52b-4263-88fc-1be6303724da"
        ],
        "e87a0f8e-d6eb-4676-a16c-ee8bafdca9fc": [
            "70d2ac51-62e5-4a08-a91c-5cc543a19de9"
        ],
        "120a717b-a62a-4034-bfa3-7681a0644e42": [
            "b4e7c314-ed97-485d-9227-9ac54fa0025d"
        ],
        "bac60919-79c0-495d-a509-8ac77033eee6": [
            "0146c9f0-7d15-446a-b1e9-9b00136b1471"
        ],
        "d6ed47e5-8005-42eb-8b21-29b50ee20f8d": [
            "91e7f59a-6fe9-4a2f-b94d-c61198374240"
        ],
        "a10a10dc-0947-40bd-a97f-83e06fed8949": [
            "80fbebee-b107-4c12-9f58-68171924bf2e"
        ],
        "f99f5a0e-a5a4-45cc-ac7b-954877751915": [
            "5a5a69ac-d6b5-49d9-b101-6d02615a5c13"
        ],
        "7760390b-4900-4e80-8083-0262d5b08c68": [
            "09015598-9dce-4b65-a670-4e1b3eb09102"
        ],
        "9a264dd6-71ae-4e28-82c0-5d7247a066af": [
            "05e4a05e-9fbb-41c8-9a4e-8e4993569fb3"
        ],
        "07f7fd16-9b40-433c-9146-5e3611e94159": [
            "74d2a0fd-0d54-45d6-bf4f-5fc7fa9ef628"
        ],
        "1fe97267-3efe-417d-8abf-f137fcf0a189": [
            "16ca16dd-0480-40c3-af35-3d84a93c5182"
        ],
        "4ad84564-c2ad-4eb9-baf8-e7eeb300ca6a": [
            "b74400cd-d8ad-4526-9380-1d72b2c0cbdf"
        ],
        "6ed5554e-89e4-467b-a873-e32aa62d6453": [
            "363f0946-5808-4310-adf2-905c6d7bdd8e"
        ],
        "1e002307-8b70-46c5-ad4b-11377714890b": [
            "c0303701-5368-4d38-8b54-2d3bd03a1b32"
        ],
        "f41dd11b-28e6-4e6b-86ec-16c92a2a6457": [
            "78b1bce4-40a8-4d90-b353-e55b7e0c478b"
        ],
        "6ac903e6-c806-42f2-9421-729ed1e5032c": [
            "0d99cc8d-01ca-4be2-899a-be05e989fe3c"
        ],
        "9d76c587-0ea4-4237-a16f-9d7423833f4f": [
            "f05a7c7c-fa4a-41bf-a26e-8936d722af55"
        ],
        "3ed3bc47-8639-4a0c-9d79-4baed79cc7e3": [
            "84a2dfe0-1415-418a-9973-0423c8b4aba4"
        ],
        "0116c9fc-fcbd-4574-9366-d9def0b35f12": [
            "0e6669c3-f0f1-44af-becf-d498bb5a5b3e"
        ],
        "502cd5df-7e7f-4c56-8e1d-79c593fc388b": [
            "14782e98-80ca-4ebf-8214-c0f309682060"
        ],
        "c88b8901-3d45-4b6c-8e59-8bed5b37c7dc": [
            "91bf7be7-d8e6-46e2-b839-35070b2248e3"
        ],
        "b4f6bf21-1431-42b5-b060-0d602e4c74e1": [
            "b0df97e1-312e-4bdf-a204-ed4cc5b4ed34"
        ],
        "035ea650-0385-4b1e-8df4-d835cb8d61f2": [
            "01ad40b8-39ad-4781-b41e-ba2b81aee8be"
        ],
        "24dff502-7c1d-4a20-9ad4-e7f9b9fa9507": [
            "3766ca60-c102-48a4-a4de-da38dd3919eb"
        ],
        "84a22574-578c-4a9e-b5e3-67da3b274a98": [
            "d64b0603-2bee-4ae3-9f42-5901e4475e22"
        ],
        "68e2492b-1254-416c-857a-fee714a701e5": [
            "dee48463-e5a8-4460-8d6c-2d9bc3f0cc71"
        ],
        "c6b78056-146c-470c-8fa0-e7beb84aed3c": [
            "49caed2b-72a4-46d0-ad67-002081609d95"
        ],
        "fdecfc33-1ffc-4caa-b336-e590930896b1": [
            "32eae5c7-8be3-467f-bdc3-3ef55e358c02"
        ],
        "8af71851-c45c-43c8-9287-44e37001819d": [
            "36a5545b-ece9-4cfb-822a-b149235da919"
        ],
        "39aa8bb8-29dc-43b7-aab1-9dfb9f659c38": [
            "5bc2c20f-d14d-4c08-a233-07d31eb1e09c"
        ],
        "666d5b86-3dd6-4671-b0c7-670e292ec7cb": [
            "b80def3c-79f8-49e1-9ba7-8a155dd94953"
        ],
        "5b90d322-9971-4ebb-94c7-0de041b31cf3": [
            "9c11b565-aef7-4f43-aa09-3fce8a5569c3"
        ],
        "703ab6bf-e844-4029-a864-3a5ba3ad5a6d": [
            "5d724b84-03d5-4c96-8525-5a3651a78079"
        ],
        "7c0e8c7c-1c76-4d70-b025-518e0f3167dc": [
            "7438c654-406c-494b-abca-55b0fcb43031"
        ],
        "ef758cdb-e388-4bf9-96f4-d130a60c6857": [
            "ad5a36c2-ccf7-42bf-bec4-18548a94bb3e"
        ],
        "cb51aa4e-3e78-4d3a-a22d-dff1e2e7502f": [
            "17cbc134-478c-4921-bd41-e7d33cc20edb"
        ],
        "68493b55-f2f3-42cf-a864-d8b2eddbfc9a": [
            "28648c05-2c7a-46fa-8562-1874567ba9fb"
        ],
        "1304ab1f-8150-4723-8bf5-b786d9ed64e6": [
            "db773932-8da0-4dff-ba9a-128ccf5ebae8"
        ],
        "93872471-ecbb-4835-b001-ac694e430aa3": [
            "d8a776fe-05c4-4951-ab9c-cf48267c63aa"
        ],
        "fd7c196c-9486-47aa-a04a-e22ce440754d": [
            "2807dcfd-8056-477c-819a-fa3ff37ae5b5"
        ],
        "a6357e03-caa6-4e8b-9cef-e57868675480": [
            "988a849e-d5ec-4d3f-84c5-19766b54645a"
        ],
        "6798f379-c248-45c2-8389-7e8f5c8a4802": [
            "62ee6289-95bb-4759-9f8a-b20558779b11"
        ],
        "fa57f66f-ad7c-46a2-a587-8f009a13b911": [
            "32e845a3-7e98-4aa9-a814-75e3e17e63f0"
        ],
        "2e0663c5-3bca-4e4e-b16f-3b6ad9592dde": [
            "59d6af4f-2179-481d-b6ca-50d4649d1c13"
        ],
        "7118f60e-d6b6-475f-b658-600cfebb8484": [
            "5b72a45c-7213-4135-a996-0cfc11b7dba0"
        ],
        "a6e48a2f-3c79-4af6-b049-3463cb5df342": [
            "d0e12969-76a5-4ca4-ac48-0d70b3a70782"
        ],
        "e6253e61-53e5-4605-bea6-022c136b3048": [
            "5b265a05-2e10-4b13-b79e-f883843c5feb"
        ],
        "01bd9e6f-d7d7-48c7-80ee-d56a37d1baa0": [
            "b9000110-602e-4c13-bfd3-e87c8d5efccc"
        ],
        "4483f62c-42dc-4ca7-9888-9005ddda790a": [
            "943fe534-56d1-4011-b8b6-288ec39fc4f0"
        ],
        "2aaed543-48bf-46de-81de-5534bcf6b359": [
            "75ac724d-a6b5-46d6-b81f-5050c1ee242b"
        ],
        "16882a41-260d-436e-a1f8-2a625b168e43": [
            "3eb9ee30-31d8-47f2-ab0a-1719518d3479"
        ],
        "66984796-e15e-47a3-a0a9-2c2eeac14738": [
            "3b708d76-3909-45f2-85ff-f4774206ea06"
        ],
        "242bd5b0-b934-4980-974c-dba06957d856": [
            "124d9bac-8627-44ef-a68f-02cc9dbbea92"
        ],
        "1b4924dc-f865-4be3-b4c5-461d7c8fe9bd": [
            "ddf44377-4520-4fb6-a592-ed59e8eef2e5"
        ],
        "cd322af3-2e3a-44c3-b5cf-ee60cd620041": [
            "cfaaba6f-049a-42d5-bcb1-5cf4f3adf102"
        ],
        "fb33e4af-e26f-4a98-a06b-329e65dc2b1f": [
            "373698a0-fa87-4c69-af76-682722bb6863"
        ],
        "0d9abf7a-7662-43c2-936e-055c9acd1265": [
            "cc64cc86-1032-440a-8504-a74f7b75c4e0"
        ],
        "e29af0fc-e7ad-4136-9ce4-a16325d84932": [
            "73dd6c94-e8d4-46c4-8402-a80f767821e5"
        ],
        "80c44469-644c-42e9-97f3-ced484235160": [
            "ea0e54b3-e29d-4f42-bc40-e5fc7b4e80d1"
        ],
        "5bc76e86-bbe3-4781-b3e9-a8791bf77101": [
            "ad096bc7-7b40-44ce-aa72-961d76518ae9"
        ],
        "28a5069e-05ac-4242-8667-3c9b86a77909": [
            "8cca1068-6827-4820-a52a-78fd3315770e"
        ],
        "a3f1ce99-134e-4fe2-9539-642f7dbfaa43": [
            "cccba130-7106-49fb-9b0c-3a2cb2dfc893"
        ],
        "7b9f6ffc-b963-4713-90a0-f7dd53f11d47": [
            "2e55b83b-f0a5-4a20-bb6e-90a413068035"
        ],
        "ca8ce895-ae57-4a75-b943-74bc8ecdb48c": [
            "4427f16c-0661-4f79-a4ce-8e041e882fad"
        ],
        "3abf8cae-eec1-468b-acb4-4aeff0bb1889": [
            "7efe26af-a562-46c2-9d49-42dc53a15026"
        ],
        "d23be4a3-301a-442d-af9d-dd9c934d9189": [
            "b60204b7-5a6c-4b26-a5d7-7f09f81c6fa8"
        ],
        "4ab27502-9003-40d3-a0d8-097e6aecb119": [
            "299c64b5-a7cb-48de-ac16-8b84625fba49"
        ],
        "8cbc61d5-1d5c-4986-9d33-c33bafe386e6": [
            "3af0a95a-1f8e-4dec-b3ce-d2464d541f38"
        ],
        "aefb53ef-9f74-439e-a25f-d276ef199242": [
            "48d9b068-53c4-4ea4-b818-61fe1f00736b"
        ],
        "5332926e-bf10-47f0-b68d-dab4e3ba77d1": [
            "99774231-5c6b-44c2-b572-77d4217a8243"
        ],
        "492b1fef-44e7-467b-b865-a3dcdf9e3afd": [
            "6b64d7a4-695f-4961-bb46-27dcc3f33c31"
        ],
        "c2613af9-f827-4fae-8154-38baa90f50c9": [
            "7cec150e-d061-4ff3-b0b4-a81d4e49a8be"
        ],
        "51cc1328-435d-409a-9755-a744b7fe22e6": [
            "1a6c05e6-10de-406e-ad43-dafceeaf4ece"
        ],
        "5aa5db40-281a-4500-8ee4-6371a0c458be": [
            "bc672b0a-d96e-432a-a2d1-e3bb0548c8c9"
        ],
        "d99858be-a9ce-4598-9da1-a751abff07ec": [
            "244cb2f9-9243-4f06-a342-fed86f817154"
        ],
        "104175db-9af5-404b-9a22-82c1b35c768a": [
            "96c96216-d619-4b27-82f7-150116ff4ca3"
        ],
        "1c1b304a-10fd-4e7a-ae08-8105d5d6f640": [
            "94a24b34-78b5-43a4-88ff-034017dbfda5"
        ],
        "d9ea282c-fdbc-4da2-9251-7c85d10b22ec": [
            "0b1abfc3-fe45-4fca-b5fe-5f7962a915f7"
        ],
        "50724ae6-ecbf-400c-89e0-c9a66c66e5c8": [
            "62c64e7b-c7e8-4f4a-9280-41db6bc0d36e"
        ],
        "651eae6d-c6e8-424d-86d5-8ef779478f64": [
            "651232e1-7d1f-4b3f-b804-2ed00f120e98"
        ],
        "1aa79707-5d33-443d-b54d-4210da0c22a0": [
            "4bc7a031-4f5e-4f3a-b640-1b420d66d5a7"
        ],
        "a07132be-d91f-416f-962e-b5cab47ae244": [
            "9f21fe6a-4773-4e47-9fa4-a7829f4a4595"
        ],
        "d128f7b2-b571-4196-a08e-72742958a936": [
            "cdcc76f5-65f6-49f0-a91e-a2eaa2fb1815"
        ],
        "a7e28b82-3d94-47a1-a682-b2c06aa88720": [
            "56e04b52-895b-4553-b941-9912f410bba9"
        ],
        "054e3d68-7c6c-433f-ad7c-6b2579ffca30": [
            "6b125747-0dc0-49af-b1a1-eb47676158eb"
        ],
        "351a5128-c1fe-4fd2-8982-3ecb46ed8412": [
            "4fe059eb-456d-4cf8-aea2-b135bae655f5"
        ],
        "c4e33c2f-540b-46e6-9d64-b9758f0285a6": [
            "db77f27c-bfaf-452d-b83f-13df63e8ed30"
        ],
        "05075e9e-11e2-44b8-b0dd-9972d9742ffd": [
            "d3e64b6c-3a41-430b-9c54-763f550963b2"
        ],
        "9b4b5e86-2fee-4b91-a6eb-37ce4250143e": [
            "6d5c45b9-676b-4011-ad31-ab0b03144c4c"
        ],
        "27a506e4-834e-4fda-b4a2-cc7789663fb7": [
            "40867467-34c7-4e3c-a036-09dafde355c2"
        ],
        "af094357-9348-48ec-87aa-fea6ba70fc61": [
            "ff2bbf1e-3728-4ba4-8d8d-c1940338847d"
        ],
        "772ade24-631f-4bf8-88e3-df35c83f1c41": [
            "ffcbd04d-6567-4afd-b8a2-0f2cf415b9ab"
        ],
        "554124dd-202e-46c5-a5c9-18b7f83b7ff4": [
            "bb364e96-ac17-4c1b-b5da-252ec717a581"
        ],
        "9f3136d1-ed9b-4ab6-8316-857b88717836": [
            "49cdee3a-0c20-4eec-81b4-e63b9370f335"
        ],
        "1da11d71-2f42-4028-8aa6-08121ff0b952": [
            "8ca8528c-b8a4-4eb3-841c-904e7da44927"
        ],
        "eb6c3d41-949b-483a-9a90-b9ab936b5005": [
            "5258f2c0-1997-4690-bed4-d25f86fc63cb"
        ],
        "b32cfbac-131b-4de9-a070-a02d35d889ff": [
            "7d5b1560-dfa9-4ce8-bc48-23dfd3faafd3"
        ],
        "6faf4e3d-6976-4239-8b48-e9d947522ecd": [
            "ca57d1cc-a457-4aa5-9fc9-cde9414ed10d"
        ],
        "016a877b-627b-4058-9201-3d6dea0f50c9": [
            "8e97489d-cfdd-4e31-99d6-203053e74184"
        ],
        "d9a5330c-ebdf-4359-bc2f-0eb7cf6d8ef4": [
            "3631b896-ffcb-44a8-b5b0-573c56d84380"
        ],
        "f3a577bb-5319-42e9-9e9f-2e30a698f18d": [
            "6fc47d6e-c8ce-4233-ab60-519c2fe80757"
        ],
        "e498299c-6c63-4f74-a573-423a36a6b46f": [
            "b8234e47-2009-4b92-89ec-9c9ac936841f"
        ],
        "ad19d31d-e22b-45b7-b08b-8e9d11f50b76": [
            "22946832-c9af-47d8-888a-e1f828091680"
        ],
        "6ec891a4-29a1-40b8-ac24-450b0a1ac989": [
            "6d469f2d-55c2-4f02-b2ed-2b5dc47a69ca"
        ],
        "f3d9f898-3a11-4fcb-8d19-fe99b4b850d1": [
            "55a2ce90-bcd4-4099-adfe-a6e09fe2e563"
        ],
        "75221a42-eaa5-4756-8428-92f359602cfd": [
            "5d5068b9-5e1a-4c20-bf86-ae1d00c1eda8"
        ],
        "7081147b-d0cb-46c0-8884-d0bc5e58a16a": [
            "a1f52fda-2b61-4b7d-8ffa-924a20c1887b"
        ],
        "baba3475-ffda-495c-8bc2-ba6e4986c0cd": [
            "51e07c29-bfd0-4a92-b279-bcf09cd7a036"
        ],
        "c5c7ee58-dcee-4f91-a9b8-11b5edb30e10": [
            "f8d411e4-ec5d-48db-8ec4-d3b95ff04ab3"
        ],
        "4855fdec-8d06-4e4e-8d98-66d353f940be": [
            "f376f465-ab16-46d9-b806-6eb39e6deaae"
        ],
        "4270916d-1e8c-4b91-9e76-40cb5687caf9": [
            "5a704c12-49dd-49ba-802c-7230727cbabb"
        ],
        "128fbe5f-01b0-4254-afd9-dd1291cd0141": [
            "6027be36-5ca5-48c0-897c-256ed099bb2a"
        ],
        "8e84aa52-0294-49b7-9ab5-adaf698b9d8c": [
            "4e4546b4-8052-4838-94ce-fd1a0eb59669"
        ],
        "517694a7-7e89-4b85-b004-127485725cf1": [
            "5553c50d-4c47-4fc4-9b17-39a31cc73111"
        ],
        "aed5eb95-372c-4b33-9105-28a4c910c198": [
            "a3b19cf5-b6ae-4913-861a-3635fa2462b7"
        ],
        "a76a5b69-fd27-4a40-86e0-82b3fccbaea4": [
            "9d35d576-73f8-457b-9b96-67d192cc3a4a"
        ],
        "4b7adaca-24e7-404c-98e4-a5ca7cd4e162": [
            "ec367c5e-3c33-4979-bd7b-cf68cf63bc31"
        ],
        "1c1adbf7-0392-41e0-9fa0-4e6bb5067bf5": [
            "44756872-f937-4ba6-a126-db8d5e7ebad6"
        ],
        "b194f6a0-d2e4-4c97-a8b2-d4a0d7d8d2dc": [
            "935c5c88-a6b0-48d3-8947-fbce9e8a45d7"
        ],
        "34b25f09-e9e8-4add-8352-d598cb147508": [
            "ce988917-5eb7-4a31-b611-89258503d7ed"
        ],
        "cc6c3ed1-b22b-45c3-a2c0-707b94a3fc8e": [
            "078a27c2-0329-4892-9f7b-dfc8b4699414"
        ],
        "1d96afe3-45c2-45ab-ac40-e217af2676d5": [
            "11e0eae0-f30a-4d46-a7f5-6cb16f8cc45a"
        ],
        "f59264be-5730-47ff-8c88-82ac0b8f954a": [
            "3bfa2fd4-b0e0-4789-a6a7-73d165d6ab7b"
        ],
        "56dae946-b5f1-45bd-89bc-750a86b38973": [
            "b7316eff-79f2-43cb-9c66-bc239234daa3"
        ],
        "50d325d7-8fe3-4155-97bf-336f90d430fa": [
            "4e5a2ce7-4f45-44db-8bd9-3635e7f7fb0a"
        ],
        "356b7962-430e-4a11-8830-49016cf53f6e": [
            "ee1bd192-fe56-4738-b28c-1caf21276b92"
        ],
        "4a8daf28-e8ca-4b05-abb9-bf98cd298622": [
            "4f95fdd5-c390-4530-9cc2-499b06457476"
        ],
        "96e6d5ef-ee1b-43ce-84fc-30d14f93cfcd": [
            "6b12af02-7d5d-42ac-88fd-a9f3c3c15c0e"
        ],
        "827d8276-a009-4898-82cd-19295792c765": [
            "aa0e0ea4-81b6-47d2-8030-f7d5fa82a752"
        ],
        "b5899782-6159-4fe2-a3e5-1c26d8420a66": [
            "6211f9f4-8067-4bd5-ad0a-661afcb25516"
        ],
        "32e41432-f946-4c1c-9c4f-779fa6a829f8": [
            "7d1130de-6b53-4429-81c0-c046744aa61c"
        ],
        "797d9325-084c-4756-8029-88cdd5586208": [
            "5ce666d6-f48e-4335-ba1d-6a06be2b5c8d"
        ],
        "71eb82d3-a74a-439d-bbd3-136981708290": [
            "f869c32c-0c28-46c6-a031-22a5f087b471"
        ],
        "4a0e3624-e7a3-49aa-8424-4d8fce5cb8a0": [
            "8a496717-17d4-445d-8ece-6550d5318817"
        ],
        "fd973568-354d-44b7-989e-88ca290198a4": [
            "0f60c0ec-17a2-428d-a2fe-4ff821a21022"
        ],
        "4a5290ed-7ddb-40e0-aee8-593a30a39b48": [
            "57c4ad01-e4c8-4436-bc79-ee5d6ade6dfe"
        ],
        "f0c0be0d-4d42-4e21-b29a-652b60718df0": [
            "16a4c7a7-162d-46b3-84cd-7c29dc4fd31b"
        ],
        "ff2446fb-a265-47e0-a47e-15527becbd83": [
            "d7f9faee-b2da-4058-bc83-89e08738b44f"
        ],
        "f33c56bb-fc90-4bff-a9af-957be8afda95": [
            "282c2886-0dbc-455b-b20f-330e1c74113e"
        ],
        "5a54ac82-bf21-4936-8c20-150ce10f1fa5": [
            "5ab4e846-861e-4e27-9609-a5396afb58dc"
        ],
        "cd0b6fea-f157-4278-893f-ed0852a28892": [
            "2a7ac2ba-6537-401d-96e7-44a9b08df02c"
        ],
        "16712718-2f3c-4df9-8b5f-301d5903fde9": [
            "9b0deb89-5040-4dde-83b1-b2fd95a0a49d"
        ],
        "384c7537-a9ff-407f-9510-e686b3888802": [
            "3e648bd9-4a5e-432d-a76e-5a27bd53b11b"
        ],
        "56168cb3-3272-473d-9bd4-0641cae9dcfd": [
            "2afadfd0-faa8-4185-8b04-66a36e639539"
        ],
        "f096fbb0-1dea-4fb1-b5eb-bd99cd09b80b": [
            "bb15a396-71ca-4cf8-9930-2c5926af7a1d"
        ],
        "73c3a9b5-96fb-4824-aa01-5187d5ca7d45": [
            "ac6c4452-105a-4c89-8280-f72a0a89e674"
        ],
        "9c23e86a-edbc-4256-b245-ac8f96188c0c": [
            "eecd6a9d-ffe0-4d40-bb3b-9195c1e1c5da"
        ],
        "ac81f651-0107-41a7-947f-562b724478c9": [
            "a7ed3125-d137-4b24-83f1-60064dcc68eb"
        ],
        "489c0b95-c0f9-40df-8c88-bbd15c749b7b": [
            "75cb1099-e5d4-4548-919c-f2e4b85af25c"
        ],
        "da5debf3-998e-402b-b507-e3ed853b03e3": [
            "9dd0414d-0d6a-4888-843f-41eb9c97ac88"
        ],
        "9454780a-bf7c-48a2-a79d-0935df3d8891": [
            "39ee05a2-f679-49ba-beb6-475b3cd85dca"
        ],
        "ce64cbe2-fd72-4725-9471-3b98f598d0c5": [
            "fbd40123-f2f1-4964-8e18-f19ebcb000f9"
        ],
        "3a4c94e9-450a-40d6-9297-a314315c034a": [
            "5b151b1b-c291-471c-a5e4-2d3368a7e129"
        ],
        "60de96fa-d55d-4876-963f-5ef67d28f848": [
            "5fb32acd-24fa-4dfd-bc66-153002580c27"
        ],
        "9a5a3337-6cf1-476c-9238-e2ac80abab3e": [
            "714f7f83-96d5-4d25-9786-35bd9ea6a1a7"
        ],
        "4d6ed988-8717-4767-a2ae-4b7c2d981df5": [
            "31e69899-ec98-47c8-80cd-e1d581052756"
        ],
        "17fd309d-5785-498c-b64c-aef1f36d66f8": [
            "353f188d-f0bc-4610-bb70-9ddbff02f283"
        ],
        "6fd927a3-c47e-49f5-a980-95fd7f571294": [
            "34cb5a3e-1acf-4a4d-a9d2-5867d0558853"
        ],
        "ac0815ca-7901-464c-97e1-ac78c68ecf68": [
            "ced4cdb9-c171-4dc1-a5d5-f5c9450a2b49"
        ],
        "2db5a11a-d1f6-4ae6-943e-0e3eacd5ce4c": [
            "bbe7ac46-47be-4fb7-9810-87f8110df30d"
        ],
        "66034cf7-8d0f-4d16-8461-2f8dd02d4329": [
            "66d5d71e-55ee-40dd-b36e-694abf2056dc"
        ],
        "2d4fac7b-1939-41e1-b6cf-561e5989f6a5": [
            "f70e7265-1c09-4354-a16f-6a4046aa6b08"
        ],
        "b13a714b-40fe-4251-b1b0-6ead26b760a8": [
            "2d4ebd7b-e175-4a14-ae1a-e2737b959868"
        ],
        "0830afc2-f44d-4d65-917c-f885b408e98e": [
            "975968d5-1eb9-4e8c-b2c9-59620706ef6d"
        ],
        "9443aa45-5e36-4edb-81cc-fadd25c54225": [
            "8f70f98e-19ab-4098-a7d4-dd0f38921364"
        ],
        "dbb225ff-428e-47aa-af49-4ee5a9174db8": [
            "75f8df93-4af0-4b48-a0d8-ecf44cb44a9d"
        ],
        "b425d421-4868-474e-a0d3-7dcff9100cf9": [
            "87649eb0-a402-418d-9591-922a452c87f5"
        ],
        "69ae90c2-4383-4e00-bb31-9b963ec64b06": [
            "9c1be897-6142-4185-a037-6cc2959d7b85"
        ],
        "3e46fb51-a04b-43db-a8c0-498aea5aa6b7": [
            "7986e018-181d-4d9e-8bc2-0eb38e998ca3"
        ],
        "d46b94d5-a5a2-44c6-8caf-64e137eaa84e": [
            "2cb70189-7584-4913-968e-656e3c64784a"
        ],
        "09c47b56-2765-43f6-ba6c-2331d8e66964": [
            "3a4685b3-0875-4101-91e8-610620c43d98"
        ],
        "b78ced1f-f7df-434f-8dd6-ec1064548ce2": [
            "11479571-1bd7-40af-9619-1e527120471c"
        ],
        "b5266ba5-a573-418c-b639-19d65b0ce501": [
            "708d1e6c-ec2f-4c53-9280-1ad66d6ea0ed"
        ],
        "5c895d32-68a5-48f6-b261-ca95912d9fcb": [
            "f0d5096e-1dc5-41bb-839f-49c0e27b34a5"
        ],
        "2654fb1b-d5cc-4982-8e75-75059059d5e6": [
            "1b635e04-193b-45ed-921a-2b6ccab0d158"
        ],
        "4b59f879-5f7f-4ecb-94c1-720331f43121": [
            "ea9fe0d7-1e41-4634-b129-be76e53dbaa8"
        ],
        "d840c7ab-660b-4a59-9162-ec5943edef79": [
            "3bd96605-63e9-4d1b-abd3-24ea8ed5d888"
        ],
        "75d3232a-5336-49d8-8a05-0a965c6c74d0": [
            "e9030ff9-2710-48bf-b2a1-e028b97de27d"
        ],
        "1dee51bc-eb0b-44ea-9d9d-2e1d6c951056": [
            "bc42a5fd-25d4-44eb-a405-8e1901a3eb70"
        ],
        "3c4f56bd-747a-4232-a987-fb29ef77701a": [
            "57e90a2b-16d5-4b89-9ef5-cc319088d526"
        ],
        "b73f1d79-3bde-4189-ad38-cb5a08a550fe": [
            "8bf7aff9-a4ae-44a7-bdbd-171c953daa1a"
        ],
        "bd9b7fee-0305-48c3-8035-a4b5d7c4c752": [
            "beb17477-5802-44c4-a6e2-a7b72715d345"
        ],
        "a83fbc58-c6ef-48b6-8de6-8af60074c828": [
            "8576696e-a17c-485a-96aa-97500940ef92"
        ],
        "b8d4235b-acd1-4900-b7fe-1043d74fe3e6": [
            "db6af6d2-41e4-4053-a495-5cf0a16add41"
        ],
        "6fb7aabd-f9bc-4fc2-95e8-e27620c0537b": [
            "28e70dc2-19a8-4abe-b9a5-a26ff0d30d19"
        ],
        "5315a59e-a902-43a7-aac2-173e5753046c": [
            "5a3a2859-ac46-4c15-bf7a-b66ac1a93941"
        ],
        "a5e0d05b-c7bc-4658-87b8-9752d5964e69": [
            "e913add9-9a50-4fc4-9f16-0fb36c1e4cde"
        ],
        "677939dd-3946-44bd-9810-dbcc73c34c34": [
            "3709cdd2-2595-49b5-bb29-090e81c8b07b"
        ],
        "3287c542-b2aa-48d4-86a5-38ed8f24782d": [
            "8ec17857-976b-47b4-a7be-bfab6b488e37"
        ],
        "fd6d5637-cf52-4c73-9771-6c025d1f2e69": [
            "f2fde255-1fa3-4336-b31b-69b41693147f"
        ],
        "7e7a0833-03d2-4586-adb3-541e476c1399": [
            "d9614bd5-412c-4847-930f-9b5d8098095f"
        ],
        "7782995a-45c0-4e3f-8ff2-f8af3a697c3c": [
            "b900fee9-c0a5-4443-b990-c7e26ca1e6f5"
        ]
    },
    "mode": "text"
}
